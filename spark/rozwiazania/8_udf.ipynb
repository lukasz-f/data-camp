{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/kodolamaczlogo.png)\n",
    "\n",
    "# Przetwarzanie Big Data z użyciem Apache Spark\n",
    "\n",
    "Autor notebooka: Jakub Nowacki.\n",
    "\n",
    "\n",
    "## Podstawy Spark SQL - UDF\n",
    "\n",
    "Podobnie jak w Hive czy wielu bazach danych, Spark SQL ma możliwość definiowania funkcji użytkownika, ang. User Defined Functions (UDF). Funkcje te biorą wartość z kolumny i przekształcają ją w inną wartość. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as func\n",
    "import pyspark.sql.types as types\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .appName('udf') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#sc = pyspark.SparkContext(appName='udf')\n",
    "#sqlContext = pyspark.sql.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wygenerujmy najpierw dane:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- kolumna: long (nullable = true)\n",
      "\n",
      "+-------+\n",
      "|kolumna|\n",
      "+-------+\n",
      "|     44|\n",
      "|     82|\n",
      "|     22|\n",
      "|     49|\n",
      "|     50|\n",
      "|     32|\n",
      "|     59|\n",
      "|     13|\n",
      "|     37|\n",
      "|     75|\n",
      "|     59|\n",
      "|     43|\n",
      "|     84|\n",
      "|     15|\n",
      "|     74|\n",
      "|     26|\n",
      "|     52|\n",
      "|     54|\n",
      "|     60|\n",
      "|      6|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "df = spark.createDataFrame([pyspark.sql.Row(kolumna=int(i)) for i in np.random.randint(0, 100, 100)])\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do stworzenia funkcji używamy normalnej referencji do funkcji Python lub lambdy. Przykładowo, chcemy funkcję która zwróci klasyfikacje wartości:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def klasyfikuj(wartosc):\n",
    "    return u'dużo' if wartosc > 50 else u'mało'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Powyższą funkcję należy teraz przekształcić w funkcje która działa na kolumnach. Robimy to używając funkcji `udf`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "klasyfikuj_udf = func.udf(klasyfikuj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "klasyfikuj_udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tej funkcji można już użyć na kolumnie DataFrame, np możemy dodać kolumnę:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|kolumna|ile?|\n",
      "+-------+----+\n",
      "|     44|mało|\n",
      "|     82|dużo|\n",
      "|     22|mało|\n",
      "|     49|mało|\n",
      "|     50|mało|\n",
      "|     32|mało|\n",
      "|     59|dużo|\n",
      "|     13|mało|\n",
      "|     37|mało|\n",
      "|     75|dużo|\n",
      "|     59|dużo|\n",
      "|     43|mało|\n",
      "|     84|dużo|\n",
      "|     15|mało|\n",
      "|     74|dużo|\n",
      "|     26|mało|\n",
      "|     52|dużo|\n",
      "|     54|dużo|\n",
      "|     60|dużo|\n",
      "|      6|mało|\n",
      "+-------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('ile?', klasyfikuj_udf('kolumna')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Należu uważać z typami; jeżeli chcemy zwrócić inny typ niż tekstowy, należy przekazać tą informację w definicji funkcji UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "oblicz_udf = func.udf(lambda v: v + 123, returnType=types.IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|kolumna|oblicz|\n",
      "+-------+------+\n",
      "|     44|   167|\n",
      "|     82|   205|\n",
      "|     22|   145|\n",
      "|     49|   172|\n",
      "|     50|   173|\n",
      "|     32|   155|\n",
      "|     59|   182|\n",
      "|     13|   136|\n",
      "|     37|   160|\n",
      "|     75|   198|\n",
      "|     59|   182|\n",
      "|     43|   166|\n",
      "|     84|   207|\n",
      "|     15|   138|\n",
      "|     74|   197|\n",
      "|     26|   149|\n",
      "|     52|   175|\n",
      "|     54|   177|\n",
      "|     60|   183|\n",
      "|      6|   129|\n",
      "+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('oblicz', oblicz_udf('kolumna')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aby wykorzystać funckje w zapytaniach SQL należy ją zarejestrować nieco inaczej, mianowicie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.udf.register('klasyfikuj', klasyfikuj, returnType=types.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|kolumna|ile?|\n",
      "+-------+----+\n",
      "|     44|mało|\n",
      "|     82|dużo|\n",
      "|     22|mało|\n",
      "|     49|mało|\n",
      "|     50|mało|\n",
      "|     32|mało|\n",
      "|     59|dużo|\n",
      "|     13|mało|\n",
      "|     37|mało|\n",
      "|     75|dużo|\n",
      "|     59|dużo|\n",
      "|     43|mało|\n",
      "|     84|dużo|\n",
      "|     15|mało|\n",
      "|     74|dużo|\n",
      "|     26|mało|\n",
      "|     52|dużo|\n",
      "|     54|dużo|\n",
      "|     60|dużo|\n",
      "|      6|mało|\n",
      "+-------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.registerTempTable('df')\n",
    "spark.sql('SELECT kolumna, klasyfikuj(kolumna) AS `ile?` FROM df').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@func.udf(types.IntegerType())\n",
    "def foo(x):\n",
    "    return 2*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|kolumna|foo|\n",
      "+-------+---+\n",
      "|     44| 88|\n",
      "|     82|164|\n",
      "|     22| 44|\n",
      "|     49| 98|\n",
      "|     50|100|\n",
      "|     32| 64|\n",
      "|     59|118|\n",
      "|     13| 26|\n",
      "|     37| 74|\n",
      "|     75|150|\n",
      "|     59|118|\n",
      "|     43| 86|\n",
      "|     84|168|\n",
      "|     15| 30|\n",
      "|     74|148|\n",
      "|     26| 52|\n",
      "|     52|104|\n",
      "|     54|108|\n",
      "|     60|120|\n",
      "|      6| 12|\n",
      "+-------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('foo', foo('kolumna')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dostęp do JVM\n",
    "\n",
    "Spark używa [py4j](https://www.py4j.org/) aby wykonywać komendy na JVM. Dostęp do klas mamy w PySpark nieco ułatwiony, mianowicie używamy `_jvm` ze `SparkContext` podając pełną nazwę klasy z pakietem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tekst <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "s = spark.sparkContext._jvm.java.lang.String('tekst')\n",
    "print(s, type(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać, py4j konwertuje typy. Możemy też użyć bardziej skomplikowanych obiektów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java.util.StringTokenizer@771f4555 <class 'py4j.java_gateway.JavaObject'>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = spark.sparkContext._jvm.java.util.StringTokenizer('Ala ma kota!')\n",
    "print(tokenizer, type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "while(tokenizer.hasMoreTokens()):\n",
    "    print(tokenizer.nextToken())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lub prościej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\s <class 'py4j.java_gateway.JavaObject'>\n",
      "[Ljava.lang.String;@624fd522 <class 'py4j.java_collections.JavaArray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Ala', 'ma', 'kota!']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = spark.sparkContext._jvm.java.util.regex.Pattern.compile('\\\\s')\n",
    "print(p, type(p))\n",
    "l = p.split('Ala ma kota!')\n",
    "print(l, type(l))\n",
    "list(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oczywiście lepiej powyższą funkcjonalność wykorzystać do funkcji zwracających RDD lub DataFrame, które można użyć bezpośrednio w Pythonie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
