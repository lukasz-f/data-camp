{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/kodolamaczlogo.png)\n",
    "\n",
    "# Przetwarzanie Big Data z użyciem Apache Spark\n",
    "\n",
    "Autor notebooka: Jakub Nowacki.\n",
    "\n",
    "## Twitter Airline Sentiment Analysis\n",
    "\n",
    "Zbiór danych oryginalnie wykorzystany został w [konkursie Kaggle](https://www.kaggle.com/crowdflower/twitter-airline-sentiment) o tym samym tytule. Zbiór zawiera tweety ludzi na temat linii litniczych. Zadanie polega na stworzeniu modelu ML używając [Spark MLlib](http://spark.apache.org/docs/latest/ml-guide.html) do określenia sentymentu określonego tweeta. \n",
    "\n",
    "## Pobieranie danych\n",
    "\n",
    "Dane można pobrać z Kaggle po zalogowaniu, niemniej, są też równierz dostępne w wielu innych miejscach. Poniższy kod pobiera dane z GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "data_path = 'data'\n",
    "if not os.path.exists(data_path):\n",
    "    os.mkdir(data_path)\n",
    "    \n",
    "file_name = 'twitter_airline.csv'\n",
    "file_path = os.path.join(data_path, file_name)\n",
    "link = 'https://github.com/sunilpankaj/Twitter-US-Airline-Sentiment/raw/master/Tweets.csv'\n",
    "if not os.path.exists(file_path):\n",
    "    print('Pobieranie pliku {} do {}'.format(link, file_path))\n",
    "    urllib.request.urlretrieve(link, file_path)\n",
    "    print('Pobieranie {} ukończone'.format(file_path))\n",
    "else:\n",
    "    print('Plik {} jest już pobrany'.format(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wczytywanie danych\n",
    "\n",
    "Dane są w formacie CSV. Proste wczytanie danych odbywa się poniżej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .appName('tf-idf_sql')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Użyto funkcji `distinct` bo są w danych drobne duplikaty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter = spark.read.csv(file_path, header=True).distinct()\n",
    "\n",
    "twitter.show()\n",
    "twitter.printSchema()\n",
    "twitter_n = twitter.count()\n",
    "twitter_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dane zostały podzielone na zbiór testowy i treningowy w proporcjach 80/20. **Proszę nie zmieniać podziału bo końcowy test będzie wykonywany na danych testowych, które powinny być takie same dla wszystkich.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter_train = twitter.limit(int(twitter_n * 0.8))\n",
    "twitter_train_n = twitter_train.count()\n",
    "twitter_train_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter_test = twitter.subtract(twitter_train)\n",
    "twitter_test_n = twitter_test.count()\n",
    "twitter_test_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter_test_n + twitter_train_n  == twitter_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie\n",
    "\n",
    "1. Wytrenować najlepszy model pod względem dokładności (accuracy).\n",
    "1. Można używać dowolnych kroków do stworzenia modelu, ale w zakresie działań z użyciem Spark (np. nie można używać scikit-learn).\n",
    "1. Można użyć dowolnego modelu, ale dostępnego w Spark MLlib.\n",
    "1. Sugeruję używanie nowego interfejsu `pyspark.ml`.\n",
    "1. Można używać innych narzędzi, ale tylko do ekploracji danych, np. robić wykresy w Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
