{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/kodolamaczlogo.png)\n",
    "\n",
    "# Przetwarzanie Big Data z użyciem Apache Spark\n",
    "\n",
    "Autor notebooka: Jakub Nowacki.\n",
    "\n",
    "## Spark 2.0\n",
    "\n",
    "Spark 2.0 wprowadził wiele optymalizacji związanych z prędkością działania; zobacz [wpis na blogu Databricks](https://databricks.com/blog/2016/07/26/introducing-apache-spark-2-0.html). \n",
    "\n",
    "Uporządkowane zostało równierz nieco API. Historycznie było wiele różnych obiektów do sterowania zadaniem: `SparkContext`, `SQLContext` czy `HiveContext`. W wersji Spark 2.0 zostały one wszystkie sprowadzone do obiektu `SparkSession`. Teraz zadania uruchamia się następująco:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .appName('spark_2_test') \\\n",
    "    .master('local[2]') \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|                    |\n",
      "|This Etext file i...|\n",
      "|cooperation with ...|\n",
      "|Future and Shakes...|\n",
      "|Etexts that are N...|\n",
      "|                    |\n",
      "|*This Etext has c...|\n",
      "|                    |\n",
      "|<<THIS ELECTRONIC...|\n",
      "|SHAKESPEARE IS CO...|\n",
      "|PROVIDED BY PROJE...|\n",
      "|MACHINE READABLE ...|\n",
      "|(1) ARE FOR YOUR ...|\n",
      "|DISTRIBUTED OR US...|\n",
      "|DISTRIBUTION INCL...|\n",
      "|TIME OR FOR MEMBE...|\n",
      "|                    |\n",
      "|*Project Gutenber...|\n",
      "|in the presentati...|\n",
      "|for your reading ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = spark.read.text('data/titus_andronicus.txt')\n",
    "lines.printSchema()\n",
    "lines.show()\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               words|\n",
      "+--------------------+\n",
      "|                  []|\n",
      "|[This, Etext, fil...|\n",
      "|[cooperation, wit...|\n",
      "|[Future, and, Sha...|\n",
      "|[Etexts, that, ar...|\n",
      "|                  []|\n",
      "|[, This, Etext, h...|\n",
      "|                  []|\n",
      "|[, THIS, ELECTRON...|\n",
      "|[SHAKESPEARE, IS,...|\n",
      "|[PROVIDED, BY, PR...|\n",
      "|[MACHINE, READABL...|\n",
      "|[, 1, ARE, FOR, Y...|\n",
      "|[DISTRIBUTED, OR,...|\n",
      "|[DISTRIBUTION, IN...|\n",
      "|[TIME, OR, FOR, M...|\n",
      "|                  []|\n",
      "|[, Project, Guten...|\n",
      "|[in, the, present...|\n",
      "|[for, your, readi...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = lines.select(func.split(lines.value, '\\W+').alias('words')) \n",
    "words.show()\n",
    "words.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|      words|\n",
      "+-----------+\n",
      "|           |\n",
      "|       This|\n",
      "|      Etext|\n",
      "|       file|\n",
      "|         is|\n",
      "|  presented|\n",
      "|         by|\n",
      "|    Project|\n",
      "|  Gutenberg|\n",
      "|         in|\n",
      "|cooperation|\n",
      "|       with|\n",
      "|      World|\n",
      "|    Library|\n",
      "|        Inc|\n",
      "|       from|\n",
      "|      their|\n",
      "|    Library|\n",
      "|         of|\n",
      "|        the|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = lines.select(func.explode(func.split(lines.value, '\\W+')).alias('words')) \n",
    "words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|      words|count|\n",
      "+-----------+-----+\n",
      "|       AWAY|    1|\n",
      "|         By|   16|\n",
      "|      those|    8|\n",
      "|irreligious|    2|\n",
      "|       hope|    9|\n",
      "|      Aside|   15|\n",
      "|      crest|    1|\n",
      "|        art|   21|\n",
      "|ingratitude|    2|\n",
      "|       some|   32|\n",
      "|        Sit|    2|\n",
      "|       Goth|    4|\n",
      "|     distil|    1|\n",
      "|      still|    7|\n",
      "|     ransom|    2|\n",
      "|        fog|    1|\n",
      "|     poetry|    1|\n",
      "|     Heaven|    1|\n",
      "|    blossom|    1|\n",
      "|      Virgo|    1|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counts = words.groupBy('words').count()\n",
    "counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|      words|\n",
      "+-----------+\n",
      "|       this|\n",
      "|      etext|\n",
      "|       file|\n",
      "|         is|\n",
      "|  presented|\n",
      "|         by|\n",
      "|    project|\n",
      "|  gutenberg|\n",
      "|         in|\n",
      "|cooperation|\n",
      "|       with|\n",
      "|      world|\n",
      "|    library|\n",
      "|        inc|\n",
      "|       from|\n",
      "|      their|\n",
      "|    library|\n",
      "|         of|\n",
      "|        the|\n",
      "|     future|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words.select(func.lower(func.col('words')).alias('words'))\\\n",
    "    .where(func.length('words') > 0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Column<b'CAST(length(words) AS STRING) AS `len_words`'>,\n",
       " pyspark.sql.column.Column,\n",
       " \"Column<b'CAST(length(m) AS STRING) AS `len_m`'>\")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def col_len(col_name):\n",
    "    return func.length(col_name).cast('string').alias('len_{}'.format(col_name))\n",
    "l = col_len('words')\n",
    "m = col_len('m')\n",
    "l, type(l), str(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+--------------------+\n",
      "|words_lower|count|            variants|\n",
      "+-----------+-----+--------------------+\n",
      "|       with|  284|  [with, WITH, With]|\n",
      "|   tribunes|   18|[tribunes, Tribun...|\n",
      "|       time|   21|  [time, TIME, Time]|\n",
      "|       this|  248|  [this, This, THIS]|\n",
      "|        the|  740|     [the, The, THE]|\n",
      "|         so|  113|        [so, SO, So]|\n",
      "|       send|   11|  [send, SEND, Send]|\n",
      "|   readable|   12|[readable, READAB...|\n",
      "|         or|  110|        [or, Or, OR]|\n",
      "|      money|    6|[money, Money, MO...|\n",
      "|        may|   60|     [may, May, MAY]|\n",
      "|       long|   25|  [long, Long, LONG]|\n",
      "|    library|   17|[library, Library...|\n",
      "|         it|  162|        [it, It, IT]|\n",
      "|         is|  200|        [is, IS, Is]|\n",
      "|        for|  262|     [for, For, FOR]|\n",
      "|     domain|    4|[domain, Domain, ...|\n",
      "|   complete|   16|[complete, Comple...|\n",
      "|         by|  131|        [by, BY, By]|\n",
      "|        but|  121|     [but, BUT, But]|\n",
      "+-----------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words.where(l > 0)\\\n",
    "    .withColumn('words_lower', func.lower(words['words']))\\\n",
    "    .groupBy('words_lower')\\\n",
    "    .agg(func.count('*').alias('count'), \n",
    "         func.collect_set('words').alias('variants'))\\\n",
    "    .orderBy(func.size('variants'), 'variants', ascending=False)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|words|\n",
      "+-----+\n",
      "| This|\n",
      "|their|\n",
      "|  the|\n",
      "| that|\n",
      "|  the|\n",
      "| This|\n",
      "| THIS|\n",
      "|  THE|\n",
      "| THAT|\n",
      "| TIME|\n",
      "|   to|\n",
      "|  The|\n",
      "|  the|\n",
      "|  The|\n",
      "| THIS|\n",
      "|  THE|\n",
      "|  THE|\n",
      "| THIS|\n",
      "|   TO|\n",
      "| THIS|\n",
      "+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words.where(words.words.rlike('^(t|T).*')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "func.trunc?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie\n",
    "\n",
    "Używając interfejsu i funkcji DataFrame lub SQL:\n",
    "\n",
    "1. Popraw jakość danych wejściowych i prezentację wyników\n",
    "1. Podaj liczność wyrazów zaczynających się od litery *t*\n",
    "1. ★ Wykonaj mapowanie używając Pythonowej funkcji długości `len` do obliczenia średniej długości wyrazu *(nie zalecane w praktyce)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co ze Spark Context?\n",
    "\n",
    "W razie potrzeby nadal jest dostępny w sesji Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark_2_test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=spark_2_test>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oczywiście dalej mamy też nadal dostępne RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(words=''),\n",
       " Row(words='This'),\n",
       " Row(words='Etext'),\n",
       " Row(words='file'),\n",
       " Row(words='is'),\n",
       " Row(words='presented'),\n",
       " Row(words='by'),\n",
       " Row(words='Project'),\n",
       " Row(words='Gutenberg'),\n",
       " Row(words='in')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nowe formaty plików\n",
    "\n",
    "Jedyną zmianą jest wprowadzenie formatu *CSV* jako wbudowanego; prezentowany powyżej format *text* jest dostępny od wersji 1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      " |-- _c20: string (nullable = true)\n",
      "\n",
      "+---+--------------------+--------------------+---+----+---+---+---+--------------------+------------+-----+----+----+----+-----+-----+----+---------+----+--------+----------+\n",
      "|_c0|                 _c1|                 _c2|_c3| _c4|_c5|_c6|_c7|                 _c8|         _c9| _c10|_c11|_c12|_c13| _c14| _c15|_c16|     _c17|_c18|    _c19|      _c20|\n",
      "+---+--------------------+--------------------+---+----+---+---+---+--------------------+------------+-----+----+----+----+-----+-----+----+---------+----+--------+----------+\n",
      "|  2|BATHGATE         ...|01  ONE FAMILY HO...|  1|3028| 25|   | A5| 412 EAST 179TH S...|            |10457|   1|   0|   1|1 842|2 048|1901|       1 | A5 |$355 000|  7/8/2013|\n",
      "|  2|BATHGATE         ...|01  ONE FAMILY HO...|  1|3039| 28|   | A1| 2329 WASHINGTON ...|            |10458|   1|   0|   1|1 103|1 290|1910|       1 | A1 |$474 819| 5/20/2013|\n",
      "|  2|BATHGATE         ...|01  ONE FAMILY HO...|  1|3046| 39|   | A1| 2075 BATHGATE AV...|            |10457|   1|   0|   1|1 986|1 344|1899|       1 | A1 |$210 000| 3/12/2013|\n",
      "|  2|BATHGATE         ...|01  ONE FAMILY HO...|  1|3046| 52|   | A1| 2047 BATHGATE AV...|            |10457|   1|   0|   1|2 329|1 431|1901|       1 | A1 |$343 116|  7/1/2013|\n",
      "|  2|BATHGATE         ...|02  TWO FAMILY HO...|  1|2900| 61|   | S2| 406 EAST TREMONT...|            |10457|   2|   1|   3|1 855|4 452|1931|       1 | S2 |      $0| 8/31/2012|\n",
      "|  2|BATHGATE         ...|02  TWO FAMILY HO...|  1|2912|158|   | B1| 505 EAST 171ST S...|            |10457|   2|   0|   2|2 000|2 400|1993|       1 | B1 |$316 500|12/27/2012|\n",
      "|  2|BATHGATE         ...|02  TWO FAMILY HO...|  1|2929|117|   | B1| 3860 3 AVENUE   ...|            |10457|   2|   0|   2|2 498|2 394|1995|       1 | B1 |$390 000| 6/24/2013|\n",
      "|  2|BATHGATE         ...|02  TWO FAMILY HO...|  1|3030| 60|   | B3| 4469 PARK AVENUE...|            |10457|   2|   0|   2|1 542|1 542|1899|       1 | B3 |$207 000| 6/27/2013|\n",
      "|  2|BATHGATE         ...|02  TWO FAMILY HO...|  1|3035| 27|   | B1| 454 EAST 179 STR...|            |10457|   2|   0|   2|1 819|2 340|1998|       1 | B1 |      $0|  7/1/2013|\n",
      "|  2|BATHGATE         ...|02  TWO FAMILY HO...|  1|3039| 65|   | B2| 465 EAST 185 STR...|            |10458|   2|   0|   2|1 667|1 296|1910|       1 | B2 |$369 000|  3/7/2013|\n",
      "|  2|BATHGATE         ...|02  TWO FAMILY HO...|  1|3040|  5|   | S2| 4654-4656 PARK A...|            |10458|   2|   1|   3|5 000|5 881|1910|       1 | S2 |$308 000| 2/28/2013|\n",
      "|  2|BATHGATE         ...|02  TWO FAMILY HO...|  1|3046| 54|   | B2| 2043 BATHGATE AV...|            |10457|   2|   0|   2|2 483|1 512|1901|       1 | B2 |      $0|  1/9/2013|\n",
      "|  2|BATHGATE         ...|02  TWO FAMILY HO...|  1|3050| 85|   | B1| 2241 BATHGATE AV...|            |10457|   2|   0|   2|1 562|3 382|2004|       1 | B1 |$443 776|10/15/2012|\n",
      "|  2|BATHGATE         ...|02  TWO FAMILY HO...|  1|3052| 37|   | S2| 4557 3 AVENUE   ...|            |10458|   2|   1|   3|  885|2 655|1931|       1 | S2 |      $0| 5/14/2013|\n",
      "|  2|BATHGATE         ...|03  THREE FAMILY ...|  1|3036| 33|   | C0| 2091 WASHINGTON ...|            |10457|   3|   0|   3|2 022|3 854|1899|       1 | C0 |      $0| 6/27/2013|\n",
      "|  2|BATHGATE         ...|03  THREE FAMILY ...|  1|3037| 37|   | C0| 4418 PARK AVENUE...|            |10457|   3|   0|   3|3 525|3 290|1899|         | C0 |$440 000| 2/15/2013|\n",
      "|  2|BATHGATE         ...|03  THREE FAMILY ...|  1|3038|125|   | C0| 468 EAST 183 STR...|            |10458|   3|   0|   3|1 216|2 916|2004|       1 | C0 |$662 115| 1/15/2013|\n",
      "|  2|BATHGATE         ...|03  THREE FAMILY ...|  1|3041| 11|   | C0| 452 CYRUS PLACE ...|            |10458|   3|   0|   3|3 742|2 280|1910|       1 | C0 |      $0| 7/25/2013|\n",
      "|  2|BATHGATE         ...|03  THREE FAMILY ...|  1|3050|  1|   | C0| 2186 WASHINGTON ...|            |10457|   3|   0|   3|1 505|2 640|1901|       1 | C0 |$460 000| 5/10/2013|\n",
      "|  2|BATHGATE         ...|03  THREE FAMILY ...|  1|3050|  1|   | C0| 2186 WASHINGTON ...|            |10457|   3|   0|   3|1 505|2 640|1901|       1 | C0 |$190 000| 8/27/2012|\n",
      "+---+--------------------+--------------------+---+----+---+---+---+--------------------+------------+-----+----+----+----+-----+-----+----+---------+----+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv = spark.read.csv('data/rollingsales_bronx.csv')\n",
    "csv.printSchema()\n",
    "csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------+---------+----+------+\n",
      "|    hood|                type|landArea|grossArea|year| price|\n",
      "+--------+--------------------+--------+---------+----+------+\n",
      "|BATHGATE|01  ONE FAMILY HOMES|    1842|     2048|1901|355000|\n",
      "|BATHGATE|01  ONE FAMILY HOMES|    1103|     1290|1910|474819|\n",
      "|BATHGATE|01  ONE FAMILY HOMES|    1986|     1344|1899|210000|\n",
      "|BATHGATE|01  ONE FAMILY HOMES|    2329|     1431|1901|343116|\n",
      "|BATHGATE|02  TWO FAMILY HOMES|    1855|     4452|1931|     0|\n",
      "|BATHGATE|02  TWO FAMILY HOMES|    2000|     2400|1993|316500|\n",
      "|BATHGATE|02  TWO FAMILY HOMES|    2498|     2394|1995|390000|\n",
      "|BATHGATE|02  TWO FAMILY HOMES|    1542|     1542|1899|207000|\n",
      "|BATHGATE|02  TWO FAMILY HOMES|    1819|     2340|1998|     0|\n",
      "|BATHGATE|02  TWO FAMILY HOMES|    1667|     1296|1910|369000|\n",
      "|BATHGATE|02  TWO FAMILY HOMES|    5000|     5881|1910|308000|\n",
      "|BATHGATE|02  TWO FAMILY HOMES|    2483|     1512|1901|     0|\n",
      "|BATHGATE|02  TWO FAMILY HOMES|    1562|     3382|2004|443776|\n",
      "|BATHGATE|02  TWO FAMILY HOMES|     885|     2655|1931|     0|\n",
      "|BATHGATE|03  THREE FAMILY ...|    2022|     3854|1899|     0|\n",
      "|BATHGATE|03  THREE FAMILY ...|    3525|     3290|1899|440000|\n",
      "|BATHGATE|03  THREE FAMILY ...|    1216|     2916|2004|662115|\n",
      "|BATHGATE|03  THREE FAMILY ...|    3742|     2280|1910|     0|\n",
      "|BATHGATE|03  THREE FAMILY ...|    1505|     2640|1901|460000|\n",
      "|BATHGATE|03  THREE FAMILY ...|    1505|     2640|1901|190000|\n",
      "+--------+--------------------+--------+---------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "houses = csv.select(\n",
    "    func.trim(func.col('_c1')).alias('hood'), \n",
    "    func.trim(func.col('_c2')).alias('type'),\n",
    "    func.regexp_replace(func.col('_c14'), '[^0-9.]', '').cast('int').alias('landArea'),\n",
    "    func.regexp_replace(func.col('_c15'), '[^0-9.]', '').cast('int').alias('grossArea'),\n",
    "    func.col('_c16').cast('int').alias('year'),\n",
    "    func.regexp_replace(func.col('_c19'), '[^0-9.]', '').cast('int') > 0.alias('price')\n",
    ")\n",
    "houses.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie\n",
    "\n",
    "Używając interfejsu i funkcji DataFrame lub SQL:\n",
    "\n",
    "1. Popraw żle odczytane wartości (*null*).\n",
    "1. Policz średnie wartości grupując po dzielnicy i typie.\n",
    "1. Zapisz wynik do pliku CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+-------------+----------------+---------------+\n",
      "|                type|BATHGATE_price|BATHGATE_year|BAYCHESTER_price|BAYCHESTER_year|\n",
      "+--------------------+--------------+-------------+----------------+---------------+\n",
      "|01  ONE FAMILY HOMES|        345733|         1902|          175829|           1941|\n",
      "|02  TWO FAMILY HOMES|        203427|         1947|          214003|           1961|\n",
      "|03  THREE FAMILY ...|        292019|         1919|          244380|           1978|\n",
      "|04  TAX CLASS 1 C...|          null|         null|          282164|           2008|\n",
      "|05  TAX CLASS 1 V...|         40730|            0|          134508|            402|\n",
      "|06  TAX CLASS 1 -...|          null|         null|            2500|           1800|\n",
      "|07  RENTALS - WAL...|        424286|         1924|           83333|           1990|\n",
      "|10  COOPS - ELEVA...|         27000|         1941|          129875|           1965|\n",
      "|14  RENTALS - 4-1...|             0|         1931|            null|           null|\n",
      "|21  OFFICE BUILDINGS|          null|         null|          598685|           1965|\n",
      "| 22  STORE BUILDINGS|        155384|         1936|          399250|           1953|\n",
      "|       27  FACTORIES|             0|         1997|               0|           1972|\n",
      "|28  COMMERCIAL CO...|          null|         null|            7605|           2008|\n",
      "|29  COMMERCIAL GA...|        605714|         1101|          769600|           1952|\n",
      "|      30  WAREHOUSES|       9733979|         1931|         1278333|           1955|\n",
      "|31  COMMERCIAL VA...|        909638|            0|            null|           null|\n",
      "|41  TAX CLASS 4 -...|        307500|         1457|          919468|           1955|\n",
      "+--------------------+--------------+-------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "houses.groupBy('type').pivot('hood', ['BATHGATE', 'BAYCHESTER'])\\\n",
    "    .agg(func.avg('price').cast('int').alias('price'), \n",
    "         func.avg('year').cast('int').alias('year'))\\\n",
    "    .orderBy('type') \\\n",
    "    .dropna('all', subset=['BATHGATE_price', 'BATHGATE_year', 'BAYCHESTER_price', 'BAYCHESTER_year'])\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column<b'hood AS `my_hood`'>,\n",
       " Column<b'type AS `my_type`'>,\n",
       " Column<b'landArea AS `my_landArea`'>,\n",
       " Column<b'grossArea AS `my_grossArea`'>,\n",
       " Column<b'year AS `my_year`'>,\n",
       " Column<b'price AS `my_price`'>]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[func.col(c).alias('my_{}'.format(c)) for c in houses.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podzapytania SQL\n",
    "\n",
    "Rozszeżony został również wachlarz dostępnych zapytań SQL, zwłaszcza użycie podzapytań, które pierwornie można było tylko uzywać z wyrażeniem `FROM`; zobacz [szczegóły na blogu Databricks](https://databricks.com/blog/2016/06/17/sql-subqueries-in-apache-spark-2-0.html). Zatem teraz można zrobić np takie zapytania:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tworzymy tymczasowy widok do zapytania SQL\n",
    "houses.createTempView('houses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------+---------+----+------+\n",
      "|    hood|                type|landArea|grossArea|year| price|\n",
      "+--------+--------------------+--------+---------+----+------+\n",
      "|BATHGATE|01  ONE FAMILY HOMES|    1842|     2048|1901|355000|\n",
      "|BATHGATE|01  ONE FAMILY HOMES|    1103|     1290|1910|474819|\n",
      "|BATHGATE|01  ONE FAMILY HOMES|    1986|     1344|1899|210000|\n",
      "|BATHGATE|01  ONE FAMILY HOMES|    2329|     1431|1901|343116|\n",
      "|BATHGATE|02  TWO FAMILY HOMES|    1855|     4452|1931|     0|\n",
      "|BATHGATE|02  TWO FAMILY HOMES|    2000|     2400|1993|316500|\n",
      "|BATHGATE|02  TWO FAMILY HOMES|    2498|     2394|1995|390000|\n",
      "|BATHGATE|02  TWO FAMILY HOMES|    1542|     1542|1899|207000|\n",
      "|BATHGATE|02  TWO FAMILY HOMES|    1819|     2340|1998|     0|\n",
      "|BATHGATE|02  TWO FAMILY HOMES|    1667|     1296|1910|369000|\n",
      "|BATHGATE|02  TWO FAMILY HOMES|    5000|     5881|1910|308000|\n",
      "|BATHGATE|02  TWO FAMILY HOMES|    2483|     1512|1901|     0|\n",
      "|BATHGATE|02  TWO FAMILY HOMES|    1562|     3382|2004|443776|\n",
      "|BATHGATE|02  TWO FAMILY HOMES|     885|     2655|1931|     0|\n",
      "|BATHGATE|03  THREE FAMILY ...|    2022|     3854|1899|     0|\n",
      "|BATHGATE|03  THREE FAMILY ...|    3525|     3290|1899|440000|\n",
      "|BATHGATE|03  THREE FAMILY ...|    1216|     2916|2004|662115|\n",
      "|BATHGATE|03  THREE FAMILY ...|    3742|     2280|1910|     0|\n",
      "|BATHGATE|03  THREE FAMILY ...|    1505|     2640|1901|460000|\n",
      "|BATHGATE|03  THREE FAMILY ...|    1505|     2640|1901|190000|\n",
      "+--------+--------------------+--------+---------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM houses \n",
    "WHERE year > (SELECT avg(year) FROM houses) \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------+---------+----+-------+\n",
      "|                hood|                type|landArea|grossArea|year|  price|\n",
      "+--------------------+--------------------+--------+---------+----+-------+\n",
      "|          BRONX PARK|02  TWO FAMILY HOMES|    2029|     4197|1915| 215000|\n",
      "|          BRONX PARK|29  COMMERCIAL GA...|   12500|     2500|1951|2200000|\n",
      "|CITY ISLAND-PELHA...|01  ONE FAMILY HOMES|   32202|     7122|1920|      0|\n",
      "|CITY ISLAND-PELHA...|01  ONE FAMILY HOMES|   19200|     2500|1975|1062500|\n",
      "|CITY ISLAND-PELHA...|01  ONE FAMILY HOMES|   12992|     1832|1975| 995000|\n",
      "|CITY ISLAND-PELHA...|01  ONE FAMILY HOMES|   24840|    10945|1965|      0|\n",
      "|CITY ISLAND-PELHA...|01  ONE FAMILY HOMES|    7500|     1848|1940| 635000|\n",
      "|CITY ISLAND-PELHA...|01  ONE FAMILY HOMES|   18227|     8421|1986|      0|\n",
      "|          CO-OP CITY|01  ONE FAMILY HOMES|    3750|     1080|1920| 175000|\n",
      "|          CO-OP CITY|02  TWO FAMILY HOMES|    1908|     2317|2011| 520000|\n",
      "|          CO-OP CITY|02  TWO FAMILY HOMES|    3750|     1664|1925| 335000|\n",
      "|          CO-OP CITY|   11A CONDO-RENTALS|       0|    96463|2013| 675000|\n",
      "|          CO-OP CITY|   11A CONDO-RENTALS|       0|    96463|2013|      0|\n",
      "|          CO-OP CITY|   11A CONDO-RENTALS|       0|    68948|2013|2800000|\n",
      "|          CO-OP CITY|29  COMMERCIAL GA...|   21025|     1941|1972|6350000|\n",
      "|      PELHAM GARDENS|01  ONE FAMILY HOMES|    4800|     2178|1940|      0|\n",
      "|      PELHAM GARDENS|01  ONE FAMILY HOMES|    3108|     2392|1950| 590000|\n",
      "|      PELHAM GARDENS|01  ONE FAMILY HOMES|    2500|     1520|1950| 450000|\n",
      "|      PELHAM GARDENS|01  ONE FAMILY HOMES|    4400|     1512|1940| 350000|\n",
      "|      PELHAM GARDENS|01  ONE FAMILY HOMES|    3333|     2001|1950| 530000|\n",
      "+--------------------+--------------------+--------+---------+----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM houses \n",
    "WHERE hood IN \n",
    "    (SELECT hood FROM\n",
    "        (SELECT hood, avg(year) FROM houses GROUP BY hood ORDER BY avg(year) DESC LIMIT 5)\n",
    "    ) \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nowy dostęp do katalogu tabel\n",
    "\n",
    "SparkSession udostępnia równierz zały katalog tabel, który jest podobny do metastore w Hive; przy połączeniu z Hive pojawią się też tabele z Hive. Katalog jest dostępny bezpośrednio z sesji Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# naciśnij tab\n",
    "spark.catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dostępna jest lista tabel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='houses', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie\n",
    "\n",
    "1. Zarejestruj DataFrame `counts` jako tymczasową tabelę; zobacz listę tabel.\n",
    "1. Usuń tabelę `counts`; zobacz listę tabel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='counts', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='houses', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.createOrReplaceTempView('counts')\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='houses', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.dropTempView('counts')\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dostępne sa też inne listy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databases:  [Database(name='default', description='Default Hive database', locationUri='file:/home/kodolamacz/Dokumenty/spark/spark-warehouse')]\n",
      "Functions:  [Function(name='!', description=None, className='org.apache.spark.sql.catalyst.expressions.Not', isTemporary=True), Function(name='%', description=None, className='org.apache.spark.sql.catalyst.expressions.Remainder', isTemporary=True), Function(name='&', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseAnd', isTemporary=True), Function(name='*', description=None, className='org.apache.spark.sql.catalyst.expressions.Multiply', isTemporary=True), Function(name='+', description=None, className='org.apache.spark.sql.catalyst.expressions.Add', isTemporary=True), Function(name='-', description=None, className='org.apache.spark.sql.catalyst.expressions.Subtract', isTemporary=True), Function(name='/', description=None, className='org.apache.spark.sql.catalyst.expressions.Divide', isTemporary=True), Function(name='<', description=None, className='org.apache.spark.sql.catalyst.expressions.LessThan', isTemporary=True), Function(name='<=', description=None, className='org.apache.spark.sql.catalyst.expressions.LessThanOrEqual', isTemporary=True), Function(name='<=>', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualNullSafe', isTemporary=True), Function(name='=', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualTo', isTemporary=True), Function(name='==', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualTo', isTemporary=True), Function(name='>', description=None, className='org.apache.spark.sql.catalyst.expressions.GreaterThan', isTemporary=True), Function(name='>=', description=None, className='org.apache.spark.sql.catalyst.expressions.GreaterThanOrEqual', isTemporary=True), Function(name='^', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseXor', isTemporary=True), Function(name='abs', description=None, className='org.apache.spark.sql.catalyst.expressions.Abs', isTemporary=True), Function(name='acos', description=None, className='org.apache.spark.sql.catalyst.expressions.Acos', isTemporary=True), Function(name='add_months', description=None, className='org.apache.spark.sql.catalyst.expressions.AddMonths', isTemporary=True), Function(name='and', description=None, className='org.apache.spark.sql.catalyst.expressions.And', isTemporary=True), Function(name='approx_count_distinct', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.HyperLogLogPlusPlus', isTemporary=True), Function(name='approx_percentile', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentile', isTemporary=True), Function(name='array', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateArray', isTemporary=True), Function(name='array_contains', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayContains', isTemporary=True), Function(name='ascii', description=None, className='org.apache.spark.sql.catalyst.expressions.Ascii', isTemporary=True), Function(name='asin', description=None, className='org.apache.spark.sql.catalyst.expressions.Asin', isTemporary=True), Function(name='assert_true', description=None, className='org.apache.spark.sql.catalyst.expressions.AssertTrue', isTemporary=True), Function(name='atan', description=None, className='org.apache.spark.sql.catalyst.expressions.Atan', isTemporary=True), Function(name='atan2', description=None, className='org.apache.spark.sql.catalyst.expressions.Atan2', isTemporary=True), Function(name='avg', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Average', isTemporary=True), Function(name='base64', description=None, className='org.apache.spark.sql.catalyst.expressions.Base64', isTemporary=True), Function(name='bigint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True), Function(name='bin', description=None, className='org.apache.spark.sql.catalyst.expressions.Bin', isTemporary=True), Function(name='binary', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True), Function(name='boolean', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True), Function(name='bround', description=None, className='org.apache.spark.sql.catalyst.expressions.BRound', isTemporary=True), Function(name='cast', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True), Function(name='cbrt', description=None, className='org.apache.spark.sql.catalyst.expressions.Cbrt', isTemporary=True), Function(name='ceil', description=None, className='org.apache.spark.sql.catalyst.expressions.Ceil', isTemporary=True), Function(name='ceiling', description=None, className='org.apache.spark.sql.catalyst.expressions.Ceil', isTemporary=True), Function(name='coalesce', description=None, className='org.apache.spark.sql.catalyst.expressions.Coalesce', isTemporary=True), Function(name='collect_list', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CollectList', isTemporary=True), Function(name='collect_set', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CollectSet', isTemporary=True), Function(name='concat', description=None, className='org.apache.spark.sql.catalyst.expressions.Concat', isTemporary=True), Function(name='concat_ws', description=None, className='org.apache.spark.sql.catalyst.expressions.ConcatWs', isTemporary=True), Function(name='conv', description=None, className='org.apache.spark.sql.catalyst.expressions.Conv', isTemporary=True), Function(name='corr', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Corr', isTemporary=True), Function(name='cos', description=None, className='org.apache.spark.sql.catalyst.expressions.Cos', isTemporary=True), Function(name='cosh', description=None, className='org.apache.spark.sql.catalyst.expressions.Cosh', isTemporary=True), Function(name='count', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Count', isTemporary=True), Function(name='count_min_sketch', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CountMinSketchAgg', isTemporary=True), Function(name='covar_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CovPopulation', isTemporary=True), Function(name='covar_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CovSample', isTemporary=True), Function(name='crc32', description=None, className='org.apache.spark.sql.catalyst.expressions.Crc32', isTemporary=True), Function(name='cube', description=None, className='org.apache.spark.sql.catalyst.expressions.Cube', isTemporary=True), Function(name='cume_dist', description=None, className='org.apache.spark.sql.catalyst.expressions.CumeDist', isTemporary=True), Function(name='current_database', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentDatabase', isTemporary=True), Function(name='current_date', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentDate', isTemporary=True), Function(name='current_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentTimestamp', isTemporary=True), Function(name='date', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True), Function(name='date_add', description=None, className='org.apache.spark.sql.catalyst.expressions.DateAdd', isTemporary=True), Function(name='date_format', description=None, className='org.apache.spark.sql.catalyst.expressions.DateFormatClass', isTemporary=True), Function(name='date_sub', description=None, className='org.apache.spark.sql.catalyst.expressions.DateSub', isTemporary=True), Function(name='datediff', description=None, className='org.apache.spark.sql.catalyst.expressions.DateDiff', isTemporary=True), Function(name='day', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfMonth', isTemporary=True), Function(name='dayofmonth', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfMonth', isTemporary=True), Function(name='dayofyear', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfYear', isTemporary=True), Function(name='decimal', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True), Function(name='decode', description=None, className='org.apache.spark.sql.catalyst.expressions.Decode', isTemporary=True), Function(name='degrees', description=None, className='org.apache.spark.sql.catalyst.expressions.ToDegrees', isTemporary=True), Function(name='dense_rank', description=None, className='org.apache.spark.sql.catalyst.expressions.DenseRank', isTemporary=True), Function(name='double', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True), Function(name='e', description=None, className='org.apache.spark.sql.catalyst.expressions.EulerNumber', isTemporary=True), Function(name='elt', description=None, className='org.apache.spark.sql.catalyst.expressions.Elt', isTemporary=True), Function(name='encode', description=None, className='org.apache.spark.sql.catalyst.expressions.Encode', isTemporary=True), Function(name='exp', description=None, className='org.apache.spark.sql.catalyst.expressions.Exp', isTemporary=True), Function(name='explode', description=None, className='org.apache.spark.sql.catalyst.expressions.Explode', isTemporary=True), Function(name='explode_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.Explode', isTemporary=True), Function(name='expm1', description=None, className='org.apache.spark.sql.catalyst.expressions.Expm1', isTemporary=True), Function(name='factorial', description=None, className='org.apache.spark.sql.catalyst.expressions.Factorial', isTemporary=True), Function(name='find_in_set', description=None, className='org.apache.spark.sql.catalyst.expressions.FindInSet', isTemporary=True), Function(name='first', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.First', isTemporary=True), Function(name='first_value', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.First', isTemporary=True), Function(name='float', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True), Function(name='floor', description=None, className='org.apache.spark.sql.catalyst.expressions.Floor', isTemporary=True), Function(name='format_number', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatNumber', isTemporary=True), Function(name='format_string', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatString', isTemporary=True), Function(name='from_json', description=None, className='org.apache.spark.sql.catalyst.expressions.JsonToStructs', isTemporary=True), Function(name='from_unixtime', description=None, className='org.apache.spark.sql.catalyst.expressions.FromUnixTime', isTemporary=True), Function(name='from_utc_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.FromUTCTimestamp', isTemporary=True), Function(name='get_json_object', description=None, className='org.apache.spark.sql.catalyst.expressions.GetJsonObject', isTemporary=True), Function(name='greatest', description=None, className='org.apache.spark.sql.catalyst.expressions.Greatest', isTemporary=True), Function(name='grouping', description=None, className='org.apache.spark.sql.catalyst.expressions.Grouping', isTemporary=True), Function(name='grouping_id', description=None, className='org.apache.spark.sql.catalyst.expressions.GroupingID', isTemporary=True), Function(name='hash', description=None, className='org.apache.spark.sql.catalyst.expressions.Murmur3Hash', isTemporary=True), Function(name='hex', description=None, className='org.apache.spark.sql.catalyst.expressions.Hex', isTemporary=True), Function(name='hour', description=None, className='org.apache.spark.sql.catalyst.expressions.Hour', isTemporary=True), Function(name='hypot', description=None, className='org.apache.spark.sql.catalyst.expressions.Hypot', isTemporary=True), Function(name='if', description=None, className='org.apache.spark.sql.catalyst.expressions.If', isTemporary=True), Function(name='ifnull', description=None, className='org.apache.spark.sql.catalyst.expressions.IfNull', isTemporary=True), Function(name='in', description=None, className='org.apache.spark.sql.catalyst.expressions.In', isTemporary=True), Function(name='initcap', description=None, className='org.apache.spark.sql.catalyst.expressions.InitCap', isTemporary=True), Function(name='inline', description=None, className='org.apache.spark.sql.catalyst.expressions.Inline', isTemporary=True), Function(name='inline_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.Inline', isTemporary=True), Function(name='input_file_block_length', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileBlockLength', isTemporary=True), Function(name='input_file_block_start', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileBlockStart', isTemporary=True), Function(name='input_file_name', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileName', isTemporary=True), Function(name='instr', description=None, className='org.apache.spark.sql.catalyst.expressions.StringInstr', isTemporary=True), Function(name='int', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True), Function(name='isnan', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNaN', isTemporary=True), Function(name='isnotnull', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNotNull', isTemporary=True), Function(name='isnull', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNull', isTemporary=True), Function(name='java_method', description=None, className='org.apache.spark.sql.catalyst.expressions.CallMethodViaReflection', isTemporary=True), Function(name='json_tuple', description=None, className='org.apache.spark.sql.catalyst.expressions.JsonTuple', isTemporary=True), Function(name='kurtosis', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Kurtosis', isTemporary=True), Function(name='lag', description=None, className='org.apache.spark.sql.catalyst.expressions.Lag', isTemporary=True), Function(name='last', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Last', isTemporary=True), Function(name='last_day', description=None, className='org.apache.spark.sql.catalyst.expressions.LastDay', isTemporary=True), Function(name='last_value', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Last', isTemporary=True), Function(name='lcase', description=None, className='org.apache.spark.sql.catalyst.expressions.Lower', isTemporary=True), Function(name='lead', description=None, className='org.apache.spark.sql.catalyst.expressions.Lead', isTemporary=True), Function(name='least', description=None, className='org.apache.spark.sql.catalyst.expressions.Least', isTemporary=True), Function(name='length', description=None, className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True), Function(name='levenshtein', description=None, className='org.apache.spark.sql.catalyst.expressions.Levenshtein', isTemporary=True), Function(name='like', description=None, className='org.apache.spark.sql.catalyst.expressions.Like', isTemporary=True), Function(name='ln', description=None, className='org.apache.spark.sql.catalyst.expressions.Log', isTemporary=True), Function(name='locate', description=None, className='org.apache.spark.sql.catalyst.expressions.StringLocate', isTemporary=True), Function(name='log', description=None, className='org.apache.spark.sql.catalyst.expressions.Logarithm', isTemporary=True), Function(name='log10', description=None, className='org.apache.spark.sql.catalyst.expressions.Log10', isTemporary=True), Function(name='log1p', description=None, className='org.apache.spark.sql.catalyst.expressions.Log1p', isTemporary=True), Function(name='log2', description=None, className='org.apache.spark.sql.catalyst.expressions.Log2', isTemporary=True), Function(name='lower', description=None, className='org.apache.spark.sql.catalyst.expressions.Lower', isTemporary=True), Function(name='lpad', description=None, className='org.apache.spark.sql.catalyst.expressions.StringLPad', isTemporary=True), Function(name='ltrim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrimLeft', isTemporary=True), Function(name='map', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateMap', isTemporary=True), Function(name='map_keys', description=None, className='org.apache.spark.sql.catalyst.expressions.MapKeys', isTemporary=True), Function(name='map_values', description=None, className='org.apache.spark.sql.catalyst.expressions.MapValues', isTemporary=True), Function(name='max', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Max', isTemporary=True), Function(name='md5', description=None, className='org.apache.spark.sql.catalyst.expressions.Md5', isTemporary=True), Function(name='mean', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Average', isTemporary=True), Function(name='min', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Min', isTemporary=True), Function(name='minute', description=None, className='org.apache.spark.sql.catalyst.expressions.Minute', isTemporary=True), Function(name='monotonically_increasing_id', description=None, className='org.apache.spark.sql.catalyst.expressions.MonotonicallyIncreasingID', isTemporary=True), Function(name='month', description=None, className='org.apache.spark.sql.catalyst.expressions.Month', isTemporary=True), Function(name='months_between', description=None, className='org.apache.spark.sql.catalyst.expressions.MonthsBetween', isTemporary=True), Function(name='named_struct', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateNamedStruct', isTemporary=True), Function(name='nanvl', description=None, className='org.apache.spark.sql.catalyst.expressions.NaNvl', isTemporary=True), Function(name='negative', description=None, className='org.apache.spark.sql.catalyst.expressions.UnaryMinus', isTemporary=True), Function(name='next_day', description=None, className='org.apache.spark.sql.catalyst.expressions.NextDay', isTemporary=True), Function(name='not', description=None, className='org.apache.spark.sql.catalyst.expressions.Not', isTemporary=True), Function(name='now', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentTimestamp', isTemporary=True), Function(name='ntile', description=None, className='org.apache.spark.sql.catalyst.expressions.NTile', isTemporary=True), Function(name='nullif', description=None, className='org.apache.spark.sql.catalyst.expressions.NullIf', isTemporary=True), Function(name='nvl', description=None, className='org.apache.spark.sql.catalyst.expressions.Nvl', isTemporary=True), Function(name='nvl2', description=None, className='org.apache.spark.sql.catalyst.expressions.Nvl2', isTemporary=True), Function(name='or', description=None, className='org.apache.spark.sql.catalyst.expressions.Or', isTemporary=True), Function(name='parse_url', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseUrl', isTemporary=True), Function(name='percent_rank', description=None, className='org.apache.spark.sql.catalyst.expressions.PercentRank', isTemporary=True), Function(name='percentile', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Percentile', isTemporary=True), Function(name='percentile_approx', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentile', isTemporary=True), Function(name='pi', description=None, className='org.apache.spark.sql.catalyst.expressions.Pi', isTemporary=True), Function(name='pmod', description=None, className='org.apache.spark.sql.catalyst.expressions.Pmod', isTemporary=True), Function(name='posexplode', description=None, className='org.apache.spark.sql.catalyst.expressions.PosExplode', isTemporary=True), Function(name='posexplode_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.PosExplode', isTemporary=True), Function(name='positive', description=None, className='org.apache.spark.sql.catalyst.expressions.UnaryPositive', isTemporary=True), Function(name='pow', description=None, className='org.apache.spark.sql.catalyst.expressions.Pow', isTemporary=True), Function(name='power', description=None, className='org.apache.spark.sql.catalyst.expressions.Pow', isTemporary=True), Function(name='printf', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatString', isTemporary=True), Function(name='quarter', description=None, className='org.apache.spark.sql.catalyst.expressions.Quarter', isTemporary=True), Function(name='radians', description=None, className='org.apache.spark.sql.catalyst.expressions.ToRadians', isTemporary=True), Function(name='rand', description=None, className='org.apache.spark.sql.catalyst.expressions.Rand', isTemporary=True), Function(name='randn', description=None, className='org.apache.spark.sql.catalyst.expressions.Randn', isTemporary=True), Function(name='rank', description=None, className='org.apache.spark.sql.catalyst.expressions.Rank', isTemporary=True), Function(name='reflect', description=None, className='org.apache.spark.sql.catalyst.expressions.CallMethodViaReflection', isTemporary=True), Function(name='regexp_extract', description=None, className='org.apache.spark.sql.catalyst.expressions.RegExpExtract', isTemporary=True), Function(name='regexp_replace', description=None, className='org.apache.spark.sql.catalyst.expressions.RegExpReplace', isTemporary=True), Function(name='repeat', description=None, className='org.apache.spark.sql.catalyst.expressions.StringRepeat', isTemporary=True), Function(name='reverse', description=None, className='org.apache.spark.sql.catalyst.expressions.StringReverse', isTemporary=True), Function(name='rint', description=None, className='org.apache.spark.sql.catalyst.expressions.Rint', isTemporary=True), Function(name='rlike', description=None, className='org.apache.spark.sql.catalyst.expressions.RLike', isTemporary=True), Function(name='rollup', description=None, className='org.apache.spark.sql.catalyst.expressions.Rollup', isTemporary=True), Function(name='round', description=None, className='org.apache.spark.sql.catalyst.expressions.Round', isTemporary=True), Function(name='row_number', description=None, className='org.apache.spark.sql.catalyst.expressions.RowNumber', isTemporary=True), Function(name='rpad', description=None, className='org.apache.spark.sql.catalyst.expressions.StringRPad', isTemporary=True), Function(name='rtrim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrimRight', isTemporary=True), Function(name='second', description=None, className='org.apache.spark.sql.catalyst.expressions.Second', isTemporary=True), Function(name='sentences', description=None, className='org.apache.spark.sql.catalyst.expressions.Sentences', isTemporary=True), Function(name='sha', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha1', isTemporary=True), Function(name='sha1', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha1', isTemporary=True), Function(name='sha2', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha2', isTemporary=True), Function(name='shiftleft', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftLeft', isTemporary=True), Function(name='shiftright', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftRight', isTemporary=True), Function(name='shiftrightunsigned', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftRightUnsigned', isTemporary=True), Function(name='sign', description=None, className='org.apache.spark.sql.catalyst.expressions.Signum', isTemporary=True), Function(name='signum', description=None, className='org.apache.spark.sql.catalyst.expressions.Signum', isTemporary=True), Function(name='sin', description=None, className='org.apache.spark.sql.catalyst.expressions.Sin', isTemporary=True), Function(name='sinh', description=None, className='org.apache.spark.sql.catalyst.expressions.Sinh', isTemporary=True), Function(name='size', description=None, className='org.apache.spark.sql.catalyst.expressions.Size', isTemporary=True), Function(name='skewness', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Skewness', isTemporary=True), Function(name='smallint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True), Function(name='sort_array', description=None, className='org.apache.spark.sql.catalyst.expressions.SortArray', isTemporary=True), Function(name='soundex', description=None, className='org.apache.spark.sql.catalyst.expressions.SoundEx', isTemporary=True), Function(name='space', description=None, className='org.apache.spark.sql.catalyst.expressions.StringSpace', isTemporary=True), Function(name='spark_partition_id', description=None, className='org.apache.spark.sql.catalyst.expressions.SparkPartitionID', isTemporary=True), Function(name='split', description=None, className='org.apache.spark.sql.catalyst.expressions.StringSplit', isTemporary=True), Function(name='sqrt', description=None, className='org.apache.spark.sql.catalyst.expressions.Sqrt', isTemporary=True), Function(name='stack', description=None, className='org.apache.spark.sql.catalyst.expressions.Stack', isTemporary=True), Function(name='std', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True), Function(name='stddev', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True), Function(name='stddev_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevPop', isTemporary=True), Function(name='stddev_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True), Function(name='str_to_map', description=None, className='org.apache.spark.sql.catalyst.expressions.StringToMap', isTemporary=True), Function(name='string', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True), Function(name='struct', description=None, className='org.apache.spark.sql.catalyst.expressions.NamedStruct', isTemporary=True), Function(name='substr', description=None, className='org.apache.spark.sql.catalyst.expressions.Substring', isTemporary=True), Function(name='substring', description=None, className='org.apache.spark.sql.catalyst.expressions.Substring', isTemporary=True), Function(name='substring_index', description=None, className='org.apache.spark.sql.catalyst.expressions.SubstringIndex', isTemporary=True), Function(name='sum', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Sum', isTemporary=True), Function(name='tan', description=None, className='org.apache.spark.sql.catalyst.expressions.Tan', isTemporary=True), Function(name='tanh', description=None, className='org.apache.spark.sql.catalyst.expressions.Tanh', isTemporary=True), Function(name='timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True), Function(name='tinyint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True), Function(name='to_date', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseToDate', isTemporary=True), Function(name='to_json', description=None, className='org.apache.spark.sql.catalyst.expressions.StructsToJson', isTemporary=True), Function(name='to_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseToTimestamp', isTemporary=True), Function(name='to_unix_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ToUnixTimestamp', isTemporary=True), Function(name='to_utc_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ToUTCTimestamp', isTemporary=True), Function(name='translate', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTranslate', isTemporary=True), Function(name='trim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrim', isTemporary=True), Function(name='trunc', description=None, className='org.apache.spark.sql.catalyst.expressions.TruncDate', isTemporary=True), Function(name='ucase', description=None, className='org.apache.spark.sql.catalyst.expressions.Upper', isTemporary=True), Function(name='unbase64', description=None, className='org.apache.spark.sql.catalyst.expressions.UnBase64', isTemporary=True), Function(name='unhex', description=None, className='org.apache.spark.sql.catalyst.expressions.Unhex', isTemporary=True), Function(name='unix_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixTimestamp', isTemporary=True), Function(name='upper', description=None, className='org.apache.spark.sql.catalyst.expressions.Upper', isTemporary=True), Function(name='var_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VariancePop', isTemporary=True), Function(name='var_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VarianceSamp', isTemporary=True), Function(name='variance', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VarianceSamp', isTemporary=True), Function(name='weekofyear', description=None, className='org.apache.spark.sql.catalyst.expressions.WeekOfYear', isTemporary=True), Function(name='when', description=None, className='org.apache.spark.sql.catalyst.expressions.CaseWhen', isTemporary=True), Function(name='window', description=None, className='org.apache.spark.sql.catalyst.expressions.TimeWindow', isTemporary=True), Function(name='xpath', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathList', isTemporary=True), Function(name='xpath_boolean', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathBoolean', isTemporary=True), Function(name='xpath_double', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathDouble', isTemporary=True), Function(name='xpath_float', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathFloat', isTemporary=True), Function(name='xpath_int', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathInt', isTemporary=True), Function(name='xpath_long', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathLong', isTemporary=True), Function(name='xpath_number', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathDouble', isTemporary=True), Function(name='xpath_short', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathShort', isTemporary=True), Function(name='xpath_string', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathString', isTemporary=True), Function(name='year', description=None, className='org.apache.spark.sql.catalyst.expressions.Year', isTemporary=True), Function(name='|', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseOr', isTemporary=True), Function(name='~', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseNot', isTemporary=True)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Databases: \", spark.catalog.listDatabases())\n",
    "print(\"Functions: \", spark.catalog.listFunctions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Można też łatwiej zmienić bazę danych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"Database 'login' does not exist.;\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/home/kodolamacz/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kodolamacz/spark/python/lib/py4j-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1022.setCurrentDatabase.\n: org.apache.spark.sql.AnalysisException: Database 'login' does not exist.;\n\tat org.apache.spark.sql.internal.CatalogImpl.requireDatabaseExists(CatalogImpl.scala:44)\n\tat org.apache.spark.sql.internal.CatalogImpl.setCurrentDatabase(CatalogImpl.scala:64)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-56659fd666dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# zmień na swój login\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetCurrentDatabase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'login'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# poniższa funkcja tylko wypisuje obecną bazę danych\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrentDatabase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kodolamacz/spark/python/pyspark/sql/catalog.py\u001b[0m in \u001b[0;36msetCurrentDatabase\u001b[0;34m(self, dbName)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetCurrentDatabase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;34m\"\"\"Sets the current default database in this session.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetCurrentDatabase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdbName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kodolamacz/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kodolamacz/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"Database 'login' does not exist.;\""
     ]
    }
   ],
   "source": [
    "# zmień na swój login\n",
    "spark.catalog.setCurrentDatabase('login') \n",
    "# poniższa funkcja tylko wypisuje obecną bazę danych\n",
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------+---------+----+------+\n",
      "|    hood|                type|landArea|grossArea|year| price|\n",
      "+--------+--------------------+--------+---------+----+------+\n",
      "|BATHGATE|01  ONE FAMILY HOMES|    1842|     2048|1901|355000|\n",
      "|BATHGATE|01  ONE FAMILY HOMES|    1103|     1290|1910|474819|\n",
      "|BATHGATE|01  ONE FAMILY HOMES|    1986|     1344|1899|210000|\n",
      "|BATHGATE|01  ONE FAMILY HOMES|    2329|     1431|1901|343116|\n",
      "|BATHGATE|02  TWO FAMILY HOMES|    1855|     4452|1931|     0|\n",
      "|BATHGATE|02  TWO FAMILY HOMES|    2000|     2400|1993|316500|\n",
      "|BATHGATE|02  TWO FAMILY HOMES|    2498|     2394|1995|390000|\n",
      "|BATHGATE|02  TWO FAMILY HOMES|    1542|     1542|1899|207000|\n",
      "|BATHGATE|02  TWO FAMILY HOMES|    1819|     2340|1998|     0|\n",
      "|BATHGATE|02  TWO FAMILY HOMES|    1667|     1296|1910|369000|\n",
      "|BATHGATE|02  TWO FAMILY HOMES|    5000|     5881|1910|308000|\n",
      "|BATHGATE|02  TWO FAMILY HOMES|    2483|     1512|1901|     0|\n",
      "|BATHGATE|02  TWO FAMILY HOMES|    1562|     3382|2004|443776|\n",
      "|BATHGATE|02  TWO FAMILY HOMES|     885|     2655|1931|     0|\n",
      "|BATHGATE|03  THREE FAMILY ...|    2022|     3854|1899|     0|\n",
      "|BATHGATE|03  THREE FAMILY ...|    3525|     3290|1899|440000|\n",
      "|BATHGATE|03  THREE FAMILY ...|    1216|     2916|2004|662115|\n",
      "|BATHGATE|03  THREE FAMILY ...|    3742|     2280|1910|     0|\n",
      "|BATHGATE|03  THREE FAMILY ...|    1505|     2640|1901|460000|\n",
      "|BATHGATE|03  THREE FAMILY ...|    1505|     2640|1901|190000|\n",
      "+--------+--------------------+--------+---------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "h = spark.table('houses')\n",
    "h.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h.write.partitionBy('hood').csv('houses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+----+----+------+--------+\n",
      "|                 _c0| _c1| _c2| _c3|   _c4|    hood|\n",
      "+--------------------+----+----+----+------+--------+\n",
      "|01  ONE FAMILY HOMES|1842|2048|1901|355000|BATHGATE|\n",
      "|01  ONE FAMILY HOMES|1103|1290|1910|474819|BATHGATE|\n",
      "|01  ONE FAMILY HOMES|1986|1344|1899|210000|BATHGATE|\n",
      "|01  ONE FAMILY HOMES|2329|1431|1901|343116|BATHGATE|\n",
      "|02  TWO FAMILY HOMES|1855|4452|1931|     0|BATHGATE|\n",
      "|02  TWO FAMILY HOMES|2000|2400|1993|316500|BATHGATE|\n",
      "|02  TWO FAMILY HOMES|2498|2394|1995|390000|BATHGATE|\n",
      "|02  TWO FAMILY HOMES|1542|1542|1899|207000|BATHGATE|\n",
      "|02  TWO FAMILY HOMES|1819|2340|1998|     0|BATHGATE|\n",
      "|02  TWO FAMILY HOMES|1667|1296|1910|369000|BATHGATE|\n",
      "|02  TWO FAMILY HOMES|5000|5881|1910|308000|BATHGATE|\n",
      "|02  TWO FAMILY HOMES|2483|1512|1901|     0|BATHGATE|\n",
      "|02  TWO FAMILY HOMES|1562|3382|2004|443776|BATHGATE|\n",
      "|02  TWO FAMILY HOMES| 885|2655|1931|     0|BATHGATE|\n",
      "|03  THREE FAMILY ...|2022|3854|1899|     0|BATHGATE|\n",
      "|03  THREE FAMILY ...|3525|3290|1899|440000|BATHGATE|\n",
      "|03  THREE FAMILY ...|1216|2916|2004|662115|BATHGATE|\n",
      "|03  THREE FAMILY ...|3742|2280|1910|     0|BATHGATE|\n",
      "|03  THREE FAMILY ...|1505|2640|1901|460000|BATHGATE|\n",
      "|03  THREE FAMILY ...|1505|2640|1901|190000|BATHGATE|\n",
      "+--------------------+----+----+----+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv('houses.csv').where(func.col('hood') == 'BATHGATE').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
