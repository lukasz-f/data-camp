{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/kodolamaczlogo.png)\n",
    "\n",
    "# Przetwarzanie Big Data z użyciem Apache Spark\n",
    "\n",
    "Autor notebooka: Jakub Nowacki.\n",
    "\n",
    "\n",
    "## Podstawy Spark SQL - UDF\n",
    "\n",
    "Podobnie jak w Hive czy wielu bazach danych, Spark SQL ma możliwość definiowania funkcji użytkownika, ang. User Defined Functions (UDF). Funkcje te biorą wartość z kolumny i przekształcają ją w inną wartość. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as func\n",
    "import pyspark.sql.types as types\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .appName('udf') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#sc = pyspark.SparkContext(appName='udf')\n",
    "#sqlContext = pyspark.sql.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wygenerujmy najpierw dane:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- kolumna: long (nullable = true)\n",
      "\n",
      "+-------+\n",
      "|kolumna|\n",
      "+-------+\n",
      "|     99|\n",
      "|     90|\n",
      "|     56|\n",
      "|     89|\n",
      "|     15|\n",
      "|     46|\n",
      "|     85|\n",
      "|     23|\n",
      "|     47|\n",
      "|      4|\n",
      "|     61|\n",
      "|     25|\n",
      "|      3|\n",
      "|     95|\n",
      "|     27|\n",
      "|     46|\n",
      "|     43|\n",
      "|     97|\n",
      "|     96|\n",
      "|     64|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "df = spark.createDataFrame([pyspark.sql.Row(kolumna=int(i)) for i in np.random.randint(0, 100, 100)])\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do stworzenia funkcji używamy normalnej referencji do funkcji Python lub lambdy. Przykładowo, chcemy funkcję która zwróci klasyfikacje wartości:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def klasyfikuj(wartosc):\n",
    "    return u'dużo' if wartosc > 50 else u'mało'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Powyższą funkcję należy teraz przekształcić w funkcje która działa na kolumnach. Robimy to używając funkcji `udf`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "klasyfikuj_udf = func.udf(klasyfikuj)\n",
    "type(klasyfikuj_udf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tej funkcji można już użyć na kolumnie DataFrame, np możemy dodać kolumnę:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|kolumna|ile?|\n",
      "+-------+----+\n",
      "|     99|dużo|\n",
      "|     90|dużo|\n",
      "|     56|dużo|\n",
      "|     89|dużo|\n",
      "|     15|mało|\n",
      "|     46|mało|\n",
      "|     85|dużo|\n",
      "|     23|mało|\n",
      "|     47|mało|\n",
      "|      4|mało|\n",
      "|     61|dużo|\n",
      "|     25|mało|\n",
      "|      3|mało|\n",
      "|     95|dużo|\n",
      "|     27|mało|\n",
      "|     46|mało|\n",
      "|     43|mało|\n",
      "|     97|dużo|\n",
      "|     96|dużo|\n",
      "|     64|dużo|\n",
      "+-------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('ile?', klasyfikuj_udf('kolumna')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Należu uważać z typami; jeżeli chcemy zwrócić inny typ niż tekstowy, należy przekazać tą informację w definicji funkcji UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "oblicz_udf = func.udf(lambda v: v + 123, returnType=types.IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|kolumna|oblicz|\n",
      "+-------+------+\n",
      "|     99|   222|\n",
      "|     90|   213|\n",
      "|     56|   179|\n",
      "|     89|   212|\n",
      "|     15|   138|\n",
      "|     46|   169|\n",
      "|     85|   208|\n",
      "|     23|   146|\n",
      "|     47|   170|\n",
      "|      4|   127|\n",
      "|     61|   184|\n",
      "|     25|   148|\n",
      "|      3|   126|\n",
      "|     95|   218|\n",
      "|     27|   150|\n",
      "|     46|   169|\n",
      "|     43|   166|\n",
      "|     97|   220|\n",
      "|     96|   219|\n",
      "|     64|   187|\n",
      "+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('oblicz', oblicz_udf('kolumna')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aby wykorzystać funckje w zapytaniach SQL należy ją zarejestrować nieco inaczej, mianowicie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.udf.register('klasyfikuj', klasyfikuj, returnType=types.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable('df')\n",
    "spark.sql('SELECT kolumna, klasyfikuj(kolumna) AS `ile?` FROM df').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dostęp do JVM\n",
    "\n",
    "Spark używa [py4j](https://www.py4j.org/) aby wykonywać komendy na JVM. Dostęp do klas mamy w PySpark nieco ułatwiony, mianowicie używamy `_jvm` ze `SparkContext` podając pełną nazwę klasy z pakietem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = spark.sparkContext._jvm.java.lang.String('tekst')\n",
    "print(s, type(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać, py4j konwertuje typy. Możemy też użyć bardziej skomplikowanych obiektów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = spark.sparkContext._jvm.java.util.StringTokenizer('Ala ma kota!')\n",
    "print(tokenizer, type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(tokenizer.hasMoreTokens()):\n",
    "    print(tokenizer.nextToken())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lub prościej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = spark.sparkContext._jvm.java.util.regex.Pattern.compile('\\\\s')\n",
    "print(p, type(p))\n",
    "l = p.split('Ala ma kota!')\n",
    "print(l, type(l))\n",
    "list(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oczywiście lepiej powyższą funkcjonalność wykorzystać do funkcji zwracających RDD lub DataFrame, które można użyć bezpośrednio w Pythonie."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
