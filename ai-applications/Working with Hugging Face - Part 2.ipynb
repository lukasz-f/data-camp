{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Hugging Face - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Pipelines with Hugging Face\n",
    "\n",
    "It's time to dive into the Hugging Face ecosystem! You'll start by learning the basics of the pipeline module and Auto classes from the transformers library. Then, you'll learn at a high level what natural language processing and tokenization is. Finally, you'll start using the pipeline module for several text-based tasks, including text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started with pipelines\n",
    "Hugging Face has an ecosystem of libraries that allow users to leverage tools at different levels. The pipeline module from the transformers library is a great place to get started with performing ML tasks. It removes the requirement for training models, allowing for quicker experimentation and results. It does this by being a wrapper around underlying objects, functions, and processes.\n",
    "\n",
    "Getting started with pipeline can be done by defining a task or model. This helps with quick experimentation as you become familiar with the library.\n",
    "\n",
    "Create your first pipelines for sentiment analysis. The input is a sentence string that is already loaded for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment from task_pipeline: POSITIVE\n",
      "Sentiment from model_pipeline: POSITIVE\n"
     ]
    }
   ],
   "source": [
    "# Import pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create the task pipeline\n",
    "task_pipeline = pipeline(task=\"sentiment-analysis\")\n",
    "\n",
    "# Create the model pipeline\n",
    "model_pipeline = pipeline(model=\"distilbert-base-uncased-finetuned-sst-2-english\", device=\"cpu\")\n",
    "\n",
    "input = \"This course is pretty good, I guess.\"\n",
    "\n",
    "# Predict the sentiment\n",
    "task_output = task_pipeline(input)\n",
    "model_output = model_pipeline(input)\n",
    "\n",
    "print(f\"Sentiment from task_pipeline: {task_output[0]['label']}\\nSentiment from model_pipeline: {model_output[0]['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using AutoClasses\n",
    "AutoClasses offer more control for machine learning tasks, and they can also be used with pipeline() for quick application. It's a nice balance of control and convenience.\n",
    "\n",
    "Continue with the sentiment analysis task and combine AutoClasses with the pipeline module.\n",
    "\n",
    "AutoModelForSequenceClassification and AutoTokenizer from the transformers library have already been imported for you and the input text is saved as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment using AutoClasses: POSITIVE\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "# Download the model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Create the pipeline\n",
    "sentimentAnalysis = pipeline(task=\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "input = \"This course is pretty good, I guess.\"\n",
    "\n",
    "# Predict the sentiment\n",
    "output = sentimentAnalysis(input)\n",
    "\n",
    "print(f\"Sentiment using AutoClasses: {output[0]['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing models with the pipeline\n",
    "One of the great benefits of the pipeline() module is the ease at which you can experiment with different models simply by changing the \"model\" input. This is a good way to determine which model works best for a particular task or dataset that you are working with.\n",
    "\n",
    "Experiment with two sentiment analysis models by creating pipelines for each, then using them to predict the sentiment for a sentence.\n",
    "\n",
    "pipeline from the transformers library is already loaded for you. The example input sentence is saved as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert Output: Extremely Positive\n",
      "Distil Output: POSITIVE\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create the pipeline\n",
    "distil_pipeline = pipeline(task=\"sentiment-analysis\", \n",
    "                           model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "input = \"This course is pretty good, I guess.\"\n",
    "\n",
    "# Predict the sentiment\n",
    "distil_output = distil_pipeline(input)\n",
    "\n",
    "# Create the second pipeline and predict the sentiment\n",
    "bert_pipeline = pipeline(task=\"sentiment-analysis\", \n",
    "                         model=\"kwang123/bert-sentiment-analysis\")\n",
    "bert_output = bert_pipeline(input)\n",
    "\n",
    "print(f\"Bert Output: {bert_output[0]['label']}\")\n",
    "print(f\"Distil Output: {distil_output[0]['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing text\n",
    "An important step to performing an NLP task is tokenizing the input text. This makes the text more understandable and manageable for the ML models, or other algorithms.\n",
    "\n",
    "Before performing tokenization, it's best to run normalization steps, i.e. removing white spaces and accents, lowercasing, and more. Each tokenizer available in Hugging Face uses it's own normalization and tokenization processes.\n",
    "\n",
    "Let's take a look at what normalization the distilbert-base-uncased tokenizer applies to the input_string, \"HOWDY, how aré yoü?\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "howdy, how are you?\n"
     ]
    }
   ],
   "source": [
    "# Import the AutoTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Download the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "input_string = \"HOWDY, how aré yoü?\"\n",
    "\n",
    "# Normalize the input string\n",
    "output = tokenizer.backend_tokenizer.normalizer.normalize_str(input_string)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing tokenizer output\n",
    "Most models in Hugging Face will have an associated tokenizer that will help prepare the input data based on what the model expects. After normalization, the tokenizer will split the input into smaller chunks based on the chosen algorithm. This is known as \"pre-tokenization\".\n",
    "\n",
    "Let's explore the different types of pre-tokenization by performing this process with two tokenizers on the same input. We will be using DistilBertTokenizer and GPT2Tokenizer which have already been loaded for you. The input text string, \"Pineapple on pizza is pretty good, I guess\" is saved as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT tokenizer: ['P', 'ine', 'apple', 'Ġon', 'Ġpizza', 'Ġis', 'Ġpretty', 'Ġgood', ',', 'ĠI', 'Ġguess', '.']\n",
      "DistilBERT tokenizer: ['pine', '##apple', 'on', 'pizza', 'is', 'pretty', 'good', ',', 'i', 'guess', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "input = \"Pineapple on pizza is pretty good, I guess.\"\n",
    "\n",
    "# Download the gpt tokenizer\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Tokenize the input\n",
    "gpt_tokens = gpt_tokenizer.tokenize(input)\n",
    "\n",
    "# Repeat for distilbert\n",
    "distil_tokenizer = DistilBertTokenizer.from_pretrained(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "distil_tokens = distil_tokenizer.tokenize(text=input)\n",
    "\n",
    "# Compare the output\n",
    "print(f\"GPT tokenizer: {gpt_tokens}\")\n",
    "print(f\"DistilBERT tokenizer: {distil_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammatical correctness\n",
    "Text classification is the process of labeling an input text into a pre-defined category. This can take the form of sentiment - positive or negative - spam detection - spam or not spam - and even grammatical errors.\n",
    "\n",
    "Explore the use of a text-classification pipeline for checking an input sentence for grammatical errors.\n",
    "\n",
    "pipeline from the transformers library is already loaded for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.9956323504447937}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a pipeline\n",
    "classifier = pipeline(\n",
    "  task=\"text-classification\", \n",
    "  model=\"abdulmatinomotoso/English_Grammar_Checker\"\n",
    ")\n",
    "\n",
    "# Predict classification\n",
    "output = classifier(\"I will walk dog\")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Natural Language Inference\n",
    "Another task under the text classification umbrella is Question Natural Language Inference, or QNLI. This determines if a piece of text contains enough information to answer a posed question. This requires the model to perform logical reasoning which are important for Q&A applications.\n",
    "\n",
    "Performing different tasks with the text-classification pipeline can be done by choosing different models. Each model is trained to predict specific labels and optimized for learning different context within a text.\n",
    "\n",
    "pipeline from the transformers library is already loaded for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.005238955840468407}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create the pipeline\n",
    "classifier = pipeline(task=\"text-classification\", model=\"cross-encoder/qnli-electra-base\")\n",
    "\n",
    "# Predict the output\n",
    "output = classifier(\"Where is the capital of France?, Brittany is known for their kouign-amann.\")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot classification\n",
    "Zero-shot classification is the ability for a transformer to predict a label from a new set of classes which it wasn't originally trained to identify. This is possible through its transfer learning capabilities. It can be an extremely valuable tool.\n",
    "\n",
    "Hugging Face pipeline() also has a zero-shot-classification task. These pipelines require both an input text and candidate labels.\n",
    "\n",
    "Build a zero-shot classifier to predict the label for the input text, a news headline that has been loaded for you.\n",
    "\n",
    "pipelines from the transformers library is already loaded for you. Note that we are using our own version of the pipeline function to enable you to learn how to use these functions without having to download the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Label: science with score: 0.9030603170394897\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Build the zero-shot classifier\n",
    "classifier = pipeline(task=\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Create the list\n",
    "candidate_labels = [\"politics\", \"science\", \"sports\"]\n",
    "\n",
    "text = \"A 75-million-year-old Gorgosaurus fossil is the first tyrannosaur skeleton ever found with a filled stomach.\"\n",
    "\n",
    "# Predict the output\n",
    "output = classifier(text, candidate_labels)\n",
    "\n",
    "print(f\"Top Label: {output['labels'][0]} with score: {output['scores'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing long text\n",
    "Summarization is a useful task for reducing large piece of text into something more manageable. This could be beneficial for multiple reasons like reducing the amount of time a reader needs to spend to obtain the important point of a piece of text.\n",
    "\n",
    "The Hugging Face pipeline() task, \"summarization\", builds a s summarization pipeline which is a quick way to perform summarization on a piece of text. You'll do that by creating a new pipeline and using it to summarize a piece of text from a Wikipedia page on Greece.\n",
    "\n",
    "pipeline from the transformers library and the original_text have already been loaded for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text length: 829\n",
      "Summary length: 473\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "original_text = \"\\nGreece has many islands, with estimates ranging from somewhere around 1,200 to 6,000, depending on the minimum size to take into account. The number of inhabited islands is variously cited as between 166 and 227.\\nThe Greek islands are traditionally grouped into the following clusters: the Argo-Saronic Islands in the Saronic Gulf near Athens; the Cyclades, a large but dense collection occupying the central part of the Aegean Sea; the North Aegean islands, a loose grouping off the west coast of Turkey; the Dodecanese, another loose collection in the southeast between Crete and Turkey; the Sporades, a small tight group off the coast of Euboea; and the Ionian Islands, chiefly located to the west of the mainland in the Ionian Sea. Crete with its surrounding islets and Euboea are traditionally excluded from this grouping.\\n\"\n",
    "\n",
    "# Create the summarization pipeline\n",
    "summarizer = pipeline(task=\"summarization\", model=\"cnicu/t5-small-booksum\")\n",
    "\n",
    "# Summarize the text\n",
    "summary_text = summarizer(original_text)\n",
    "\n",
    "# Compare the length\n",
    "print(f\"Original text length: {len(original_text)}\")\n",
    "print(f\"Summary length: {len(summary_text[0]['summary_text'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using min_length and max_length\n",
    "The pipeline() function, has two important parameters: min_length and max_length. These are useful for adjusting the length of the resulting summary text to be short, longer, or within a certain number of words. You might want to do this if there are space constraints (i.e., small storage), to enhance readability, or improve the quality of the summary.\n",
    "\n",
    "You'll experiment with a short and long summarizer by setting these two parameters to a small range, then a wider range.\n",
    "\n",
    "pipeline from the transformers library and the original_text have already been loaded for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greece has many islands, with estimates ranging\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greece has many islands, with estimates ranging from somewhere around 1,200 to 6,000 depending on the minimum size to take into account. The number of inhabited islands is variously cited as between 166 and 227. The Greek islands are traditionally grouped into the following clusters: the Argo-Saronic Islands in the Saronic Gulf near Athens; the Cyclades, a large but dense collection occupying the central part of the Aegean Sea; the North Aegesan islands, an loose group\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "original_text = \"\\nGreece has many islands, with estimates ranging from somewhere around 1,200 to 6,000, depending on the minimum size to take into account. The number of inhabited islands is variously cited as between 166 and 227. The Greek islands are traditionally grouped into the following clusters: the Argo-Saronic Islands in the Saronic Gulf near Athens; the Cyclades, a large but dense collection occupying the central part of the Aegean Sea; the North Aegean islands, a loose grouping off the west coast of Turkey; the Dodecanese, another loose collection in the southeast between Crete and Turkey; the Sporades, a small tight group off the coast of Euboea; and the Ionian Islands, chiefly located to the west of the mainland in the Ionian Sea. Crete with its surrounding islets and Euboea are traditionally excluded from this grouping.\\n\"\n",
    "\n",
    "# Create a short summarizer\n",
    "short_summarizer = pipeline(task=\"summarization\", model=\"cnicu/t5-small-booksum\", min_length=1, max_length=10)\n",
    "\n",
    "# Summarize the input text\n",
    "short_summary_text = short_summarizer(original_text)\n",
    "\n",
    "# Print the short summary\n",
    "print(short_summary_text[0][\"summary_text\"])\n",
    "\n",
    "# Repeat for a long summarizer\n",
    "long_summarizer = pipeline(task=\"summarization\", model=\"cnicu/t5-small-booksum\", min_length=50, max_length=150)\n",
    "\n",
    "long_summary_text = long_summarizer(original_text)\n",
    "\n",
    "# Print the long summary\n",
    "print(long_summary_text[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing several inputs\n",
    "Often times, you'll be working on projects where summarization will occur over an entire dataset or list of items, not just a single piece of text. Fortunately, this can be done by passing in a list of text items. This will return a list of summarized texts.\n",
    "\n",
    "You'll build a final summarization pipeline and use it to summarize a list of text items from the wiki dataset.\n",
    "\n",
    "pipeline from the transformers library and the dataset wiki have already been loaded for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647e11d53196483096bc3543c0d6998d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed94e3ec584642238df5bbee4199cd64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary 1: Anarchism is a political philosophy and movement that is skeptical of all justifications for authority and seeks to abolish institutions it claims maintain unnecessary coercion and hierarchy . As a historically left-wing movement, this reading\n",
      "Summary 2: Surface albedo is defined as the ratio of radiosity Je to the irradiance Ee (flux per unit area) received by a surface. The proportion reflected is not only determined by properties of the surface\n",
      "Summary 3: A, or a, is the first letter and the first vowel of the Latin alphabet, used in the modern English alphabet, the alphabets of other western European languages and others worldwide. Its name in English is a (\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "\n",
    "wikipedia = load_dataset(\"wikimedia/wikipedia\", name=\"20231101.en\", split=\"train\")\n",
    "\n",
    "# Create the list\n",
    "text_to_summarize = [w[\"text\"] for w in wikipedia.select(range(3))]\n",
    "\n",
    "# Create the pipeline\n",
    "summarizer = pipeline(\"summarization\", \n",
    "                      model=\"cnicu/t5-small-booksum\", \n",
    "                      min_length=20, \n",
    "                      max_length=50,\n",
    "                      device='cpu')\n",
    "\n",
    "# Summarize each item in the list\n",
    "summaries = summarizer(text_to_summarize, truncation=True)\n",
    "\n",
    "# Create for-loop to print each summary\n",
    "for i in range(0,3):\n",
    "  print(f\"Summary {i+1}: {summaries[i]['summary_text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
