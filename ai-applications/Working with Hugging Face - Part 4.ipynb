{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Hugging Face - Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning and Embeddings\n",
    "\n",
    "Explore the different frameworks for fine-tuning, text generation, and embeddings. Start with the basics of fine-tuning a pre-trained model on a specific dataset and task to improve performance. Then, use Auto classes to generate the text from prompts and images. Finally, you will explore how to generate and use embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing a dataset\n",
    "Fine-tuning a model requires several steps including identifying the model to fine-tune, preparing the dataset, creating the training loop object, then saving the model.\n",
    "\n",
    "A model trained on English text classification has been identified for you, but it's up to you to prepare the imdb dataset in order to fine-tune this model to classify the sentiment of movie reviews.\n",
    "\n",
    "The imdb dataset is already loaded for you and saved as dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "dataset = load_dataset(\"stanfordnlp/imdb\")\n",
    "\n",
    "# Use tokenizer on text\n",
    "dataset = dataset.map(lambda row: tokenizer(row[\"text\"], padding='max_length', max_length=512, truncation=True), \n",
    "                      keep_in_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the trainer\n",
    "To fine-tune a model, it must be trained on new data. This is the process of the model learning patterns within a training dataset, then evaluating how well it can predict patterns in an unseen test dataset. The goal is to help the model build an understanding of patterns while also being generalizable to new data yet to be seen.\n",
    "\n",
    "Build a training object to fine-tune the \"distilbert-base-uncased-finetuned-sst-2-english\" model to be better at identifying sentiment of movie reviews.\n",
    "\n",
    "The training_data and testing_data dataset are available for you. Trainer and TrainingArguments from transformers are also loaded. They were modified for the purpose of this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Create training arguments\n",
    "training_args = TrainingArguments(output_dir=\"./results\")\n",
    "\n",
    "training_data = dataset['train']\n",
    "testing_data = dataset['test']\n",
    "\n",
    "# Create the trainer\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset=training_data, \n",
    "    eval_dataset=testing_data\n",
    ")\n",
    "\n",
    "# Start the trainer\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "local_path = \"./fine_tuned_model\"\n",
    "trainer.save_model(local_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the fine-tuned model\n",
    "Now that the model is fine-tuned, it can be used within pipeline tasks, such as for sentiment analysis. At this point, the model is typically saved to a local directory (i.e. on your own computer), so a local file path is needed.\n",
    "\n",
    "You'll use the newly fine-tuned distilbert model. There is a sentence, \"I am a HUGE fan of romantic comedies.\", saved as text_example.\n",
    "\n",
    "Note: we are using our own pipeline module for this exercise for teaching purposes. The model is \"saved\" (i.e. not really) under the path ./fine_tuned_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text_example = \"I am a HUGE fan of romantic comedies.\"\n",
    "\n",
    "# Create the classifier\n",
    "classifier = pipeline(task=\"sentiment-analysis\", model=local_path)\n",
    "\n",
    "# Classify the text\n",
    "results = classifier(text=text_example)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating text from a text prompt\n",
    "Generating text can be accomplished using Auto classes from the Hugging Face transformers library. It can be a useful method for developing content, such as technical documentation or creative material.\n",
    "\n",
    "You'll walk through the steps to process the text prompt, \"Wear sunglasses when its sunny because\", then generate new text from it.\n",
    "\n",
    "AutoTokenizer and AutoModelForCausalLM from the transformers library are already loaded for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wear sunglasses when its sunny because it's a hot day.\n",
      "\n",
      "The best way to get\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Set model name\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "# Get the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Wear sunglasses when its sunny because\"\n",
    "\n",
    "# Tokenize the input\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate the text output\n",
    "output = model.generate(input_ids, num_return_sequences=1)\n",
    "\n",
    "# Decode the output\n",
    "generated_text = tokenizer.decode(output[0])\n",
    "\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a caption for an image\n",
    "Generating text can be done for modalities other than text, such as images. This has a lot of benefits including faster content creation by generating captions from images.\n",
    "\n",
    "You'll create a caption for a fashion image using the Microsoft GIT model (\"microsoft/git-base-coco\").\n",
    "\n",
    "AutoProcessor and AutoModelForCausalLM from the transformers library is already loaded for you along with the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cade4117ceb8450bb90cd0f975eddd14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/503 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f334958ec48e4d298abc1696caaf054d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/453 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95dd4006b2549978f6d554eb9268257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f896f6461046eba377ddfa7b1d09b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196b4fdcdc3c4fb1afb64685408bf160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f21ce31970cb4104a8a1e680d86730b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.82k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e30711050134f5f89bf9fd981f23e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/707M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcffe417a0f14f3ab0effa04633cc9ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukasz/.local/share/virtualenvs/ai-applications-6dpgqVvJ/lib/python3.11/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] a woman wearing a black sweater and gray pants. [SEP]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "from PIL import Image\n",
    "\n",
    "image = Image.open('fashion.jpeg')\n",
    "\n",
    "# Get the processor and model\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-base-coco\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base-coco\")\n",
    "\n",
    "# Process the image\n",
    "pixels = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "# Generate the ids\n",
    "output = model.generate(pixel_values=pixels)\n",
    "\n",
    "# Decode the output\n",
    "caption = processor.batch_decode(output)\n",
    "\n",
    "print(caption[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings for a sentence\n",
    "Embeddings are playing an increasingly big role in ML and AI systems. A common use case is embedding text to support search.\n",
    "\n",
    "The sentence-transformers package from Hugging Face is useful for getting started with embedding models. You'll compare the embedding shape from two different models - \"all-MiniLM-L6-v2\" and \"sentence-transformers/paraphrase-albert-small-v2\". This can determine which is better suited for a project (i.e. because of storage constraints).\n",
    "\n",
    "The sentence used for embedding, \"Programmers, do you put your comments (before|after) the related code?\", is saved as sentence.\n",
    "\n",
    "SentenceTransformer from the sentence-transformers package was already loaded for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukasz/.local/share/virtualenvs/ai-applications-6dpgqVvJ/lib/python3.11/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f9a4c1fbf141d3bc4b0d3cf3dec015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b792cb9ce3604d8fbf2465223a535faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d361cac5ff9d46a387d8e3bb61f01fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "195f2b36c1604b268ff5a04a73d6945b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f480b8e4e74362924ea788fb981c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c022209fac6a43ca95a2f3b0ecbcd7fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d27ad38a5de4fd2ac157817d7ca881e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94cfc2e5e216491e95667bcd97a7c772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "754f373f57494aa3bc78dc85b53274de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7c518a1ead949539871043f9216acea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09cf29cf6eff49b689ede1fe16a421f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486a1f34d2eb485badcc3b63ae2d2838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1058341474646e4b7010af4b22f43a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e8860e0b2a64c1cb28cbbcb560ed246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/4.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac617584efd407cae43d41cb017447e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d300f60c6a4b049e2113bca10d42fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/827 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d92d2462ea42db8afd3e066405cb3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/46.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d1baa17b4948b7a6702e2fde530aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/465 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9adc224bc7fc48bcaa5b62126f95a60d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbbcabb74e0a402db63f69a0fcf5aaab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce4ce9d4d11477691a551969b184fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/245 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e9dfea029d4865b6c9bd60e31608d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentence = \"Programmers, do you put your comments (before|after) the related code?\"\n",
    "\n",
    "# Create the first embedding model\n",
    "embedder1 = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Embed the sentence\n",
    "embedding1 = embedder1.encode([sentence])\n",
    "\n",
    "# Create and use second embedding model\n",
    "embedder2 = SentenceTransformer(\"sentence-transformers/paraphrase-albert-small-v2\")\n",
    "embedding2 = embedder2.encode([sentence])\n",
    " \n",
    "# Compare the shapes\n",
    "print(embedding1.shape == embedding2.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using semantic search\n",
    "The similarity, or closeness, between a query and the other sentences, or documents, is the foundation for semantic search. This is a search method which takes into account context and intent of the query. Similarity measures, such as cosine similarity, are used to quantify the distance between the query and each sentence within the dimensional space. Results of a search are based on the closest sentences to the query.\n",
    "\n",
    "You will use semantic search to return the top two Reddit threads relevant to the user query, \"I need a desktop book reader for Mac\".\n",
    "\n",
    "The embedder and sentence_embeddings are already loaded for you along with util.semantic_search()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can anyone suggest a desktop book reader for Mac that works similar to Stanza on the iPhone? (Score: 0.8011)\n",
      "I'm looking for a good quality headset that doesn't cost too much. Any recommendations? (Score: 0.1437)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import util\n",
    "\n",
    "sentences = ['Programmers, do you put your comments (before|after) the related code?',\n",
    " 'How sure are we that there were never any intelligent dinosaurs?',\n",
    " 'Can anyone suggest a desktop book reader for Mac that works similar to Stanza on the iPhone?',\n",
    " 'I will be in Lima, Ohio Monday night/tuesday on business. What is there to do, and see in the area?',\n",
    " \"I'm looking for a good quality headset that doesn't cost too much. Any recommendations?\",\n",
    " 'How do I get a list of all the duplicate items using LINQ?',\n",
    " \"Please help me figure out why it's so tough for me to connect to Valve games. It's driving me insane.\",\n",
    " \"Is there such a thing as 'good' instant coffee?\",\n",
    " 'How do I get the distinct/unique values in a column in Excel?']\n",
    "\n",
    "query = \"I need a desktop book reader for Mac\"\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "sentence_embeddings = embedder.encode(sentences)\n",
    "\n",
    "# Generate embeddings\n",
    "query_embedding = embedder.encode([query])[0]\n",
    "\n",
    "# Compare embeddings\n",
    "hits = util.semantic_search(query_embedding, sentence_embeddings, top_k=2)\n",
    "\n",
    "# Print the top results\n",
    "for hit in hits[0]:\n",
    "    print(sentences[hit[\"corpus_id\"]], \"(Score: {:.4f})\".format(hit[\"score\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
