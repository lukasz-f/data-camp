{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Łączenie klasyfikatorów (_ensembling_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "dataset = np.loadtxt('Dane/pima-indians-diabetes.data', delimiter=\",\")\n",
    "# split data into X and y\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "print(X.shape)\n",
    "print(np.mean(Y))\n",
    "\n",
    "seed = 7\n",
    "test_size = 0.33\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "\n",
    "models = [LogisticRegression(),DecisionTreeClassifier(),SVC(probability=True), LinearDiscriminantAnalysis(), QuadraticDiscriminantAnalysis(), RandomForestClassifier()]\n",
    "\n",
    "for model in models:\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict_proba(X_test)[:,1]\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0), \"AUC: \", roc_auc_score(y_score=y_pred,y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [LogisticRegression(),\n",
    "          DecisionTreeClassifier(),\n",
    "          SVC(probability=True),\n",
    "          LinearDiscriminantAnalysis(), \n",
    "          QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "model = VotingClassifier(estimators=list(zip([\"lr\",\"dt\",\"svm\",\"lda\",\"qda\"],models)),voting=\"hard\")\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = VotingClassifier(estimators=list(zip([\"lr\",\"dt\",\"svm\",\"lda\",\"qda\"],models)),voting=\"soft\")\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict_proba(X_test)[:,1]\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0), roc_auc_score(y_score=y_pred,y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porównanie różnych zestawów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [LogisticRegression(),\n",
    "          SVC(probability=True),\n",
    "          LinearDiscriminantAnalysis()]\n",
    "\n",
    "for model in models:\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict_proba(X_test)[:,1]\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0), \"AUC: \", roc_auc_score(y_score=y_pred,y_true=y_test))\n",
    "\n",
    "model = VotingClassifier(estimators=list(zip([\"lr\",\"svm\",\"lda\"],models)),voting=\"soft\")\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict_proba(X_test)[:,1]\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0), roc_auc_score(y_score=y_pred,y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [LogisticRegression(),\n",
    "          DecisionTreeClassifier(), \n",
    "          QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "for model in models:\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict_proba(X_test)[:,1]\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0), \"AUC: \", roc_auc_score(y_score=y_pred,y_true=y_test))\n",
    "\n",
    "\n",
    "model = VotingClassifier(estimators=list(zip([\"lr\",\"dt\",\"qda\"],models)),voting=\"hard\")\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict_proba(X_test)[:,1]\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0), roc_auc_score(y_score=y_pred,y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost jest to komitet drzew regresyjnych, który stosuje się zarówno do problemu regresji jak i klasyfikacji. Wartość zwracana przez model (dla $i$-tej obserwacji):\n",
    "\n",
    "$$\\hat{y}_i = \\sum\\limits_{k=1}^K f_k(x_i),$$\n",
    "\n",
    "gdzie $f_k(\\cdot)$ - wartość zwracana przez $k$-te drzewo.\n",
    "\n",
    "\n",
    "Predykcja zależy od problemu:\n",
    "\n",
    " 1) Regresja\n",
    "\n",
    "  - predykcja: $ pred_i = \\hat{y}_i $\n",
    "    \n",
    "\n",
    "2) Klasyfikacja binarna\n",
    "\n",
    "  - predykcja: $pred_i = p(x_i) = sigmoid(\\hat{y}_i) = \\frac{1}{1+\\exp{(-\\hat{y}_i)}}$\n",
    "  \n",
    "3) Klasyfikacja wieloklasowa\n",
    "\n",
    "  - predykcja: $pred_i^c =softmax(\\hat{y}_i)^c = \\frac{\\exp{(\\hat{y}_i^c)}}{\\sum\\limits_c\\exp{(\\hat{y}_i^c)}}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Uczenie modelu - budowanie komitetu - polega na konstruowaniu kolejnych drzew w taki sposób, aby zminimalizować fukcję celu:\n",
    "\n",
    "### $$Obj(\\Theta) = L(\\Theta) + \\Omega(\\Theta),$$\n",
    "\n",
    "gdzie\n",
    "\n",
    "$\\Theta$ - model (zestaw konkretnych drzew)\n",
    "\n",
    "$L(\\Theta)$ - funkcja straty (miara dopasowania modelu)\n",
    "\n",
    "$\\Omega(\\Theta)$ - regularyzacja (miara złozoności modelu)\n",
    "\n",
    "Fukcja straty zależy od problemu:\n",
    "\n",
    " 1) Regresja\n",
    "    \n",
    "  - funkcja straty - kwadratowa: $\\sum\\limits_i (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "\n",
    "2) Klasyfikacja binarna\n",
    "\n",
    "  - funkcja straty - log-loss (_binomial cross-entropy_): $ - \\frac{1}{n}\\sum\\limits_{i=1}^n \\big( y_i\\log(p(x_i)) + (1-y_i)\\log(1-p(x_i))   \\big) $\n",
    "\n",
    "3) Klasyfikacja wieloklasowa\n",
    "\n",
    "  - funkcja straty - log-loss (_multinomial cross-entropy_): $ = -\\frac{1}{n}\\sum_{i=1}^n\\sum\\limits_c y_{ij} \\log(p(x_i)_j) $\n",
    "\n",
    "\n",
    "Drzewa uczone są iteracyjnie:\n",
    "\n",
    "\\begin{split}\\hat{y}_i^{(0)} &= 0\\\\\n",
    "\\hat{y}_i^{(1)} &= f_1(x_i) = \\hat{y}_i^{(0)} + f_1(x_i)\\\\\n",
    "\\hat{y}_i^{(2)} &= f_1(x_i) + f_2(x_i)= \\hat{y}_i^{(1)} + f_2(x_i)\\\\\n",
    "&\\dots\\\\\n",
    "\\hat{y}_i^{(t)} &= \\sum_{k=1}^t f_k(x_i)= \\hat{y}_i^{(t-1)} + f_t(x_i)\n",
    "\\end{split}\n",
    "\n",
    "W danym kroku znajdujemy drzewo, które najlepiej poprawia naszą funkcję celu.\n",
    "\n",
    "Regularyzacja:\n",
    "\n",
    "\\begin{split}\\Omega(\\Theta) = \\sum_{i=1}^t\\Omega(f_i),  \\\\\n",
    "\\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda \\sum_{j=1}^T w_j^2,\n",
    "\\end{split}\n",
    "\n",
    "gdzie $T$ - liczba liści w drzewie $f$, $w_j$ wartość zwracana przez drzewo dla obseracji w $j$-tym liściu. Uwaga: wartości $w_{(\\cdot)}$ są wyliczane na podstawie funkcji celu - w przybliżeniu znajdowane są takie, które dają najlepszy wynik.\n",
    "\n",
    "\n",
    "W praktyce dodaje się kolejną regularyzację:\n",
    "\n",
    "$$\\hat{y}_i^{(k)} = \\hat{y}_i^{(k-1)} + \\eta \\cdot f_k(x_i),$$\n",
    "\n",
    "gdzie $\\eta$ - _learning rate_.\n",
    "\n",
    "https://xgboost.readthedocs.io/en/latest/model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "predictions = np.round(y_pred)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0), roc_auc_score(y_score=y_pred,y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "predictions = np.round(y_pred)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(roc_auc_score(y_score=y_pred,y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jak szukać parametrów?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co do zasady: zaczynamy od małej liczby drzew i dużego learning rate. Natomiast nic nie stoi na przeszkodzie, żeby sobie znaleźć sensowne te wartości:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()\n",
    "params = {\n",
    "    \"n_estimators\": [3,5,10,20,50],\n",
    "    \"learning_rate\": [0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "gscv1 = GridSearchCV(param_grid=params, estimator=xgb,cv=20)\n",
    "gscv1.fit(X_train,y_train)\n",
    "s = gscv1.grid_scores_\n",
    "s.sort(key= lambda x: x[1],reverse=True)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zatem widzimy, że 5 drzew daje sensowne rezultaty, a jednocześnie to bardzo mała liczba -> podczas optymalizowania kolejnych parametrów model będzie szybko się uczył. Natomiast trzeba zwrócić uwagę, że różnice nie są duże, więc wzięcie 20 drzew też miałoby sens, bo jednak troche wiecej drzew daje lepszą stabilonść modelu, a 20 to nie jest jakoś bardzo dużo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Szukamy kolejnych parametrów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inicjujemy przy uzyciu poprzednich najlepszych parametrow i badamy kolejne parametry\n",
    "xgb = XGBClassifier(**gscv1.best_params_) #LUB gscv.best_estimator_.get_params()\n",
    "\n",
    "params = {\"max_depth\": [3,4,5],\n",
    "         \"min_child_weight\": [3, 5, 7, 10 ,15, 20]}\n",
    "gscv2 = GridSearchCV(param_grid=params, estimator=xgb,cv=20)\n",
    "gscv2.fit(X_train,y_train)\n",
    "s = gscv2.grid_scores_\n",
    "s.sort(key= lambda x: x[1],reverse=True)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Skrajne wartości z naszego zakresu wyszły najlepiej - trzeba poszerzyć siatkę:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(**gscv1.best_params_)\n",
    "params = {\"max_depth\": [2,3,4,5,6,7,8],\n",
    "         \"min_child_weight\": [3, 5, 7, 10 ,15, 20,25,30,40,50]}\n",
    "gscv2 = GridSearchCV(param_grid=params, estimator=xgb,cv=20)\n",
    "gscv2.fit(X_train,y_train)\n",
    "s = gscv2.grid_scores_\n",
    "s.sort(key= lambda x: x[1],reverse=True)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TU wszystko jedna skad wezmiemy parametry - albo gscv2.best_estimator_.get_params() albo gscv2.best_params,\n",
    "# bo i tak potrzebujemy wziac jako stale te z drugiego kroku optymalizacji a z pierwszego optymalizujemy\n",
    "# ale gdybysmy chcieli miec stale optymalne z krokow 1 i 2 to trzeba wziac gscv2.best_estimator_.get_params(),\n",
    "# bo inaczej wzielibysmy optymalne z kroku 2 a domyslne z 1\n",
    "xgb = XGBClassifier(**gscv2.best_estimator_.get_params())\n",
    "params = {\n",
    "    \"n_estimators\": [25,50,100,300,500,1000],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.15, 0.2]\n",
    "}\n",
    "\n",
    "gscv3 = GridSearchCV(param_grid=params, estimator=xgb,cv=20)\n",
    "gscv3.fit(X_train,y_train) \n",
    "\n",
    "s = gscv3.grid_scores_\n",
    "s.sort(key= lambda x: x[1],reverse=True)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob = gscv3.best_estimator_.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(accuracy_score(y_pred=prob>0.5,y_true=y_test), roc_auc_score(y_score=prob,y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gscv3.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Czyli po optymalizacji wynik jest gorszy...\n",
    "\n",
    "### Dlaczego tak sie stało?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rzućmy okiem na aspekt statystyczny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for seed in range(10):\n",
    "\n",
    "    test_size = 0.33\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "    \n",
    "    model = XGBClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0), roc_auc_score(y_score=y_pred,y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widzimy, że w zależności od podziału accuracy waha w przedziale (73%,78%) -> TEN ZBIÓR JEST ZA MAŁY ŻEBY RZETELNIE PORÓWNYWAĆ XGBOOSTA DLA RÓŻNYCH PARAMETRÓW. Żeby móc stwierdzić, że jakieś parametry są istotnie lepsze, musiały one dać accuracy (na oko) 3 % większe niż średnia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cace study: dane \"adults\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Wczytaj dane treningowe i testowe\n",
    "\n",
    "import pandas as pd\n",
    "train_set = pd.read_csv('Dane/adult.data', header = None)\n",
    "test_set = pd.read_csv('Dane/adult.test',skiprows = 1, header = None) # Make sure to skip a row for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_labels = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', \n",
    "              'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
    "             'wage_class']\n",
    "train_set.columns = col_labels\n",
    "test_set.columns = col_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Wyświetl informacje o typach zmiennych:\n",
    "\n",
    "train_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Usun braki danych (oznaczone jako ' ?')\n",
    "\n",
    "train = train_set.replace(' ?', np.nan).dropna()\n",
    "test = test_set.replace(' ?', np.nan).dropna()\n",
    "\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Polacz zbiory\n",
    "\n",
    "dataset = pd.concat([train,test])\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Zamien zmienna objasniana na binarna\n",
    "dataset['wage_class'] = dataset.wage_class.replace({' <=50K.': 0,' <=50K':0, ' >50K.':1, ' >50K':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Usun zmienna fnlwgt\n",
    "dataset.drop([\"fnlwgt\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(dataset.education,dataset.education_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Zaproponuj sposbob obluzenia informacji dotyczących edukacji\n",
    "dataset.drop([\"education\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Wypisz informacje o rozkladach wszystkich zmiennych (nie tylko numerycznych)\n",
    "dataset.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Wypisz czestosci poszczegolnych narodowosci\n",
    "\n",
    "dataset.native_country.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Przeanalizuj rozklad y w zaleznosci od narodowosci:\n",
    "# 1) rysujac histogramy w podgrupach wyznaczonych przez narodowosc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "dataset['wage_class'].hist(by=dataset['native_country'],figsize=(12,12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2) rysujac histogram proporcji w grupach\n",
    "x = dataset.groupby('native_country')[\"wage_class\"].mean()\n",
    "x.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Zaproponuj sposob obsluzenia tej zmiennej\n",
    "\n",
    "d = dict(pd.cut(x[x.index!=\" United-States\"],5,labels=range(5)))\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset['native_country'] = dataset['native_country'].replace(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Zamien zmienne nominalne na numeryczne \n",
    "#(tak zeby zmienna o k-wartosciach byla reprezentowana przez k-1 zmiennych binarnych)\n",
    "dataset = pd.get_dummies(dataset,drop_first=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = dataset.iloc[:train.shape[0]]\n",
    "test = dataset.iloc[train.shape[0]:]\n",
    "\n",
    "X_train = train.drop(\"wage_class\",axis=1)\n",
    "y_train = train.wage_class\n",
    "\n",
    "X_test = test.drop(\"wage_class\",axis=1)\n",
    "y_test = test.wage_class\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zaczynamy modelowanie\n",
    "\n",
    "Zaczynamy od określenia punktu wyjścia. Naucz i przetestuj klasyfikator XGBoost. Interesują nas: accuracy orac AUC.\n",
    "\n",
    "Dla pełniejszego obrazu jeszcze wcześniej przeanalizujmy też jakość drzewa decyzyjnego:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Domyślne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = DecisionTreeClassifier()\n",
    "m.fit(X_train,y_train)\n",
    "\n",
    "accuracy_score(y_test,m.predict(X_test)), roc_auc_score(y_test,m.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoptymalizowane:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "params = {\n",
    "    \"criterion\": [\"gini\",\"entropy\"],\n",
    "    \"max_depth\": [3,4,5,6,7,8,9,10],\n",
    "    \"min_samples_leaf\": [5,10,15,20,30,50]\n",
    "}\n",
    "\n",
    "gscv = GridSearchCV(param_grid=params, estimator=dt,cv=20)\n",
    "gscv.fit(X_train,y_train)\n",
    "s = gscv.grid_scores_\n",
    "s.sort(key=lambda x: x[1], reverse= True)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = gscv.best_estimator_\n",
    "m.fit(X_train,y_train)\n",
    "\n",
    "accuracy_score(y_test,m.predict(X_test)), roc_auc_score(y_test,m.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train,y_train)\n",
    "\n",
    "accuracy_score(y_test,xgb.predict(X_test)), roc_auc_score(y_test,xgb.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie. Zbadaj schematy optymalizacji parametrów.\n",
    "\n",
    "Podejście 1\n",
    "\n",
    "Ustalamy małą liczbę drzew i duży learning rate. Następnie iteracyjnie optymalizujemy parametry: maksymalana głębokość drzew, minimalna liczb (waga) dzieci, parametr regularyzacji lambda, parametr gamma. Nastepnie zoptymalizuj liczbę drzew, learning rate i jeszcze raz pozostałe parametry. Przetestuj najlepszy model na zbiorze testowym.\n",
    "\n",
    "Podejście 2\n",
    "\n",
    "Ustalamy małą liczbę drzew i duży learning rate. Następnie \"grid searchem\" zoptymalizuj pozostałe parametry. Powtórz oba kroki dwukrotnie, uwzględniając wyniki z pierwszego kroku - zmień siatki parametrów tak, aby ich zakres nie był za duży i pokrywał okolice najlepszych parametrów.\n",
    "\n",
    "Podejści 3\n",
    "\n",
    "Ustalamy małą liczbę drzew i duży learning rate. Następnie randomizowany grid search po pozostałych parametrach dwuetapowy: w pierwszym etapie po prostu randomizowany grid search, a w drugim etapie zwykły grid search z mała siatką pokrywającą okolice najlepszych wyników. Na koniec zwiekszamy liczbę drzew zmiejszamy learning rate.\n",
    "\n",
    "\n",
    "Inne sensowne podejście (Podobne do 1 i 2, lecz skrócone):\n",
    "https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podejscie 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xbg = XGBClassifier()\n",
    "params = {\"max_depth\":[5,7,10,12,15,20]}\n",
    "cv = GridSearchCV(cv=5, estimator=xbg,param_grid=params)\n",
    "cv.fit(X_train,y_train)\n",
    "cv.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xbg = XGBClassifier().set_params(**cv.best_estimator_.get_params())\n",
    "params = {\"min_child_weight\":[5,10,15,20,30,50]}\n",
    "cv = GridSearchCV(cv=5, estimator=xbg,param_grid=params)\n",
    "cv.fit(X_train,y_train)\n",
    "cv.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xbg = XGBClassifier().set_params(**cv.best_estimator_.get_params())\n",
    "params = {\"reg_lambda\":[0.5,1,2,3]}\n",
    "cv = GridSearchCV(cv=5, estimator=xbg,param_grid=params)\n",
    "cv.fit(X_train,y_train)\n",
    "cv.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xbg = XGBClassifier().set_params(**cv.best_estimator_.get_params())\n",
    "params = {\"n_estimators\":[20,50,100,200,300]}\n",
    "cv = GridSearchCV(cv=5, estimator=xbg,param_grid=params)\n",
    "cv.fit(X_train,y_train)\n",
    "cv.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xbg = XGBClassifier().set_params(**cv.best_estimator_.get_params())\n",
    "params = {\"learning_rate\":[0.01,0.05,0.1,0.15,0.2]}\n",
    "cv = GridSearchCV(cv=5, estimator=xbg,param_grid=params)\n",
    "cv.fit(X_train,y_train)\n",
    "cv.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(cv.best_params_)\n",
    "accuracy_score(y_test,cv.best_estimator_.predict(X_test)), roc_auc_score(y_test,cv.best_estimator_.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podejscie 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xbg = XGBClassifier(learning_rate=0.2,n_estimators=20)\n",
    "params = {\"max_depth\":[5,7,10,12,15,20],\n",
    "         \"min_child_weight\":[5,10,15,20,30,50],\n",
    "         \"reg_lambda\":[0.5,1,2,3]}\n",
    "cv = GridSearchCV(cv=5, estimator=xbg,param_grid=params)\n",
    "cv.fit(X_train,y_train,v)\n",
    "cv.grid_scores_\n",
    "cv.grid_scores_.sort(key= lambda x: x[1],reverse=True)\n",
    "cv.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xbg = XGBClassifier().set_params(**cv.best_estimator_.get_params())\n",
    "params = {\"learning_rate\":[0.01,0.05,0.1,0.15,0.2],\n",
    "         \"n_estimators\":[30,50,100,200]}\n",
    "cv = GridSearchCV(cv=5, estimator=xbg,param_grid=params)\n",
    "cv.fit(X_train,y_train)\n",
    "cv.grid_scores_.sort(key= lambda x: x[1],reverse=True)\n",
    "cv.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(cv.best_params_)\n",
    "accuracy_score(y_test,cv.best_estimator_.predict(X_test)), roc_auc_score(y_test,cv.best_estimator_.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podejscie 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import scipy.stats as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()\n",
    "params = {\"n_estimators\":ss.randint(10,300),\n",
    "         \"learning_rate\":ss.uniform(0.01,0.3),\n",
    "          \"max_depth\": ss.randint(5,30),\n",
    "         \"min_child_weight\":ss.randint(5,50),\n",
    "         \"reg_lambda\":ss.uniform(0.1,3)}\n",
    "\n",
    "cv = RandomizedSearchCV(cv=5, estimator=xbg,param_distributions=params,n_iter=30,verbose=100)\n",
    "cv.fit(X_train,y_train)\n",
    "cv.grid_scores_.sort(key= lambda x: x[1],reverse=True)\n",
    "cv.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = cv.grid_scores_.copy()   ### TRZEBA SKOPIOWAC BO INACZEJ NIE DZIALA\n",
    "x.sort(key= lambda x: x[1],reverse=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(cv.best_params_)\n",
    "accuracy_score(y_test,cv.best_estimator_.predict(X_test)), roc_auc_score(y_test,cv.best_estimator_.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Braki danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jak obsługiwać braki danych?\n",
    "\n",
    "### Propozycja (w przypadku zmiennej numerycznej):\n",
    "\n",
    "1. Gdy dla danej zmiennej braków jest bardzo mało (np. 2% zbioru), wyrzucić obserwacje zawieracjące te braki.\n",
    "\n",
    "2. Gdy dla danej zmiennej braków jest bardzo dużo (ponad 60%), usunąć z danych tę zmienną\n",
    "\n",
    "3. W pozostałych przypadkach uzupełnić braki średnią.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nie, nie, nie i jeszcze raz nie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Podejście rzetelne:\n",
    "\n",
    "1) Spróbować wyjaśnić pochodzenie braków\n",
    "  - może wynikają z błędu w jakimś systemmie -> trzeba naprawić system\n",
    "  - może ktoś nie wiedzał jakie wartości wprowadzić -> trzeba stworzyć lepszą (jakąkolwiek) dokumentację/instrukcję\n",
    "  - itd.\n",
    "  - jak to wygląda w praktyce? ...\n",
    "  - wyjaśnienie jest super, ale z reguły nierealizowalne ( co nie znaczy, że nie warto próbować), dlatego trzeba sobie radzić\n",
    "  \n",
    "2) Spróbować wyjaśnić znaczenie braku\n",
    "  - brak w kolumnie \"liczba dzieci\" -> może oznaczać wartość 0\n",
    "  - brak w kolumnie \"wykształcenie\" -> może oznaczać, że osoba nie ma nawet podstawowego\n",
    "  - brak może wynikać z wartości innej zmiennej -> czy posiadasz dzieci - nie, liczba dzieci - brak\n",
    "  - wówczas braki możemy uzupełnić \n",
    "  \n",
    "3) Jeżeli rozważamy zmienną kategoryczną:\n",
    "  - warto rozważyć nadanie brakom klasy \"brak\" i traktować ją jak normalna wartość\n",
    "  - można tez upełnić wartością występującą najczęściej lub wartością losową\n",
    "  - jeżeli braki stanowią przytłaczającą większość -> można zamienić zmienną na binarną o wartości 1, gdy wystepuje brak lub 0 wpp.\n",
    "\n",
    "4) Jeżeli rozważamy zmienną numeryczną:\n",
    "  - można uzupełnić średnią -> gdy zmienna ma rozkład symetryczny\n",
    "  - można uzupełnić medianą lub modą -> gdy zmienna ma rozkład skośny  \n",
    "  - można tez upełnić wartością występującą wartością losową (z rozsądnego rozkładu)\n",
    "  - jeżeli braki stanowią przytłaczającą większość -> można zamienić zmienną na binarną o wartości 1, gdy wystepuje brak lub 0 wpp.\n",
    "  - można skategoryzować zmienną - wartości numeryczne pogrupować na przedziały i dodać kategorię \"brak\"\n",
    "  - uzupełnić braki i jednocześnie dodać do danych zmienną binarną o wartości 1, gdy wystepuje brak lub 0 wpp.\n",
    "  \n",
    "5) Usunąć obserwacje z brakiem jeśli jest ich mało\n",
    "\n",
    "6) Usunąć zmienną jeśli braków jest dużo\n",
    "\n",
    "7) Uzupełnić używając modelu predykcyjnego z wykorzystaniem pozostałych zmiennych\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case study \n",
    "\n",
    "\n",
    "Opis problemu i danych:\n",
    "https://discuss.analyticsvidhya.com/t/hackathon-3-x-predict-customer-worth-for-happy-customer-bank/3802\n",
    "\n",
    "Interesują nac dwie miary jakości: \n",
    "- AUC\n",
    "- Zysk, liczony w następujący sposób:\n",
    "\n",
    "  - przypisanie obserwacji kosztuje nas 100 \"zł\"\n",
    "  - Trafienie predykcą w klasę 1 przynosi nam 1000 \"zł\" zarobku.\n",
    "\n",
    "Cel: osiągnąć jak największy zysk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('Dane/Dataset/Train_nyOWmfK.csv',encoding=\"latin1\")\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Przykładowa** transformacja danych - nie musi być wcale najlepsza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def transform_data(x,k_most_common_cities=10, k_most_common_Employer_Name=10,\n",
    "                    k_most_common_Salary_Account = 10):\n",
    "    \n",
    "    y = x.drop(\"ID\",axis=1,inplace=False)\n",
    "    \n",
    "    y[\"Age\"] = [115-int(s[-2:]) for s in x.DOB]\n",
    "    y.drop(\"DOB\",axis=1,inplace=True)\n",
    " \n",
    "\n",
    "    y.drop([\"City\",\"Employer_Name\"],axis=1,inplace=True)    \n",
    "    \n",
    "\n",
    "\n",
    "    #most_common_cities = x.City.value_counts()[:k_most_common_cities]\n",
    "    #y.City = x.City.apply(lambda c: c if c in most_common_cities else \"Other\")\n",
    " \n",
    "\n",
    "    y.drop(\"Lead_Creation_Date\",axis=1,inplace=True)\n",
    "    #d_max = max([datetime.datetime.strptime(d, \"%d-%b-%y\") for d in x.Lead_Creation_Date])\n",
    "    #def Lead_Creation_time_distance(d):\n",
    "    #    d = datetime.datetime.strptime(d, \"%d-%b-%y\")\n",
    "    #    dif = d_max-d\n",
    "    #    return dif.days\n",
    "    #y.Lead_Creation_Date = x.Lead_Creation_Date.apply(Lead_Creation_time_distance)\n",
    "    \n",
    "    y.Loan_Amount_Applied.fillna(x['Loan_Amount_Applied'].median(), inplace=True) \n",
    "    \n",
    "    #most_common_Employer_Name = x.Employer_Name.value_counts()[:k_most_common_Employer_Name]\n",
    "    #y.Employer_Name = x.Employer_Name.apply(lambda c: c if c in most_common_Employer_Name else \"Other\")\n",
    "  \n",
    "    y.Existing_EMI.fillna(0, inplace=True)\n",
    "\n",
    "    y.drop(\"Salary_Account\",axis=1,inplace=True)\n",
    "    #most_common_Salary_Account = x.Salary_Account.value_counts()[:k_most_common_Salary_Account]\n",
    "    #y.Salary_Account = x.Salary_Account.apply(lambda c: c if c in most_common_Salary_Account else \"Other\")\n",
    "  \n",
    "    #y.Loan_Amount_Submitted.fillna(0) # juz bez sprawdzania - warto stworzyc dodatkowa zmienna 0-1\n",
    "    \n",
    "    y.Processing_Fee = x.Processing_Fee.apply(lambda r: 1 if pd.isnull(r) else 0)\n",
    "    y.Interest_Rate = x.Interest_Rate.apply(lambda r: 1 if pd.isnull(r) else 0)\n",
    "    \n",
    "    y.Loan_Tenure_Applied = x.Loan_Tenure_Applied.fillna(x.Loan_Tenure_Applied.median())\n",
    "    \n",
    "    \n",
    "    \n",
    "    y.Loan_Amount_Submitted = x.Loan_Amount_Submitted.apply(lambda r: 1 if pd.isnull(r) else 0)\n",
    "    \n",
    "    y.Loan_Tenure_Submitted = x.Loan_Tenure_Submitted.apply(lambda r: 1 if pd.isnull(r) else 0)\n",
    "    \n",
    "    y.EMI_Loan_Submitted = x.EMI_Loan_Submitted.apply(lambda r: 1 if pd.isnull(r) else 0)\n",
    " \n",
    "    #most_common_Source = x.Source.value_counts()[:2]\n",
    "    #y.Source = x.Source.apply(lambda c: c if c in most_common_Source else \"Other\")\n",
    "    \n",
    "\n",
    "    y['Source'] = x['Source'].apply(lambda r: 'others' if r not in ['S122','S133'] else r)\n",
    "  \n",
    "    le = LabelEncoder()\n",
    "    var_to_encode = ['Device_Type','Filled_Form','Gender','Var1','Var2','Mobile_Verified','Source']\n",
    "    for col in var_to_encode:\n",
    "        y[col] = le.fit_transform(y[col])\n",
    "\n",
    "        \n",
    "    y = pd.get_dummies(y, columns=var_to_encode)\n",
    "    \n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = transform_data(data)\n",
    "\n",
    "test_size = 20000\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop([\"LoggedIn\",\"Disbursed\"],axis=1,inplace=False),data.Disbursed,test_size=test_size,random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_pipeline(clf,treshold):\n",
    "    \"\"\"\n",
    "    Funkcja wypisuje rozne miary na podstawie predykcji\n",
    "    \"\"\"\n",
    "    clf.fit(X_train,y_train)\n",
    "    pred = clf.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    print(1000*np.sum(((pred>treshold)==y_test) & (y_test==1))-100*np.sum(pred>treshold))\n",
    "    print(np.mean((pred>treshold)==y_test))\n",
    "    print(roc_auc_score(y_score=pred,y_true=y_test))\n",
    "    print(precision_score(y_pred=pred>treshold,y_true=y_test), recall_score(y_pred=pred>treshold,y_true=y_test), f1_score(y_pred=pred>treshold,y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przykladowe próby rozwiązania. Nie ma tu jakiegoś rozwiązania \"najlpeszego\", ale ogólnie XGBoost przoduje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "logreg = LogisticRegression()\n",
    "sc = StandardScaler()\n",
    "\n",
    "log_reg = Pipeline([(\"SS\",sc),(\"PCA\",pca),(\"LogReg\",logreg)])\n",
    "\n",
    "log_reg.set_params(PCA__n_components=39)\n",
    "log_reg.fit(X_train,y_train)\n",
    "\n",
    "clf = log_reg\n",
    "\n",
    "evaluate_pipeline(clf,0.06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_pipeline(clf,0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_pipeline(clf,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "evaluate_pipeline(xgb,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_estimators\": [50,100,500],\n",
    "    \"max_depth\": [5,10]\n",
    "}\n",
    "gscv = GridSearchCV(xgb,params,\"roc_auc\")#,n)\n",
    "gscv.fit(X_train,y_train)\n",
    "gscv.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb = gscv.best_estimator_\n",
    "\n",
    "clf = xgb\n",
    "\n",
    "evaluate_pipeline(clf,0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb = gscv.best_estimator_\n",
    "\n",
    "clf = xgb\n",
    "\n",
    "evaluate_pipeline(clf,0.0845)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = gscv.best_estimator_\n",
    "\n",
    "zyski = []\n",
    "for treshold in np.linspace(0.05,0.1,num=20):\n",
    "    clf.fit(X_train,y_train)\n",
    "    pred = clf.predict_proba(X_test)[:,1]\n",
    "    zyski.append(1000*np.sum(((pred>treshold)==y_test) & (y_test==1))-100*np.sum(pred>treshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(zyski)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis,QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_pipeline(LinearDiscriminantAnalysis(),0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
