{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analiza danych tekstowych - text mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obróbka tekstu\n",
    "\n",
    "Najważniejszy element analizy tekstów!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki = \"http://en.wikipedia.org/wiki/\"\n",
    "titles = [\"Integral\", \"Riemann_integral\", \"Riemann-Stieltjes_integral\", \"Derivative\",\n",
    "    \"Limit_of_a_sequence\", \"Edvard_Munch\", \"Vincent_van_Gogh\", \"Jan_Matejko\",\n",
    "    \"Lev_Tolstoj\", \"Franz_Kafka\", \"J._R._R._Tolkien\"]\n",
    "\n",
    "from urllib.request import urlopen\n",
    "\n",
    "def parse(url):        \n",
    "    x = urlopen(url)\n",
    "    x = x.read()\n",
    "    x = BeautifulSoup(x)\n",
    "    x = x.find(\"div\",id=\"bodyContent\")\n",
    "    x = x.find_all(\"p\")\n",
    "    return(x)\n",
    "\n",
    "articles = [parse(url) for url in [wiki+x for x in titles]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "articles = [re.sub(\"<.+?>\",\" \",str(a)) for a in articles]\n",
    "articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articles = [nltk.word_tokenize(a) for a in articles]\n",
    "#articles = nltk.tokenize.regexp_tokenize(articles[0],\"(\\w+[-\\w+]?)|\\d+\") # mozna samemu regexem okreslic deffinicje \"slowa\"\n",
    "articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?nltk.word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?nltk.TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "articles = [[w for w in a if w not in string.punctuation] for a in articles]\n",
    "articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articles = [[w.lower() for w in a] for a in articles]\n",
    "articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articles = [[w for w in a if w not in nltk.corpus.stopwords.words(\"english\")] for a in articles]\n",
    "articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.PorterStemmer()\n",
    "articles = [[stemmer.stem(w) for w in a] for a in articles]\n",
    "articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articles = [' '.join(x) for x in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articles[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reprezentacje tekstu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podstawowa reprezentacja macierzowa - liczności"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = CountVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b',min_df=3,max_df=0.5)\n",
    "dtm = c.fit(articles)\n",
    "art = dtm.transform(articles)\n",
    "art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtm.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtm.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "art.getcol(dtm.get_feature_names().index(\"kingdom\")).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "art.getrow(0).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = np.array(dtm.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie - stworzyć macierz ze słowami, w której i-ty wiersz przechowuje k najczęstszych słów z i-tego dokumentu.\n",
    "\n",
    "Mamy tu do czynienia z macierzami rzadkimi - wymagają specjalnej obsługi, bo w normalnej postaci byłyby za duże."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def top_words(M,dtm,k):\n",
    "    words = np.array(dtm.get_feature_names())\n",
    "    return(np.array([words[np.squeeze(np.array(np.argsort(M[i,:].todense())))[-k:]] for i in range(M.shape[0])]))\n",
    "\n",
    "top_words(art,dtm,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSI\n",
    "\n",
    "Czyli SVD przy użyciu gotowego narzędzia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=3) # CHCEMY WZIĄĆ Z MACIERZY V TYLKO 3 CEHCY UKRYTE\n",
    "svd.fit(art)\n",
    "lsi = svd.transform(art) # CZYLI TO JEST TA NASZA V!\n",
    "\n",
    "lsi # PRZEANALIZUJMY PODOBIEŃSTWO - WIERSZE TO DOKUMENTY - JEST DOBRZE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "### https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "?TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "art = tfidf.fit_transform(articles)\n",
    "art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_words(art,tfidf,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytanie: czy tfidf można zastosować do czegoś innego niż wazności słów w teksście?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uwaga: do mierzenia podobieństwa tekstów dobrze sprawdza się miara cosinusowa! (np. w grupowaniu)\n",
    "\n",
    "https://en.wikipedia.org/wiki/Cosine_similarity\n",
    "\n",
    "Mierzy ona podobieństwo wektorów na podstawie rozkładu wartości elementów (proporcji), a nie wartości bewzględnych - matematycznie - podobieństwo jest określane na podstawie kąta pomiędzy wektorami (wartości cosinusa tego kąta), a nie na podstawie długości wektorów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie - otagować teksty korpusu 20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(list(newsgroups_train.target_names)) # kategorie tematyczne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_train.target # kategorie kolejnych dokumentów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_train.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model LDA - Latent Dirichlet Allocation (ukryta alokacja Dirichleta)\n",
    "\n",
    "\n",
    "Motywacja: przedstawienie tekstu jako mieszanki tematów.\n",
    "\n",
    "\n",
    "Temat - rozkład prawdopodobieństwa na zbiorze słów.\n",
    "\n",
    "\n",
    "Przykład:\n",
    "*  <s>Mam</s> gorączkę <s>i</s> katar.\n",
    "* Graliśmy <s>i</s> siatkówkę.\n",
    "* Sport <s>to</s> zdrowie.\n",
    "\n",
    "\n",
    "Ile \"tematów\" widzimy?\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "Intyicyjnie: dwa tematy: \"sport\" oraz \"zdrowie\"\n",
    "* Pierwsze zdanie = 100% zdrowie\n",
    "* Drugie zdanie = 100% sport\n",
    "* Trzecie zdanie = 50% sport + 50% zdrowie\n",
    "\n",
    "\n",
    "## Rozkład Dirichleta \n",
    "\n",
    "Jest to rozkład, na którym opiera się model LDA\n",
    "\n",
    "Gęstość trójwymiarowego rozkładu Dirichleta Dir($\\alpha$).\n",
    "\n",
    "Wektor losowy $(x_1 , ..., x_K )$ z $K$-wymiarowego rozkładu Dirichleta to punkt na $(K-1)$-wymiarowym sympleksie, czyli $x_1 + ... + x_K = 1$, $x_i \\geq 0$.\n",
    "\n",
    "$\\alpha=3$ | $\\alpha=0.95$\n",
    "- | - \n",
    "![alt](dir3.jpg) | ![alt](dir095.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ale $\\alpha$ może być wektorem, czyli $\\alpha = (\\alpha_1, ..., \\alpha_K)$. Co wówczas?\n",
    "\n",
    "<img src=\"dirichlet.png\" widht=\"200\">\n",
    "\n",
    "Źródło: http://jonathan-huang.org/research/hln/hlnfit.html\n",
    "\n",
    "### Czyli rozkład jest niesymetryczny. Odpowiada to temu, że tematy mogą mieć różną częstość, co bardzo odpowiada rzeczywistości. Wtedy wartość oczekiwana $X_i$ (i-tej współrzędnej wektora z rozkładu Dirichleta) wynosi \n",
    "## $$\\frac{\\alpha_i}{\\sum\\limits_{j=1}^K \\alpha_j}$$\n",
    "\n",
    "czyli średni udział $i$-tego tematu jest proporcjonalny do $\\alpha_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wizualizacja wartości z rozkłau Dirichleta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "for i in range(1,13):\n",
    "    plt.subplot(2,6,i)\n",
    "    plt.bar(range(5),np.random.dirichlet((0.5, 0.1, 0.1, 0.1, 0.1), 1)[0])\n",
    "    plt.ylim(0,1)\n",
    "    plt.xticks(range(5))\n",
    "plt.show()\n",
    "\n",
    "# PIERWSZY JEST NAJCZĘSTSZY (bo piwerwsza wartość w wektorze alfa jest największa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = np.random.dirichlet((0.2, 0.1, 0.1), 20).transpose()\n",
    "\n",
    "plt.barh(range(20), s[0])\n",
    "plt.barh(range(20), s[1], left=s[0], color='g')\n",
    "plt.barh(range(20), s[2], left=s[0]+s[1], color='r')\n",
    "plt.title(\"Lengths of Strings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "# Model LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"LDA_doc.jpg\">\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"Smoothed_LDA.png\">\n",
    "\n",
    "\n",
    ",gdzie\n",
    "\n",
    "$\\theta_d \\sim Dir(\\alpha)$  - rozkład tematów w dokumencie\n",
    "\n",
    "$Z \\sim Discr(\\theta)$ - temat, którego pochodz słowo\n",
    "\n",
    "$W \\sim Discr(\\phi_Z)$ - słowo\n",
    "\n",
    "$\\phi_i \\sim Dir(\\beta)$ - tematy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dokumenty będą składać sie tylko z kilku tematów (alfa będzie mała)\n",
    "\n",
    "Tematy będą charakteryzowane również tylko przez cześć słów (beta małe). Dzięki temu możemy ludzkim okiem rozróżnić i zintepretować tematy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uwaga\n",
    "\n",
    "W modelu LDA są 3 parametry: K - liczba tematów, wektor lub skalar alfa oraz wektor lub skalar beta.\n",
    "\n",
    "Dobre oprogramowanie do pracy z modelem LDA to takie, które wymaga podania od użytkownika jedynie K, a parametry alfa i beta estymuje sobie sam (wykorzystując uznane metody estymacji optymalnych ich wartości) i pozwala wybrać czy mają być skalarami czy wektorami. Alfa w praktyce powinna być wektorem - róże wartości elementów wektora odpowiadają różnym częstościom tematów. Natomiast Beta czasem może dać lepsze wyniki jako wektor, a czasem jako skalar. Takim narzędziem jest np. MALLET (http://mallet.cs.umass.edu/). W modułach Pythona takich elastycznych funkcjonalności nie ma, ale można z tym żyć.\n",
    "\n",
    "Istnieją dwie powszechnie stosowane metody dopasowaywania modelu: wnioskowanie wariacyjne oraz próbkowanie Gibbsa. Ta druga metoda jest lepsza. (https://stats.stackexchange.com/questions/8485/a-good-gibbs-sampling-tutorials-and-references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA w module GENSIM\n",
    "\n",
    "gensim to modul stworzony do analizy tekstów: https://radimrehurek.com/gensim/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "a = [nltk.word_tokenize(x) for x in articles]\n",
    "\n",
    "dictionary = corpora.Dictionary(a)\n",
    "corpus = [dictionary.doc2bow(text) for text in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary.keys()[:10]  # - ID slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary[0]   # - slowa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus[0] # - dokumenty w postali listy krotek (id slowa, licznosc wystapien tego slowa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LdaModel(corpus=corpus,id2word=dictionary,num_topics=3,alpha=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?LdaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"auto\" dla parametrów alfa i eta (eta jest uzywane zamiennie z beta) oznacza, że model znajdzie optymalne wartości, ale będą to wektory.\n",
    "\n",
    "Do uczenia modelu wykorzystywane jest wnioskowanie wariacyjne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wypisania rozkładów tematów w dokumentach (jest trudne bo dane są zapisane w formacie rzadkim zapisana jest tylko cześć tematów - te, które mają bardzo niski udział zostają pominięte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(articles)):\n",
    "    print(model.get_document_topics(corpus[i],minimum_probability=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rozkład słów w temacie $k$ - analogicznie jak wyżej, dane są w formacie rzadkim, dlatego mamy tylko najczęstsze słowa. Są to krotki (id_slowa, prawdopodobieństwo wystapienia słowa w tym temacie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 0\n",
    "model.get_topic_terms(topicid=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA w module lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lda\n",
    "import lda.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Wczytujemy dane: jest to zbiór notek prasowych agencji REUTERS\n",
    "\n",
    "X = lda.datasets.load_reuters()\n",
    "vocab = lda.datasets.load_reuters_vocab()\n",
    "titles = lda.datasets.load_reuters_titles()\n",
    "print(X)                                # - macierz wystapien slow\n",
    "print(vocab[:10])                       # - lista slow\n",
    "print(titles[:10])                      # - tytuly dokumentow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?lda.LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten moduł działa dla ustalonych parametrów alfa i beta - to jest słabe.\n",
    "\n",
    "Ten moduł dopasowuje model przy użyciu próbkowania Gibbsa - to jest dobre (n_iter to parametr tej metody - liczba iteracji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = lda.LDA(n_topics=20, n_iter=500,random_state=1)\n",
    "model.fit(X)\n",
    "\n",
    "#Funkcja drukuje postęp uczenia modelu - iteracje i wartość wunkcji log wiarogodności (i na początku kilka dodatkowych info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_word = model.topic_word_\n",
    "n_top_words = 8\n",
    "\n",
    "print(topic_word) # macierz prawdopodobieństw słów w tematach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wypisanie najczestszych slow w tematach\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-n_top_words:-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.components_ # tematy - jeden wiersz = jeden temat   to samo model.topic_word_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.loglikelihood() # wzrtość funkcji log wiarogodności modelu "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie: Pogrupować zbiór 20NEWSGROUP.\n",
    "\n",
    "Przetestować różne reprezentacje korpusu:\n",
    "\n",
    "- macierz liczności wystąpień słów\n",
    "- macierz tfidf\n",
    "- lsi\n",
    "- lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Materiały i źródła:\n",
    "\n",
    "https://radimrehurek.com/gensim/wiki.html\n",
    "\n",
    "http://radimrehurek.com/topic_modeling_tutorial/2%20-%20Topic%20Modeling.html\n",
    "\n",
    "https://www.ariddell.org/lda.html\n",
    "\n",
    "pokazać na koniec modelu LDA: http://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/pyLDAvis_overview.ipynb\n",
    "\n",
    "http://brandonrose.org/clustering\n",
    "\n",
    "http://totoharyanto.staff.ipb.ac.id/files/2012/10/Building-Machine-Learning-Systems-with-Python-Richert-Coelho.pdf\n",
    "\n",
    "https://de.dariah.eu/tatom/topic_model_python.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
