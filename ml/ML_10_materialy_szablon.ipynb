{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import nltk\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 75\n",
    "\n",
    "with open(\"Dane/kod.txt\") as f:\n",
    "    sentences = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [s for s in sentences if s.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['class RNNNumpy:\\n',\n",
       " '    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\\n',\n",
       " '        # Assign instance variables\\n',\n",
       " '        self.word_dim = word_dim\\n',\n",
       " '        self.hidden_dim = hidden_dim\\n',\n",
       " '        self.bptt_truncate = bptt_truncate\\n',\n",
       " '        # Randomly initialize the network parameters\\n',\n",
       " '        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\\n',\n",
       " '        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\\n',\n",
       " '        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_sentences = [list(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['c',\n",
       "  'l',\n",
       "  'a',\n",
       "  's',\n",
       "  's',\n",
       "  ' ',\n",
       "  'R',\n",
       "  'N',\n",
       "  'N',\n",
       "  'N',\n",
       "  'u',\n",
       "  'm',\n",
       "  'p',\n",
       "  'y',\n",
       "  ':',\n",
       "  '\\n'],\n",
       " [' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'd',\n",
       "  'e',\n",
       "  'f',\n",
       "  ' ',\n",
       "  '_',\n",
       "  '_',\n",
       "  'i',\n",
       "  'n',\n",
       "  'i',\n",
       "  't',\n",
       "  '_',\n",
       "  '_',\n",
       "  '(',\n",
       "  's',\n",
       "  'e',\n",
       "  'l',\n",
       "  'f',\n",
       "  ',',\n",
       "  ' ',\n",
       "  'w',\n",
       "  'o',\n",
       "  'r',\n",
       "  'd',\n",
       "  '_',\n",
       "  'd',\n",
       "  'i',\n",
       "  'm',\n",
       "  ',',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'i',\n",
       "  'd',\n",
       "  'd',\n",
       "  'e',\n",
       "  'n',\n",
       "  '_',\n",
       "  'd',\n",
       "  'i',\n",
       "  'm',\n",
       "  '=',\n",
       "  '1',\n",
       "  '0',\n",
       "  '0',\n",
       "  ',',\n",
       "  ' ',\n",
       "  'b',\n",
       "  'p',\n",
       "  't',\n",
       "  't',\n",
       "  '_',\n",
       "  't',\n",
       "  'r',\n",
       "  'u',\n",
       "  'n',\n",
       "  'c',\n",
       "  'a',\n",
       "  't',\n",
       "  'e',\n",
       "  '=',\n",
       "  '4',\n",
       "  ')',\n",
       "  ':',\n",
       "  '\\n'],\n",
       " [' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  '#',\n",
       "  ' ',\n",
       "  'A',\n",
       "  's',\n",
       "  's',\n",
       "  'i',\n",
       "  'g',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'i',\n",
       "  'n',\n",
       "  's',\n",
       "  't',\n",
       "  'a',\n",
       "  'n',\n",
       "  'c',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'v',\n",
       "  'a',\n",
       "  'r',\n",
       "  'i',\n",
       "  'a',\n",
       "  'b',\n",
       "  'l',\n",
       "  'e',\n",
       "  's',\n",
       "  '\\n']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocabulary size 75.\n",
      "\n",
      "Example sentence: 'class RNNNumpy:\n",
      "'\n",
      "\n",
      "Example sentence after Pre-processing: '['c', 'l', 'a', 's', 's', ' ', 'R', 'N', 'N', 'N', 'u', 'm', 'p', 'y', ':', '\\n']'\n",
      "[50, 59, 48, 66, 66, 1, 37, 34, 34, 34, 68, 60, 63, 72, 19]\n",
      "[59, 48, 66, 66, 1, 37, 34, 34, 34, 68, 60, 63, 72, 19, 0]\n"
     ]
    }
   ],
   "source": [
    "vocab = np.unique([item for sublist in tokenized_sentences for item in sublist]) \n",
    "index_to_word = [x[0] for x in vocab]\n",
    "\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    " \n",
    "print( \"Using vocabulary size %d.\" % len(vocab))\n",
    "\n",
    "# Create the training data\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])\n",
    "\n",
    "\n",
    "print( \"\\nExample sentence: '%s'\" % sentences[0])\n",
    "print( \"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[0])\n",
    "print( X_train[0])\n",
    "print( y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNNumpy:\n",
    "     \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(self, x):\n",
    "    # The total number of time steps\n",
    "    T = len(x)\n",
    "    # During forward propagation we save all hidden states in s because need them later.\n",
    "    # We add one additional element for the initial hidden, which we set to 0\n",
    "    h = np.zeros((T + 1, self.hidden_dim))\n",
    "    h[-1] = np.zeros(self.hidden_dim)\n",
    "    # The outputs at each time step. Again, we save them for later.\n",
    "    o = np.zeros((T, self.word_dim))\n",
    "    # For each time step...\n",
    "    for t in np.arange(T):\n",
    "        # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
    "         \n",
    "        x_t =  np.eye(len(vocab))[x[t]] # Kodowanie one-hot. \n",
    "                                        # Formalnie rzecz biorac, x_t jest gotowym wektorem wprowadzanym do sieci,\n",
    "                                        # ale dla wygody tu przetwarzamy indeks na wektor\n",
    "        \n",
    "        h[t] = np.tanh(np.dot(self.U, x_t) + np.dot(self.W, h[t-1])) # obliczanie stanu ukrytego\n",
    "        o[t] = softmax(np.dot(self.V, h[t])) # predykcja dla x_t\n",
    "                                             # uwaga: w zalenosci od problemu bedzie nas interesowac \n",
    "                                             #        kazda predykcja lub tylko ostatnia\n",
    "        \n",
    "    return [o, h]\n",
    " \n",
    "RNNNumpy.forward_propagation = forward_propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "    # Perform forward propagation and return index of the highest score\n",
    "    o, s = self.forward_propagation(x)\n",
    "    return np.argmax(o, axis=1)\n",
    " \n",
    "RNNNumpy.predict = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 75)\n",
      "[[ 0.0137885   0.01259464  0.01312937 ...,  0.0121929   0.01291467\n",
      "   0.01447739]\n",
      " [ 0.01307076  0.01411536  0.01411408 ...,  0.01378971  0.01418804\n",
      "   0.01405048]\n",
      " [ 0.01281134  0.01272381  0.01326489 ...,  0.01275765  0.01318253\n",
      "   0.01373512]\n",
      " ..., \n",
      " [ 0.01314944  0.01428113  0.01362903 ...,  0.01324931  0.0143878\n",
      "   0.01310582]\n",
      " [ 0.013139    0.01351207  0.0123365  ...,  0.01295007  0.01473019\n",
      "   0.0127632 ]\n",
      " [ 0.01313058  0.01459512  0.01356838 ...,  0.01243141  0.01246906\n",
      "   0.01237279]]\n"
     ]
    }
   ],
   "source": [
    "# Test:\n",
    "\n",
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "o, s = model.forward_propagation(X_train[10])\n",
    "print(o.shape)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33,)\n",
      "[58 68  5 56 45 67 11 42 69 11 26 48 58 11 53  1 68 15 69 52 20 24 46 27 34\n",
      " 21 24  5 57 56 69 73 49]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_train[10])\n",
    "print(predictions.shape)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_total_loss(self, x, y):\n",
    "    L = 0\n",
    "    # For each sentence...\n",
    "    for i in np.arange(len(y)):\n",
    "        o, s = self.forward_propagation(x[i])\n",
    "        # We only care about our prediction of the \"correct\" words\n",
    "        correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "        # Add to the loss based on how off we were\n",
    "        L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "    return L\n",
    " \n",
    "def calculate_loss(self, x, y):\n",
    "    # Divide the total loss by the number of training examples\n",
    "    N = np.sum((len(y_i) for y_i in y))\n",
    "    return self.calculate_total_loss(x,y)/N\n",
    "\n",
    "RNNNumpy.calculate_total_loss = calculate_total_loss\n",
    "RNNNumpy.calculate_loss = calculate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for random predictions: 4.317488\n",
      "Actual loss: 4.324430\n"
     ]
    }
   ],
   "source": [
    "# Limit to 1000 examples to save time\n",
    "print(\"Loss for random predictions: %f\" % np.log(vocabulary_size))\n",
    "print(\"Actual loss: %f\" % model.calculate_loss(X_train[:1000], y_train[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bptt(self, x, y):\n",
    "    T = len(y)\n",
    "    # Perform forward propagation\n",
    "    o, s = self.forward_propagation(x)\n",
    "    \n",
    "    # We accumulate the gradients in these variables\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape)\n",
    "    delta_o = o\n",
    "    delta_o[np.arange(len(y)), y] -= 1.\n",
    "    \n",
    "    # For each output backwards...\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        dLdV += np.outer(delta_o[t], s[t].T)\n",
    "        # Initial delta calculation\n",
    "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "        # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "            # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "            dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
    "            dLdU[:,x[bptt_step]] += delta_t\n",
    "            # Update delta for next step\n",
    "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "    return [dLdU, dLdV, dLdW]\n",
    " \n",
    "RNNNumpy.bptt = bptt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 10.1 ms per loop\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "\n",
    "# Performs one step of SGD.\n",
    "def numpy_sdg_step(self, x, y, learning_rate):\n",
    "    # Calculate the gradients\n",
    "    dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "    # Change parameters according to gradients and learning rate\n",
    "    self.U -= learning_rate * dLdU\n",
    "    self.V -= learning_rate * dLdV\n",
    "    self.W -= learning_rate * dLdW\n",
    " \n",
    "RNNNumpy.sgd_step = numpy_sdg_step\n",
    "# Outer SGD Loop\n",
    "# - model: The RNN model instance\n",
    "# - X_train: The training data set\n",
    "# - y_train: The training data labels\n",
    "# - learning_rate: Initial learning rate for SGD\n",
    "# - nepoch: Number of times to iterate through the complete dataset\n",
    "# - evaluate_loss_after: Evaluate the loss after this many epochs\n",
    "def train_with_sgd(self, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "    # We keep track of the losses so we can plot them later\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        # Optionally evaluate the loss\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = model.calculate_loss(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            \n",
    "            # Adjust the learning rate if loss increases\n",
    "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5 \n",
    "                print(\"Setting learning rate to %f\" % learning_rate)\n",
    "            sys.stdout.flush()\n",
    "        # For each training example...\n",
    "        for i in range(len(y_train)):\n",
    "            # One SGD step\n",
    "            self.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1\n",
    "    return(losses)\n",
    "\n",
    "RNNNumpy.train_with_sgd = train_with_sgd\n",
    "            \n",
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "%timeit model.sgd_step(X_train[10], y_train[10], 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting learning rate to 0.002500\n",
      "Setting learning rate to 0.001250\n",
      "Setting learning rate to 0.000625\n",
      "Setting learning rate to 0.000313\n",
      "Setting learning rate to 0.000156\n",
      "Setting learning rate to 0.000078\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "# Train on a small subset of the data to see what happens\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "losses = model.train_with_sgd(X_train, y_train, nepoch=100, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_text(s,n=3):\n",
    "    \"\"\"\n",
    "    \n",
    "    Funkcja przewiduje n nastepnych liter dla podanegou tekstu (napisu) s.\n",
    "    \n",
    "    \"\"\"\n",
    "    s = list(s)\n",
    "    \n",
    "    X_new = np.asarray([[word_to_index[w] for w in sent] for sent in s])[:,0]\n",
    "    \n",
    "    pred = np.zeros(n,dtype=\"int\")\n",
    "    for i in range(n):\n",
    "        pred[i] = model.predict(np.concatenate([X_new, pred[:i]]))[-1]\n",
    "    print(\"original: \", ''.join([x[0] for x in s]) )\n",
    "    print(\"prediction: \", ''.join([x[0] for x in s])+''.join([[index_to_word[w] for w in sent] for sent in [pred]][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original:  la\n",
      "prediction:  late \n"
     ]
    }
   ],
   "source": [
    "generate_text('la', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original:  pri\n",
      "prediction:  print(\"%\n"
     ]
    }
   ],
   "source": [
    "generate_text('pri', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original:  r\n",
      "prediction:  renum_oralen if ranum\n"
     ]
    }
   ],
   "source": [
    "generate_text('r', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original:  R\n",
      "prediction:  RNNNumpy.calculate_to\n"
     ]
    }
   ],
   "source": [
    "generate_text('R', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original:  R\n",
      "prediction:  RNNNumpy.calculate_total_loss(self, x, y):\n",
      "(x[n])\n",
      ", forea_d_pracanin  h time toumeftime(tor(sed_dielo\n"
     ]
    }
   ],
   "source": [
    "generate_text('R', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case study: IMBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMDB dataset CERTIFICATE_VERIFY_FAILED exception workaround\n",
    "import requests\n",
    "requests.packages.urllib3.disable_warnings()\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    # Legacy Python that doesn't verify HTTPS certificates by default\n",
    "    pass\n",
    "else:\n",
    "    # Handle target environment that doesn't support HTTPS verification\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, SimpleRNN, LSTM, Bidirectional\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n",
      "[ [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
      " [1, 194, 1153, 194, 2, 78, 228, 5, 6, 1463, 4369, 2, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 2, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 2, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 2, 2, 349, 2637, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 2, 5, 2, 656, 245, 2350, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]\n",
      " [1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 2, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 2, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]]\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "print(x_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 400)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Przetworzenie danych:\n",
    "\n",
    "# Padding\n",
    "# uzupelnienie zerami aby wyrownac wielkosc tablic\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    1,   14,   22,   16,   43,\n",
       "         530,  973, 1622, 1385,   65,  458, 4468,   66, 3941,    4,  173,\n",
       "          36,  256,    5,   25,  100,   43,  838,  112,   50,  670,    2,\n",
       "           9,   35,  480,  284,    5,  150,    4,  172,  112,  167,    2,\n",
       "         336,  385,   39,    4,  172, 4536, 1111,   17,  546,   38,   13,\n",
       "         447,    4,  192,   50,   16,    6,  147, 2025,   19,   14,   22,\n",
       "           4, 1920, 4613,  469,    4,   22,   71,   87,   12,   16,   43,\n",
       "         530,   38,   76,   15,   13, 1247,    4,   22,   17,  515,   17,\n",
       "          12,   16,  626,   18,    2,    5,   62,  386,   12,    8,  316,\n",
       "           8,  106,    5,    4, 2223,    2,   16,  480,   66, 3785,   33,\n",
       "           4,  130,   12,   16,   38,  619,    5,   25,  124,   51,   36,\n",
       "         135,   48,   25, 1415,   33,    6,   22,   12,  215,   28,   77,\n",
       "          52,    5,   14,  407,   16,   82,    2,    8,    4,  107,  117,\n",
       "           2,   15,  256,    4,    2,    7, 3766,    5,  723,   36,   71,\n",
       "          43,  530,  476,   26,  400,  317,   46,    7,    4,    2, 1029,\n",
       "          13,  104,   88,    4,  381,   15,  297,   98,   32, 2071,   56,\n",
       "          26,  141,    6,  194,    2,   18,    4,  226,   22,   21,  134,\n",
       "         476,   26,  480,    5,  144,   30,    2,   18,   51,   36,   28,\n",
       "         224,   92,   25,  104,    4,  226,   65,   16,   38, 1334,   88,\n",
       "          12,   16,  283,    5,   16, 4472,  113,  103,   32,   15,   16,\n",
       "           2,   19,  178,   32],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    1,  194, 1153,  194,    2,   78,  228,    5,    6,\n",
       "        1463, 4369,    2,  134,   26,    4,  715,    8,  118, 1634,   14,\n",
       "         394,   20,   13,  119,  954,  189,  102,    5,  207,  110, 3103,\n",
       "          21,   14,   69,  188,    8,   30,   23,    7,    4,  249,  126,\n",
       "          93,    4,  114,    9, 2300, 1523,    5,  647,    4,  116,    9,\n",
       "          35,    2,    4,  229,    9,  340, 1322,    4,  118,    9,    4,\n",
       "         130, 4901,   19,    4, 1002,    5,   89,   29,  952,   46,   37,\n",
       "           4,  455,    9,   45,   43,   38, 1543, 1905,  398,    4, 1649,\n",
       "          26,    2,    5,  163,   11, 3215,    2,    4, 1153,    9,  194,\n",
       "         775,    7,    2,    2,  349, 2637,  148,  605,    2,    2,   15,\n",
       "         123,  125,   68,    2,    2,   15,  349,  165, 4362,   98,    5,\n",
       "           4,  228,    9,   43,    2, 1157,   15,  299,  120,    5,  120,\n",
       "         174,   11,  220,  175,  136,   50,    9, 4373,  228,    2,    5,\n",
       "           2,  656,  245, 2350,    5,    4,    2,  131,  152,  491,   18,\n",
       "           2,   32,    2, 1212,   14,    9,    6,  371,   78,   22,  625,\n",
       "          64, 1382,    9,    8,  168,  145,   23,    4, 1690,   15,   16,\n",
       "           4, 1355,    5,   28,    6,   52,  154,  462,   33,   89,   78,\n",
       "         285,   16,  145,   95]], dtype=int32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_train = 5000\n",
    "n_test = 2000\n",
    "x_train = x_train[:n_train]\n",
    "y_train = y_train[:n_train]\n",
    "x_test = x_test[:n_train]\n",
    "y_test = y_test[:n_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddingi\n",
    "\n",
    "Przeanalizujmy co się dzieje w RNN, gdy podajemy słowa w reprezentacji one hot.\n",
    "\n",
    "## $$ h_t = f( W^h * h_{t-1} + W^x * x_t + b)$$\n",
    "\n",
    "Zatem jeśli x to \"one-hot\" z 1 na pozycji i to:\n",
    "\n",
    "## $$ W^x * x_t = W^x[:,i],  $$\n",
    "\n",
    "Czyli wkład informacji embeddinga sprowadza się do wzięcia odpowieniej kolumny macierzy wag.\n",
    "\n",
    "Czyli i-ta kolumna macierzy wag jest w pewnym sensie reprezentacją słowa i.\n",
    "\n",
    "Zatem pójdźmy krok dalej: stwórzmy sobie dodatkową warstwę w sieci, zawierającą reprezentacje słów, które będą przekazywane do wyliczenia stanu ukrytego.\n",
    "\n",
    "\n",
    "Wówczas sieć z warstwą embeddingów ma postać:\n",
    "\n",
    "$x_t$ - id słowa wejściowego w momencie $t$.\n",
    "\n",
    "$EMB$ - macierz embeddingów\n",
    "\n",
    "<br>\n",
    "\n",
    "$$emb_t = EMB[x_t]$$\n",
    "$$ h_t = f( W^h * h_{t-1} + W^x * emb_t + b)$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Ta warstwa nazywa się EMBEDDING'ami (embedding layer).\n",
    "\n",
    "\n",
    "<img src=\"https://image.slidesharecdn.com/translatefrombadenglishtogoodone-2-160606105036/95/aibigdata-lab-2016-11-638.jpg?cb=1465210454\" width=\"700\">\n",
    "Źródło: https://www.slideshare.net/Geeks_Lab/aibigdata-lab-2016-62764857\n",
    "\n",
    "\n",
    "\n",
    "### Zauważmy, że embeddingi są parametrami sieci, ale jednocześnie reprezentacją słów. Oznacza to, że trenując sieć, uczymy embeddingi, czyli uczymy się reprezentacji słów.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zwykład sieć rekurencyjna ( z embeddingami)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3750 samples, validate on 1250 samples\n",
      "Epoch 1/100\n",
      "3750/3750 [==============================] - 35s - loss: 0.6980 - acc: 0.5061 - val_loss: 0.6906 - val_acc: 0.5104\n",
      "Epoch 2/100\n",
      "3750/3750 [==============================] - 35s - loss: 0.6704 - acc: 0.5925 - val_loss: 0.6780 - val_acc: 0.5632\n",
      "Epoch 3/100\n",
      "3750/3750 [==============================] - 35s - loss: 0.6210 - acc: 0.6904 - val_loss: 0.6718 - val_acc: 0.5632\n",
      "Epoch 4/100\n",
      "3750/3750 [==============================] - 36s - loss: 0.5114 - acc: 0.7693 - val_loss: 0.6366 - val_acc: 0.6392\n",
      "Epoch 5/100\n",
      "3750/3750 [==============================] - 38s - loss: 0.4692 - acc: 0.7912 - val_loss: 0.6532 - val_acc: 0.6224\n",
      "Epoch 6/100\n",
      "3750/3750 [==============================] - 39s - loss: 0.5149 - acc: 0.7504 - val_loss: 0.6504 - val_acc: 0.6232\n",
      "Epoch 7/100\n",
      "3750/3750 [==============================] - 42s - loss: 0.4077 - acc: 0.8099 - val_loss: 0.6884 - val_acc: 0.6408\n",
      "Epoch 8/100\n",
      "3750/3750 [==============================] - 41s - loss: 0.3238 - acc: 0.8573 - val_loss: 0.7022 - val_acc: 0.6624\n",
      "Epoch 9/100\n",
      "3750/3750 [==============================] - 42s - loss: 0.2697 - acc: 0.8864 - val_loss: 0.7676 - val_acc: 0.6528\n",
      "Epoch 10/100\n",
      "3750/3750 [==============================] - 43s - loss: 0.2044 - acc: 0.9165 - val_loss: 0.8396 - val_acc: 0.6696\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1163a6b00>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "model.add(SimpleRNN(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5, monitor='val_loss')\n",
    "\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=batch_size, callbacks=[early_stopping], validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 14s    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.67300000000000004"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RNN + dense pomiędzy zwracanym stanem ukrytym a outputem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3750 samples, validate on 1250 samples\n",
      "Epoch 1/3\n",
      "3750/3750 [==============================] - 42s - loss: 0.6956 - acc: 0.4997 - val_loss: 0.6889 - val_acc: 0.5336\n",
      "Epoch 2/3\n",
      "3750/3750 [==============================] - 44s - loss: 0.6395 - acc: 0.6480 - val_loss: 0.5999 - val_acc: 0.6832\n",
      "Epoch 3/3\n",
      "3750/3750 [==============================] - 44s - loss: 0.4428 - acc: 0.8045 - val_loss: 0.5157 - val_acc: 0.7624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11ece4eb8>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "model.add(SimpleRNN(100))\n",
    "model.add(Dense(100, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=3, batch_size=batch_size, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 15s    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.77439999999999998"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 400, 50)           250000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_3 (SimpleRNN)     (None, 100)               15100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 275,301\n",
      "Trainable params: 275,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dwuwarstwowa sieć rekurencyjna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3750 samples, validate on 1250 samples\n",
      "Epoch 1/3\n",
      "3750/3750 [==============================] - 83s - loss: 0.7198 - acc: 0.4997 - val_loss: 0.6969 - val_acc: 0.4896\n",
      "Epoch 2/3\n",
      "3750/3750 [==============================] - 83s - loss: 0.6981 - acc: 0.5125 - val_loss: 0.6941 - val_acc: 0.5136\n",
      "Epoch 3/3\n",
      "3750/3750 [==============================] - 92s - loss: 0.6871 - acc: 0.5389 - val_loss: 0.6856 - val_acc: 0.5264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11ff01a90>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "model.add(SimpleRNN(100, return_sequences=True))\n",
    "model.add(SimpleRNN(50))\n",
    "\n",
    "model.add(Dense(100, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=3, batch_size=batch_size, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 32s    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.51300000000000001"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 400, 50)           250000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_4 (SimpleRNN)     (None, 400, 100)          15100     \n",
      "_________________________________________________________________\n",
      "simple_rnn_5 (SimpleRNN)     (None, 50)                7550      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 277,851\n",
      "Trainable params: 277,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dwukierunkowa sieć rekurencyjna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3750 samples, validate on 1250 samples\n",
      "Epoch 1/3\n",
      "3750/3750 [==============================] - 50s - loss: 0.7062 - acc: 0.4893 - val_loss: 0.7025 - val_acc: 0.5104\n",
      "Epoch 2/3\n",
      "3750/3750 [==============================] - 51s - loss: 0.6855 - acc: 0.5499 - val_loss: 0.6803 - val_acc: 0.5280\n",
      "Epoch 3/3\n",
      "3750/3750 [==============================] - 51s - loss: 0.5320 - acc: 0.7307 - val_loss: 0.5668 - val_acc: 0.6728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x122253dd8>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "\n",
    "model.add(Bidirectional(SimpleRNN(100)))\n",
    "\n",
    "model.add(Dense(100, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=3, batch_size=batch_size, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 22s    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.68720000000000003"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 400, 50)           250000    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200)               30200     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 300,401\n",
      "Trainable params: 300,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dwukierunkowa sieć rekurencyjna + dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie. Powtórz powyższe modele z komórką LSTM\n",
    "\n",
    "Przyjąć patience = 1 w early stoppingu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 400, 50)           250000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 310,501\n",
      "Trainable params: 310,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3750 samples, validate on 1250 samples\n",
      "Epoch 1/100\n",
      "3750/3750 [==============================] - 215s - loss: 0.6646 - acc: 0.6109 - val_loss: 0.5826 - val_acc: 0.7184\n",
      "Epoch 2/100\n",
      "3750/3750 [==============================] - 239s - loss: 0.4128 - acc: 0.8216 - val_loss: 0.5023 - val_acc: 0.7632\n",
      "Epoch 3/100\n",
      "3750/3750 [==============================] - 224s - loss: 0.2408 - acc: 0.9075 - val_loss: 0.4809 - val_acc: 0.7928\n",
      "Epoch 4/100\n",
      "3750/3750 [==============================] - 224s - loss: 0.1614 - acc: 0.9445 - val_loss: 0.5989 - val_acc: 0.7928\n",
      "Epoch 5/100\n",
      "3750/3750 [==============================] - 1829s - loss: 0.1416 - acc: 0.9512 - val_loss: 0.6432 - val_acc: 0.7288\n",
      "Epoch 6/100\n",
      "3750/3750 [==============================] - 228s - loss: 0.1578 - acc: 0.9411 - val_loss: 0.6511 - val_acc: 0.7944\n",
      "5000/5000 [==============================] - 57s    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.79700000000000004"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "early_stopping = EarlyStopping(patience=2, monitor='val_loss')\n",
    "\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=batch_size, callbacks=[early_stopping], validation_split=0.25)\n",
    "\n",
    "model.evaluate(x_test, y_test)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case study: Analiza sentymentu\n",
    "\n",
    "Zaprogramować eksperymenty i puścić przed wyjściem na obiad.\n",
    "\n",
    "1. Simple RNN\n",
    "2. LSTM - porównaj na zbiorze testowym jakość działania modelu wziętego z najlepszej iteracji oraz modelu po zatrzymaniu uczenia\n",
    "3. LSTM + warstwa dense na końcu\n",
    "4. BiLSTM\n",
    "5. dwuwarstwowy LSTM\n",
    "6. CNN + LSTM - przepuścić dane przez warstwę konwolucyjną (conv1d) + max pooling, a następnie przejechać LSTM'em po tym wyszło."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 300)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "file_with_filtered_embeddings = \"Dane/data_poleval/embeddings.txt\"\n",
    "\n",
    "words2ids = {}\n",
    "embeddings = []\n",
    "\n",
    "embeddings.append(np.zeros(300)) # rezerwujemy embeddingi na paddin i nieznane slowa\n",
    "embeddings.append(np.zeros(300))\n",
    "\n",
    "i = 0\n",
    "with open(file_with_filtered_embeddings,\"r\") as f:\n",
    "    for line in f:\n",
    "        toks = line.split(\" \")\n",
    "        word = toks[0]\n",
    "        embeddings.append(np.array([float(x) for x in toks[1:]]))\n",
    "        words2ids[word] = i+2 # +3 - przesuniecie po to zeby specjalne embeddingi byly na pozycji 0 i 1\n",
    "        i = i + 1\n",
    "\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mają': 2,\n",
       " 'cytrusów': 3,\n",
       " 'ponadto': 4,\n",
       " 'pięknej': 5,\n",
       " 'pudrowego': 6,\n",
       " 'produkt': 7,\n",
       " 'brodzików': 8,\n",
       " 'U': 9,\n",
       " 'podziwu': 10,\n",
       " 'słowem': 11,\n",
       " 'diabłem': 12,\n",
       " 'malarze': 13,\n",
       " 'mój': 14,\n",
       " 'przestać': 15,\n",
       " 'whiskey': 16,\n",
       " 'żadnej': 17,\n",
       " 'Swój': 18,\n",
       " 'pieprzową': 19,\n",
       " 'figury': 20,\n",
       " 'kupować': 21,\n",
       " 'pozostałych': 22,\n",
       " 'komplety': 23,\n",
       " 'zamachu': 24,\n",
       " 'dalekosiężne': 25,\n",
       " 'leżą': 26,\n",
       " 'aprobatą': 27,\n",
       " 'bujnym': 28,\n",
       " 'podobieństwa': 29,\n",
       " 'sukienki': 30,\n",
       " 'uciemiężone': 31,\n",
       " 'koszulę': 32,\n",
       " 'koszuli': 33,\n",
       " 'koszule': 34,\n",
       " 'dłużej': 35,\n",
       " 'wypuści': 36,\n",
       " 'tworząc': 37,\n",
       " 'rekompensuje': 38,\n",
       " 'przecudowny': 39,\n",
       " 'wyróżniających': 40,\n",
       " 'należytemu': 41,\n",
       " 'konkretnych': 42,\n",
       " 'święto': 43,\n",
       " 'patologiem': 44,\n",
       " 'przeciętnej': 45,\n",
       " 'Donatellę': 46,\n",
       " 'powstańcza': 47,\n",
       " 'testu': 48,\n",
       " 'mieszkania': 49,\n",
       " 'tę': 50,\n",
       " 'adekwatna': 51,\n",
       " 'oczekiwaniami': 52,\n",
       " 'natomiast': 53,\n",
       " 'polecenia': 54,\n",
       " 'nasz': 55,\n",
       " 'Można': 56,\n",
       " 'Uwielbiam': 57,\n",
       " 'sprawców': 58,\n",
       " 'Używała': 59,\n",
       " 'wyjątkowy': 60,\n",
       " 'kolorze': 61,\n",
       " 'lekarzy': 62,\n",
       " 'lekarze': 63,\n",
       " 'ogólnie': 64,\n",
       " 'wody': 65,\n",
       " 'wywołuje': 66,\n",
       " 'wywołują': 67,\n",
       " 'zielony': 68,\n",
       " 'zielono': 69,\n",
       " 'zielone': 70,\n",
       " 'podróbki': 71,\n",
       " 'natarczywą': 72,\n",
       " 'Niska': 73,\n",
       " 'wychodzą': 74,\n",
       " 'wtedy': 75,\n",
       " 'wyróżnia': 76,\n",
       " 'patrzyły': 77,\n",
       " 'opcja': 78,\n",
       " 'gustownie': 79,\n",
       " 'boski': 80,\n",
       " 'Ducha': 81,\n",
       " 'orientalnych': 82,\n",
       " 'wartość': 83,\n",
       " 'najgorszy': 84,\n",
       " 'cechy': 85,\n",
       " 'zmienili': 86,\n",
       " 'Niepowtarzalny': 87,\n",
       " 'własny': 88,\n",
       " 'amerykańskiej': 89,\n",
       " 'oblicza': 90,\n",
       " 'oblicze': 91,\n",
       " 'stylu': 92,\n",
       " 'bawełniane': 93,\n",
       " 'problemy': 94,\n",
       " 'problemu': 95,\n",
       " 'Przez': 96,\n",
       " 'Przed': 97,\n",
       " 'zainteresuje': 98,\n",
       " 'biorą': 99,\n",
       " 'przeznaczona': 100,\n",
       " 'czuła': 101,\n",
       " 'przestępców': 102,\n",
       " 'aktywność': 103,\n",
       " 'rewolucyjnym': 104,\n",
       " 'towarzyszyły': 105,\n",
       " 'ubraniem': 106,\n",
       " 'wypróbować': 107,\n",
       " 'lubiła': 108,\n",
       " 'kiedy': 109,\n",
       " 'uważnie': 110,\n",
       " 'Zróżnicowana': 111,\n",
       " 'upływie': 112,\n",
       " 'skąpo': 113,\n",
       " 'Miło': 114,\n",
       " 'pomimo': 115,\n",
       " 'Nie': 116,\n",
       " 'powiadomić': 117,\n",
       " 'flakonie': 118,\n",
       " 'flakonik': 119,\n",
       " 'najczulszym': 120,\n",
       " 'lustrował': 121,\n",
       " 'kosztują': 122,\n",
       " 'Gilada': 123,\n",
       " 'trawą': 124,\n",
       " 'choinkę': 125,\n",
       " 'namiętny': 126,\n",
       " 'kojarzyła': 127,\n",
       " 'swojego': 128,\n",
       " 'dobrze': 129,\n",
       " 'działa': 130,\n",
       " 'zauważył': 131,\n",
       " 'powagi': 132,\n",
       " 'Minusem': 133,\n",
       " 'cytoplazmatyczne': 134,\n",
       " 'podobnym': 135,\n",
       " 'straty': 136,\n",
       " 'jedynie': 137,\n",
       " 'pozbawioną': 138,\n",
       " 'pozbawiona': 139,\n",
       " 'przestępstwa': 140,\n",
       " 'naczyń': 141,\n",
       " 'adidas': 142,\n",
       " 'córki': 143,\n",
       " 'mistrz': 144,\n",
       " 'zimny': 145,\n",
       " 'ołtarza': 146,\n",
       " 'fajnym': 147,\n",
       " 'znajdujące': 148,\n",
       " 'Jest': 149,\n",
       " 'lider': 150,\n",
       " 'wyjście': 151,\n",
       " 'wyjścia': 152,\n",
       " 'słodkich': 153,\n",
       " 'Przechodzień': 154,\n",
       " 'sprawiają': 155,\n",
       " 'przecież': 156,\n",
       " 'Rocznie': 157,\n",
       " 'odbierają': 158,\n",
       " 'Kolejną': 159,\n",
       " 'Kolejna': 160,\n",
       " 'Kolejny': 161,\n",
       " 'oszpecić': 162,\n",
       " 'wandali': 163,\n",
       " 'ginie': 164,\n",
       " 'podtrzyma': 165,\n",
       " 'uznanie': 166,\n",
       " 'odświeżający': 167,\n",
       " 'odświeżająca': 168,\n",
       " 'podrabianych': 169,\n",
       " 'mdłości': 170,\n",
       " 'W': 171,\n",
       " 'dziwne': 172,\n",
       " 'dziwny': 173,\n",
       " 'sejsmicznej': 174,\n",
       " 'materiału': 175,\n",
       " 'Zachowała': 176,\n",
       " 'odpowiadać': 177,\n",
       " 'odpowiadał': 178,\n",
       " 'nakrętki': 179,\n",
       " 'cielisty': 180,\n",
       " 'Davidoff': 181,\n",
       " 'tanich': 182,\n",
       " 'możliwość': 183,\n",
       " 'markowych': 184,\n",
       " 'zmniejszając': 185,\n",
       " 'Magdę': 186,\n",
       " 'trwa': 187,\n",
       " 'wąskim': 188,\n",
       " 'ludzkości': 189,\n",
       " 'Plus': 190,\n",
       " 'słodkiego': 191,\n",
       " 'zastrzeżeń': 192,\n",
       " 'deski': 193,\n",
       " 'wiruje': 194,\n",
       " 'zbrodniarz': 195,\n",
       " 'złota': 196,\n",
       " 'równocześnie': 197,\n",
       " 'niezapomniane': 198,\n",
       " 'środowisk': 199,\n",
       " 'szanować': 200,\n",
       " 'męskość': 201,\n",
       " 'kwiatowego': 202,\n",
       " 'kadzidła': 203,\n",
       " 'zeznania': 204,\n",
       " 'rocznicy': 205,\n",
       " 'gwarem': 206,\n",
       " 'jedwabisty': 207,\n",
       " 'lubili': 208,\n",
       " 'jakimi': 209,\n",
       " 'urządzenie': 210,\n",
       " 'elektryzującym': 211,\n",
       " 'najbliższej': 212,\n",
       " 'dalej': 213,\n",
       " 'kwiatowa': 214,\n",
       " 'kwiatowo': 215,\n",
       " 'kwiatowy': 216,\n",
       " 'kwiatową': 217,\n",
       " 'Materiał': 218,\n",
       " 'napaść': 219,\n",
       " 'radością': 220,\n",
       " 'starannie': 221,\n",
       " 't': 222,\n",
       " 'charakterem': 223,\n",
       " 'charakterek': 224,\n",
       " 'chronionej': 225,\n",
       " 'historycznej': 226,\n",
       " 'spędzenie': 227,\n",
       " 'zobaczysz': 228,\n",
       " 'młodego': 229,\n",
       " 'najwyższej': 230,\n",
       " 'szklanek': 231,\n",
       " 'nietuzinkowy': 232,\n",
       " 'powiatem': 233,\n",
       " 'pomoc': 234,\n",
       " 'usiąść': 235,\n",
       " 'pomysł': 236,\n",
       " 'stringów': 237,\n",
       " 'Mężczyźni': 238,\n",
       " 'oczekuje': 239,\n",
       " 'Rush': 240,\n",
       " 'etapie': 241,\n",
       " 'korzystnie': 242,\n",
       " 'kandydaci': 243,\n",
       " 'oprawionych': 244,\n",
       " 'używa': 245,\n",
       " 'Stefano': 246,\n",
       " 'miniaturkę': 247,\n",
       " 'kolorystyka': 248,\n",
       " 'gwałtowne': 249,\n",
       " 'Czekaj': 250,\n",
       " 'miesięcy': 251,\n",
       " 'słowa': 252,\n",
       " 'chcą': 253,\n",
       " 'Zwiefkę': 254,\n",
       " 'pozwoliła': 255,\n",
       " 'pazur': 256,\n",
       " 'samonośny': 257,\n",
       " 'Kolorki': 258,\n",
       " 'mycia': 259,\n",
       " 'odnoszę': 260,\n",
       " 'Twój': 261,\n",
       " 'przeciętnego': 262,\n",
       " 'typowej': 263,\n",
       " 'użyciu': 264,\n",
       " 'użycie': 265,\n",
       " 'zanim': 266,\n",
       " 'dotkniętego': 267,\n",
       " 'objęciach': 268,\n",
       " 'przedstawicielach': 269,\n",
       " 'obiekt': 270,\n",
       " 'pewniej': 271,\n",
       " 'zmianą': 272,\n",
       " 'wieczorem': 273,\n",
       " 'łączą': 274,\n",
       " 'swoim': 275,\n",
       " 'spasuje': 276,\n",
       " 'prawdziwa': 277,\n",
       " 'prawdziwy': 278,\n",
       " 'fajnych': 279,\n",
       " 'nieziemski': 280,\n",
       " 'nietrzeźwość': 281,\n",
       " 'dróg': 282,\n",
       " 'kształt': 283,\n",
       " 'kolorową': 284,\n",
       " 'wiele': 285,\n",
       " 'wielu': 286,\n",
       " 'Oświadczył': 287,\n",
       " 'nietrwałe': 288,\n",
       " 'sztuczny': 289,\n",
       " 'Komendy': 290,\n",
       " 'młodej': 291,\n",
       " 'poeta': 292,\n",
       " 'malutka': 293,\n",
       " 'mężem': 294,\n",
       " 'zareklamowany': 295,\n",
       " 'sukienkach': 296,\n",
       " 'zainstalować': 297,\n",
       " 'haftem': 298,\n",
       " 'On': 299,\n",
       " 'Od': 300,\n",
       " 'OK': 301,\n",
       " 'utworzenia': 302,\n",
       " 'uzupełnienie': 303,\n",
       " 'zmysłowość': 304,\n",
       " 'nieprzyzwoitość': 305,\n",
       " 'warunków': 306,\n",
       " 'wiadomo': 307,\n",
       " 'wyrafinowany': 308,\n",
       " 'tymi': 309,\n",
       " 'biustowi': 310,\n",
       " 'snu': 311,\n",
       " 'zdarzyło': 312,\n",
       " 'zdarzyła': 313,\n",
       " 'porządnie': 314,\n",
       " '30ml': 315,\n",
       " 'Panowie': 316,\n",
       " 'próbki': 317,\n",
       " 'Sama': 318,\n",
       " 'Elegancki': 319,\n",
       " 'mdli': 320,\n",
       " 'konserwatywnie': 321,\n",
       " 'sposób': 322,\n",
       " 'polską': 323,\n",
       " 'polska': 324,\n",
       " 'taką': 325,\n",
       " 'taka': 326,\n",
       " 'taki': 327,\n",
       " 'pana': 328,\n",
       " 'pani': 329,\n",
       " 'położeniu': 330,\n",
       " 'utrzymania': 331,\n",
       " 'szli': 332,\n",
       " 'Irga': 333,\n",
       " 'wymienionych': 334,\n",
       " 'wyostrza': 335,\n",
       " 'przykład': 336,\n",
       " 'gęba': 337,\n",
       " 'zakładania': 338,\n",
       " 'brytyjscy': 339,\n",
       " 'lepiej': 340,\n",
       " 'wczesny': 341,\n",
       " 'owocnych': 342,\n",
       " 'pereł': 343,\n",
       " 'wyciszający': 344,\n",
       " 'bałaganu': 345,\n",
       " 'skórzanych': 346,\n",
       " 'chętny': 347,\n",
       " 'pozytywnie': 348,\n",
       " 'zastaw': 349,\n",
       " 'tygodniem': 350,\n",
       " 'zamykaniem': 351,\n",
       " 'podkreśla': 352,\n",
       " 'zdecydowanych': 353,\n",
       " 'podczas': 354,\n",
       " 'ok': 355,\n",
       " 'of': 356,\n",
       " 'od': 357,\n",
       " 'wspaniałej': 358,\n",
       " 'poprzez': 359,\n",
       " 'szczególne': 360,\n",
       " 'piersi': 361,\n",
       " 'Później': 362,\n",
       " 'wojnie': 363,\n",
       " 'posiadam': 364,\n",
       " 'import': 365,\n",
       " 'tego': 366,\n",
       " 'zamówiony': 367,\n",
       " 'pasowała': 368,\n",
       " 'zgodnym': 369,\n",
       " 'prezentuje': 370,\n",
       " 'przyjęła': 371,\n",
       " 'Christian': 372,\n",
       " 'zmysłowego': 373,\n",
       " 'stołecznych': 374,\n",
       " 'twarde': 375,\n",
       " 'muzyce': 376,\n",
       " 'kolacji': 377,\n",
       " 'chociaż': 378,\n",
       " 'szkoda': 379,\n",
       " 'antyutleniacze': 380,\n",
       " 'Ratko': 381,\n",
       " 'bladymi': 382,\n",
       " 'testy': 383,\n",
       " 'Coco': 384,\n",
       " 'szans': 385,\n",
       " 'miseczek': 386,\n",
       " 'kwiatów': 387,\n",
       " 'zapachem': 388,\n",
       " 'energicznej': 389,\n",
       " 'ofercie': 390,\n",
       " 'Niedzieli': 391,\n",
       " 'dizajnie': 392,\n",
       " 'optycznie': 393,\n",
       " 'nagrodach': 394,\n",
       " 'prysznica': 395,\n",
       " 'atrakcyjność': 396,\n",
       " 'Czwartku': 397,\n",
       " 'Generalnie': 398,\n",
       " 'najpiękniejszych': 399,\n",
       " 'duszę': 400,\n",
       " 'duszy': 401,\n",
       " 'zamieniony': 402,\n",
       " 'przereklamowany': 403,\n",
       " 'Przy': 404,\n",
       " 'czarna': 405,\n",
       " 'czarny': 406,\n",
       " 'drogeryjnych': 407,\n",
       " 'czarną': 408,\n",
       " 'Perfumy': 409,\n",
       " 'podnosi': 410,\n",
       " 'inaczej': 411,\n",
       " 'rynku': 412,\n",
       " 'Powązkach': 413,\n",
       " 'tłumu': 414,\n",
       " 'białek': 415,\n",
       " 'Jędrzejowskiego': 416,\n",
       " 'zawsze': 417,\n",
       " 'owocowe': 418,\n",
       " 'lekkie': 419,\n",
       " 'lekkim': 420,\n",
       " 'Frontery': 421,\n",
       " 'takiego': 422,\n",
       " 'porwana': 423,\n",
       " 'Odrobina': 424,\n",
       " 'kontrowersyjnych': 425,\n",
       " 'rozwijały': 426,\n",
       " 'pozorom': 427,\n",
       " 'pomidora': 428,\n",
       " 'Fascynująca': 429,\n",
       " 'uwiera': 430,\n",
       " 'gładkich': 431,\n",
       " 'POLECAM': 432,\n",
       " 'żywić': 433,\n",
       " 'użytkowniczka': 434,\n",
       " 'Idealnie': 435,\n",
       " 'protestów': 436,\n",
       " 'rokowała': 437,\n",
       " 'wyższe': 438,\n",
       " 'łąką': 439,\n",
       " 'alternatywa': 440,\n",
       " 'Całkiem': 441,\n",
       " 'porażką': 442,\n",
       " 'Dzień': 443,\n",
       " 'klientem': 444,\n",
       " 'zabrał': 445,\n",
       " 'Hoop': 446,\n",
       " 'chłopakowi': 447,\n",
       " 'przepiękny': 448,\n",
       " 'światowym': 449,\n",
       " 'eksmitowanych': 450,\n",
       " 'Cenowo': 451,\n",
       " 'zupy': 452,\n",
       " 'Dobrze': 453,\n",
       " 'horrendalna': 454,\n",
       " 'konkurencji': 455,\n",
       " 'głosowania': 456,\n",
       " 'praktyczny': 457,\n",
       " 'praktyczne': 458,\n",
       " 'Większość': 459,\n",
       " 'Napewno': 460,\n",
       " 'rozumieć': 461,\n",
       " 'zdolny': 462,\n",
       " 'wpływem': 463,\n",
       " 'Perfum': 464,\n",
       " 'torebce': 465,\n",
       " 'młodzieżą': 466,\n",
       " 'żądania': 467,\n",
       " 'kontakt': 468,\n",
       " 'głębokich': 469,\n",
       " 'żałuję': 470,\n",
       " 'wypadek': 471,\n",
       " 'Zajmuje': 472,\n",
       " 'pochód': 473,\n",
       " 'zupełnie': 474,\n",
       " 'ładnym': 475,\n",
       " 'drzewny': 476,\n",
       " 'drzewno': 477,\n",
       " 'drzewną': 478,\n",
       " 'znana': 479,\n",
       " 'znane': 480,\n",
       " 'znany': 481,\n",
       " 'nieco': 482,\n",
       " 'porzeczkę': 483,\n",
       " 'genialny': 484,\n",
       " 'rywalizowało': 485,\n",
       " 'ukochanym': 486,\n",
       " 'najbliższy': 487,\n",
       " 'płaskich': 488,\n",
       " 'dojrzałego': 489,\n",
       " 'nieudanych': 490,\n",
       " 'model': 491,\n",
       " 'nakazom': 492,\n",
       " 'rycinie': 493,\n",
       " 'używam': 494,\n",
       " 'używał': 495,\n",
       " 'używać': 496,\n",
       " 'Focusa': 497,\n",
       " 'uroczystego': 498,\n",
       " 'cudownego': 499,\n",
       " 'końcowym': 500,\n",
       " 'wzrok': 501,\n",
       " 'tata': 502,\n",
       " 'przestrzennej': 503,\n",
       " 'trzyma': 504,\n",
       " 'rodziców': 505,\n",
       " 'pyszniła': 506,\n",
       " 'parkiecie': 507,\n",
       " 'żadna': 508,\n",
       " 'żadną': 509,\n",
       " 'domów': 510,\n",
       " 'Pour': 511,\n",
       " 'klasą': 512,\n",
       " 'klasę': 513,\n",
       " 'włożę': 514,\n",
       " 'klasy': 515,\n",
       " 'urzekł': 516,\n",
       " 'urzeka': 517,\n",
       " 'wyglądu': 518,\n",
       " 'wygląda': 519,\n",
       " 'miłośników': 520,\n",
       " 'Polskim': 521,\n",
       " 'pół': 522,\n",
       " 'nieprzewidywalności': 523,\n",
       " 'gubią': 524,\n",
       " 'czegoś': 525,\n",
       " 'uroku': 526,\n",
       " 'prysznic': 527,\n",
       " 'Rosję': 528,\n",
       " 'ramiona': 529,\n",
       " 'sympatykom': 530,\n",
       " 'jakość': 531,\n",
       " 'wypowiedzieli': 532,\n",
       " 'efektywną': 533,\n",
       " 'całymi': 534,\n",
       " 'utrzymywał': 535,\n",
       " 'pożarniczy': 536,\n",
       " 'dyskotekę': 537,\n",
       " 'Wkoło': 538,\n",
       " 'przeminie': 539,\n",
       " 'Cyganeria': 540,\n",
       " 'także': 541,\n",
       " 'Polki': 542,\n",
       " 'Noszę': 543,\n",
       " 'Uniwersytetu': 544,\n",
       " 'Było': 545,\n",
       " 'specyficzny': 546,\n",
       " 'odczuwasz': 547,\n",
       " 'męża': 548,\n",
       " 'egoistycznych': 549,\n",
       " 'zmienił': 550,\n",
       " 'zmienić': 551,\n",
       " 'zmienia': 552,\n",
       " 'mdława': 553,\n",
       " 'czystym': 554,\n",
       " 'efektu': 555,\n",
       " 'imię': 556,\n",
       " 'Kolor': 557,\n",
       " 'najbliższym': 558,\n",
       " 'duchu': 559,\n",
       " 'bunt': 560,\n",
       " 'republiki': 561,\n",
       " 'spotykany': 562,\n",
       " 'kwiatowym': 563,\n",
       " 'zmienione': 564,\n",
       " 'zostawiając': 565,\n",
       " 'Ma': 566,\n",
       " 'Mi': 567,\n",
       " 'strefie': 568,\n",
       " 'uwodzić': 569,\n",
       " 'dystansem': 570,\n",
       " 'odstępy': 571,\n",
       " 'niczym': 572,\n",
       " 'drobnej': 573,\n",
       " 'nutę': 574,\n",
       " 'nuty': 575,\n",
       " 'nuta': 576,\n",
       " 'mediacjach': 577,\n",
       " 'końca': 578,\n",
       " 'koszulce': 579,\n",
       " 'religijni': 580,\n",
       " 'stronę': 581,\n",
       " 'realizmu': 582,\n",
       " 'zagranicznego': 583,\n",
       " 'wykonania': 584,\n",
       " 'przywiązanej': 585,\n",
       " 'zakup': 586,\n",
       " 'ktorą': 587,\n",
       " 'wrażliwą': 588,\n",
       " 'stanie': 589,\n",
       " 'stanik': 590,\n",
       " 'odstrasza': 591,\n",
       " 'Kompletnie': 592,\n",
       " 'przygody': 593,\n",
       " 'Flakonik': 594,\n",
       " 'odparzeń': 595,\n",
       " 'wieczory': 596,\n",
       " 'zbrodnie': 597,\n",
       " 'koloniami': 598,\n",
       " 'intryguje': 599,\n",
       " 'jeszcze': 600,\n",
       " 'obracająca': 601,\n",
       " 'Lauren': 602,\n",
       " 'kłują': 603,\n",
       " 'tytuł': 604,\n",
       " 'wykazuje': 605,\n",
       " 'rozwaliło': 606,\n",
       " 'rozwaliły': 607,\n",
       " 'czasami': 608,\n",
       " 'tknąć': 609,\n",
       " 'jakie': 610,\n",
       " 'pozostawienie': 611,\n",
       " 'obiedzie': 612,\n",
       " 'wzorów': 613,\n",
       " 'szybka': 614,\n",
       " 'szybki': 615,\n",
       " 'szybko': 616,\n",
       " 'dochodzi': 617,\n",
       " 'uzależnił': 618,\n",
       " 'utrzymują': 619,\n",
       " 'przebojowych': 620,\n",
       " 'utrzymuje': 621,\n",
       " 'kark': 622,\n",
       " 'Bogu': 623,\n",
       " 'spokojem': 624,\n",
       " 'kusząco': 625,\n",
       " 'kuszące': 626,\n",
       " 'kuszący': 627,\n",
       " 'Klein': 628,\n",
       " 'samym': 629,\n",
       " 'ulubionych': 630,\n",
       " 'stoi': 631,\n",
       " 'Wyraźnie': 632,\n",
       " 'wiedzą': 633,\n",
       " 'ciepło': 634,\n",
       " 'Buzka': 635,\n",
       " 'absolutnej': 636,\n",
       " 'trwałości': 637,\n",
       " 'ma': 638,\n",
       " 'mi': 639,\n",
       " 'mu': 640,\n",
       " 'zapada': 641,\n",
       " 'seksapil': 642,\n",
       " 'radośnie': 643,\n",
       " 'pięknie': 644,\n",
       " 'utrzyma': 645,\n",
       " 'interesów': 646,\n",
       " 'jakiej': 647,\n",
       " 'brakuje': 648,\n",
       " 'jakieś': 649,\n",
       " 'śliwkę': 650,\n",
       " 'wyczuwalny': 651,\n",
       " 'podkreślone': 652,\n",
       " 'przeznaczony': 653,\n",
       " 'biustonoszach': 654,\n",
       " 'liberalny': 655,\n",
       " 'doniesienia': 656,\n",
       " 'doskonale': 657,\n",
       " 'depresji': 658,\n",
       " 'Niby': 659,\n",
       " 'mnóstwem': 660,\n",
       " 'przechodzi': 661,\n",
       " 'sytuacji': 662,\n",
       " 'wykrochmaloną': 663,\n",
       " 'gust': 664,\n",
       " 'tłumie': 665,\n",
       " 'jednorazówki': 666,\n",
       " 'radość': 667,\n",
       " 'wszytsko': 668,\n",
       " 'Słodkiego': 669,\n",
       " 'uśmiechać': 670,\n",
       " 'wiedział': 671,\n",
       " 'zależy': 672,\n",
       " 'owocujący': 673,\n",
       " 'upiekli': 674,\n",
       " 'odpowiedni': 675,\n",
       " 'Odważna': 676,\n",
       " 'zgięcie': 677,\n",
       " 'radosnym': 678,\n",
       " 'kolorystyce': 679,\n",
       " 'drugiego': 680,\n",
       " 'trzecia': 681,\n",
       " 'nutami': 682,\n",
       " 'rodzinnego': 683,\n",
       " 'Przedsiębiorcę': 684,\n",
       " 'Givenchy': 685,\n",
       " 'mankament': 686,\n",
       " 'kategorii': 687,\n",
       " 'Skierowany': 688,\n",
       " 'upolowania': 689,\n",
       " 'niego': 690,\n",
       " 'Posiada': 691,\n",
       " 'Chociaż': 692,\n",
       " 'liczne': 693,\n",
       " 'liczni': 694,\n",
       " 'ludzie': 695,\n",
       " 'przezroczystą': 696,\n",
       " 'uwielbia': 697,\n",
       " 'podziękowania': 698,\n",
       " 'włoski': 699,\n",
       " 'Ładniejszy': 700,\n",
       " 'Pań': 701,\n",
       " 'Pan': 702,\n",
       " 'zajmuje': 703,\n",
       " 'napadem': 704,\n",
       " 'zaskakujący': 705,\n",
       " 'z': 706,\n",
       " 'niska': 707,\n",
       " 'ściśnięcia': 708,\n",
       " 'zaczęły': 709,\n",
       " 'Mühlhausowi': 710,\n",
       " 'spełnił': 711,\n",
       " 'policja': 712,\n",
       " 'policji': 713,\n",
       " 'Marka': 714,\n",
       " 'cudna': 715,\n",
       " 'doświadczają': 716,\n",
       " 'starym': 717,\n",
       " 'groźne': 718,\n",
       " 'kotom': 719,\n",
       " 'wyglądać': 720,\n",
       " 'Dune': 721,\n",
       " 'wystąpienie': 722,\n",
       " 'fasonów': 723,\n",
       " 'kłodzkiego': 724,\n",
       " 'skromne': 725,\n",
       " 'skromny': 726,\n",
       " 'nietuzinkowym': 727,\n",
       " 'Muszę': 728,\n",
       " 'zakupów': 729,\n",
       " 'rodzinę': 730,\n",
       " 'rodziną': 731,\n",
       " 'rodzina': 732,\n",
       " 'Wizytek': 733,\n",
       " 'mililitrów': 734,\n",
       " 'moja': 735,\n",
       " 'romantyczki': 736,\n",
       " 'szczególnego': 737,\n",
       " 'oddała': 738,\n",
       " 'ekspertów': 739,\n",
       " 'istniała': 740,\n",
       " 'okazyjnej': 741,\n",
       " 'rannych': 742,\n",
       " 'sąsiedzi': 743,\n",
       " 'najdroższe': 744,\n",
       " 'Kujawy': 745,\n",
       " 'perfumach': 746,\n",
       " 'takim': 747,\n",
       " 'stosunku': 748,\n",
       " 'piłkarskiej': 749,\n",
       " 'powojennej': 750,\n",
       " 'bycia': 751,\n",
       " 'zapewnia': 752,\n",
       " 'pikanterii': 753,\n",
       " 'wpisała': 754,\n",
       " 'wydajne': 755,\n",
       " 'wspomnienia': 756,\n",
       " 'toczą': 757,\n",
       " 'ścisnąć': 758,\n",
       " 'Szczególnie': 759,\n",
       " 'dziękuję': 760,\n",
       " 'kogoś': 761,\n",
       " 'koleżankę': 762,\n",
       " 'duże': 763,\n",
       " 'koleżanki': 764,\n",
       " 'zwiększając': 765,\n",
       " 'kreacje': 766,\n",
       " 'poranku': 767,\n",
       " 'odszkodowania': 768,\n",
       " 'poszukuję': 769,\n",
       " 'staniczek': 770,\n",
       " 'godnym': 771,\n",
       " 'tradycyjnym': 772,\n",
       " 'całemu': 773,\n",
       " 'mocna': 774,\n",
       " 'znanej': 775,\n",
       " 'zasłania': 776,\n",
       " 'odnieść': 777,\n",
       " 'Ksiądz': 778,\n",
       " 'powietrzu': 779,\n",
       " 'powietrza': 780,\n",
       " 'powietrze': 781,\n",
       " 'amarantowa': 782,\n",
       " 'Tymczasem': 783,\n",
       " 'całą': 784,\n",
       " 'leży': 785,\n",
       " 'cały': 786,\n",
       " 'Cmentarza': 787,\n",
       " 'gatunkowo': 788,\n",
       " 'spytało': 789,\n",
       " 'buja': 790,\n",
       " 'Śliczny': 791,\n",
       " 'Śliczna': 792,\n",
       " 'Biernacki': 793,\n",
       " 'energicznych': 794,\n",
       " 'własną': 795,\n",
       " 'złożył': 796,\n",
       " 'żony': 797,\n",
       " 'żona': 798,\n",
       " 'partii': 799,\n",
       " 'ani': 800,\n",
       " 'wibrującą': 801,\n",
       " 'reklama': 802,\n",
       " 'macochy': 803,\n",
       " 'flaszka': 804,\n",
       " 'beznadziejny': 805,\n",
       " 'niszcząc': 806,\n",
       " 'modeluje': 807,\n",
       " 'modelują': 808,\n",
       " 'lekką': 809,\n",
       " 'własnych': 810,\n",
       " 'niespodzianek': 811,\n",
       " 'Zachowują': 812,\n",
       " 'lekko': 813,\n",
       " 'lekki': 814,\n",
       " 'Zakupiła': 815,\n",
       " 'tracąc': 816,\n",
       " 'Prokuratury': 817,\n",
       " 'trucizna': 818,\n",
       " 'pełne': 819,\n",
       " 'odpowiada': 820,\n",
       " 'ułożenia': 821,\n",
       " 'osobiście': 822,\n",
       " 'pasku': 823,\n",
       " 'sporo': 824,\n",
       " 'spora': 825,\n",
       " 'spory': 826,\n",
       " 'sporą': 827,\n",
       " 'Beatki': 828,\n",
       " 'nadgarstkowe': 829,\n",
       " 'Oczywiście': 830,\n",
       " 'podążyła': 831,\n",
       " 'Armani': 832,\n",
       " 'sytuacjami': 833,\n",
       " 'niewielki': 834,\n",
       " 'bezpieczeństwa': 835,\n",
       " 'przy': 836,\n",
       " 'codzień': 837,\n",
       " 'codzien': 838,\n",
       " 'ocenili': 839,\n",
       " 'części': 840,\n",
       " 'stołka': 841,\n",
       " 'wspaniałego': 842,\n",
       " 'esencjonalny': 843,\n",
       " 'zginie': 844,\n",
       " 'rozkrzewiona': 845,\n",
       " 'dekolcie': 846,\n",
       " 'zakrętka': 847,\n",
       " 'zdecydowanie': 848,\n",
       " 'For': 849,\n",
       " 'waniliowy': 850,\n",
       " 'śmy': 851,\n",
       " 'nigdy': 852,\n",
       " 'własnego': 853,\n",
       " 'domu': 854,\n",
       " 'uwielbiają': 855,\n",
       " 'pieniądze': 856,\n",
       " 'postkomuch': 857,\n",
       " 'żydowskich': 858,\n",
       " 'wypróbowała': 859,\n",
       " 'Cieszą': 860,\n",
       " 'Stan': 861,\n",
       " 'Kolorystyka': 862,\n",
       " 'nowymi': 863,\n",
       " 'wiązanie': 864,\n",
       " 'wizerunkiem': 865,\n",
       " 'użytkowania': 866,\n",
       " 'Komor': 867,\n",
       " 'berecie': 868,\n",
       " 'zapachową': 869,\n",
       " 'zapachowe': 870,\n",
       " 'zapachowa': 871,\n",
       " 'zaprojektowane': 872,\n",
       " 'sylwetkę': 873,\n",
       " 'sylwetki': 874,\n",
       " 'zawiedzie': 875,\n",
       " 'wicestarosta': 876,\n",
       " 'kształcie': 877,\n",
       " 'urzędzie': 878,\n",
       " 'letnie': 879,\n",
       " 'letnia': 880,\n",
       " 'znaleźć': 881,\n",
       " 'rząd': 882,\n",
       " 'jesienne': 883,\n",
       " 'jesienną': 884,\n",
       " 'róży': 885,\n",
       " 'różu': 886,\n",
       " 'Jesteśmy': 887,\n",
       " 'bergamotki': 888,\n",
       " 'nocną': 889,\n",
       " 'piersiami': 890,\n",
       " 'ludzkiej': 891,\n",
       " 'rozciąga': 892,\n",
       " 'nieciekawy': 893,\n",
       " 'końcu': 894,\n",
       " 'arbitra': 895,\n",
       " 'optymizmu': 896,\n",
       " 'Całokształt': 897,\n",
       " 'udało': 898,\n",
       " 'Potem': 899,\n",
       " 'wakacje': 900,\n",
       " 'chorobami': 901,\n",
       " 'Lwowskich': 902,\n",
       " 'ciężki': 903,\n",
       " 'ustach': 904,\n",
       " 'przed': 905,\n",
       " 'mokrym': 906,\n",
       " 'dziedzinach': 907,\n",
       " 'diametralnie': 908,\n",
       " 'witali': 909,\n",
       " 'przecudne': 910,\n",
       " 'przecudny': 911,\n",
       " 'nadawania': 912,\n",
       " 'Znalezisko': 913,\n",
       " 'długi': 914,\n",
       " 'długo': 915,\n",
       " 'krój': 916,\n",
       " 'zostawiam': 917,\n",
       " 'kokarda': 918,\n",
       " 'Spodziewał': 919,\n",
       " 'Majteczki': 920,\n",
       " 'słodszych': 921,\n",
       " 'jesiennym': 922,\n",
       " 'wyniesione': 923,\n",
       " 'wyszczuplającego': 924,\n",
       " 'używając': 925,\n",
       " 'euforia': 926,\n",
       " 'euforię': 927,\n",
       " 'silniejsze': 928,\n",
       " 'cytrusami': 929,\n",
       " 'wanien': 930,\n",
       " 'silnej': 931,\n",
       " 'Mimo': 932,\n",
       " 'większe': 933,\n",
       " 'większą': 934,\n",
       " 'perfum': 935,\n",
       " 'pralce': 936,\n",
       " 'Założony': 937,\n",
       " 'torebki': 938,\n",
       " 'dotyk': 939,\n",
       " 'czymś': 940,\n",
       " 'dużym': 941,\n",
       " 'związkach': 942,\n",
       " 'prezentacji': 943,\n",
       " 'brutalni': 944,\n",
       " 'ręcznie': 945,\n",
       " 'pełnowymiarowego': 946,\n",
       " 'Obecnie': 947,\n",
       " 'wprawia': 948,\n",
       " 'Kupuję': 949,\n",
       " 'Pi': 950,\n",
       " 'Po': 951,\n",
       " 'ludobójstwo': 952,\n",
       " 'obserwatorzy': 953,\n",
       " 'Stanik': 954,\n",
       " 'rozwija': 955,\n",
       " 'chciała': 956,\n",
       " 'izolacji': 957,\n",
       " 'rywalem': 958,\n",
       " 'pojechać': 959,\n",
       " 'ostatecznego': 960,\n",
       " 'niezbyt': 961,\n",
       " 'zna': 962,\n",
       " 'Przepiękne': 963,\n",
       " 'Przepiękny': 964,\n",
       " 'niezwykle': 965,\n",
       " 'ostrą': 966,\n",
       " 'Kusi': 967,\n",
       " 'ostre': 968,\n",
       " 'ostry': 969,\n",
       " 'ks': 970,\n",
       " 'ku': 971,\n",
       " 'Kiedy': 972,\n",
       " 'przekombinowany': 973,\n",
       " 'dekoracyjny': 974,\n",
       " 'podkreślają': 975,\n",
       " 'D': 976,\n",
       " 'proszą': 977,\n",
       " 'gdzie': 978,\n",
       " 'podobał': 979,\n",
       " 'podobać': 980,\n",
       " 'zakomunikowali': 981,\n",
       " 'niespecjalnie': 982,\n",
       " 'fanów': 983,\n",
       " 'światowa': 984,\n",
       " 'widnieją': 985,\n",
       " 'postanowiła': 986,\n",
       " 'owa': 987,\n",
       " 'kolorów': 988,\n",
       " 'otrzymała': 989,\n",
       " 'nagminnie': 990,\n",
       " 'zalety': 991,\n",
       " 'Młodych': 992,\n",
       " 'wyglądem': 993,\n",
       " 'majtek': 994,\n",
       " 'przesłodzony': 995,\n",
       " 'Wracam': 996,\n",
       " 'aromaty': 997,\n",
       " 'czesto': 998,\n",
       " 'wyprofilowana': 999,\n",
       " 'szczyt': 1000,\n",
       " 'tytoniu': 1001,\n",
       " ...}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words2ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.68923  , -0.728562 , -0.67156  ,  0.452569 ,  1.48135  ,\n",
       "       -0.713579 ,  1.27009  , -0.427368 ,  0.908545 , -1.30073  ,\n",
       "       -0.742945 ,  2.13016  ,  2.15647  , -0.657836 , -1.43292  ,\n",
       "       -0.432578 , -0.370477 ,  0.788179 , -0.439624 , -1.42123  ,\n",
       "        0.310941 ,  1.36179  ,  0.0817599, -1.7725   , -2.02928  ,\n",
       "       -1.27779  ,  0.776984 , -0.589031 , -2.23493  , -1.11084  ,\n",
       "        2.1565   , -0.0829913,  1.62006  ,  2.88058  , -1.30103  ,\n",
       "       -1.09147  , -0.391706 , -0.708378 ,  0.576634 , -1.87945  ,\n",
       "        1.01122  , -2.27341  , -2.89224  , -0.21766  ,  2.39044  ,\n",
       "       -0.969998 ,  1.07507  ,  0.668908 ,  0.393559 ,  0.0897984,\n",
       "       -0.539963 ,  2.9958   , -2.62722  ,  1.317    , -0.59901  ,\n",
       "       -2.1971   , -1.61251  ,  0.313467 ,  1.80279  , -0.0893186,\n",
       "        0.704141 , -1.02295  , -1.42039  ,  2.33347  , -2.55586  ,\n",
       "       -1.478    ,  1.61922  ,  2.75623  , -0.383152 , -0.52249  ,\n",
       "       -2.45576  ,  2.1599   , -1.4243   , -0.343521 ,  1.27545  ,\n",
       "        0.491944 ,  0.618774 , -0.0998952,  1.2947   ,  0.825262 ,\n",
       "       -0.69509  , -0.319014 ,  0.324287 , -2.59738  , -0.637168 ,\n",
       "        1.81155  , -1.0284   ,  0.589438 ,  2.85367  ,  0.603518 ,\n",
       "        0.318728 , -0.412848 , -2.08189  ,  1.01839  ,  1.8939   ,\n",
       "        1.55598  ,  0.213352 ,  0.461722 ,  0.11826  ,  0.0267475,\n",
       "       -0.0369905, -1.10652  , -0.740529 , -0.426003 , -1.54028  ,\n",
       "        0.0773929,  1.57397  ,  0.468904 ,  0.409823 , -1.01757  ,\n",
       "       -0.45491  , -0.449531 ,  2.00295  ,  0.540048 ,  2.12338  ,\n",
       "        0.683898 , -0.596373 , -0.104335 ,  0.678856 , -2.97153  ,\n",
       "        0.654097 ,  0.6221   ,  1.33486  , -0.590548 ,  1.37716  ,\n",
       "       -0.660667 ,  0.732702 ,  1.94802  , -1.23631  ,  3.0867   ,\n",
       "       -2.91285  ,  0.646966 , -1.06234  ,  0.0869951,  0.229186 ,\n",
       "       -0.86691  , -0.597497 , -1.96825  , -0.456173 , -1.09852  ,\n",
       "        1.04545  , -0.387094 , -0.649523 , -0.0902257, -1.9431   ,\n",
       "       -0.976336 ,  1.01738  ,  0.194592 , -0.074333 ,  0.715081 ,\n",
       "        0.0595207,  1.32296  , -0.295649 ,  2.86259  , -2.96882  ,\n",
       "       -1.10962  , -1.98632  , -1.22619  , -0.284225 ,  0.392745 ,\n",
       "       -0.421852 , -0.0695059, -2.04821  , -0.539996 ,  1.01838  ,\n",
       "        0.865285 , -0.860027 ,  0.485041 ,  0.439294 ,  1.08262  ,\n",
       "       -2.04917  , -0.431008 , -0.535928 ,  1.59408  ,  1.77033  ,\n",
       "       -0.538983 ,  3.30947  ,  1.00562  , -0.981646 , -0.118559 ,\n",
       "        2.41239  , -0.793075 , -0.85939  ,  1.21884  , -1.47254  ,\n",
       "       -0.389585 , -2.17884  ,  1.06063  , -3.05622  , -1.06393  ,\n",
       "        0.0178502, -0.172516 ,  0.251927 , -1.00737  ,  1.66495  ,\n",
       "        2.43406  ,  0.133554 ,  1.1932   , -2.30438  , -1.99064  ,\n",
       "       -0.312041 , -1.93902  ,  0.26563  , -1.00119  ,  1.17534  ,\n",
       "        1.35473  ,  0.947308 , -0.0339226, -0.778325 , -1.2718   ,\n",
       "       -0.722659 , -0.266433 , -2.38414  , -2.42451  , -0.721286 ,\n",
       "       -1.37891  , -1.65384  ,  0.32894  ,  1.75604  , -0.77313  ,\n",
       "        1.51866  , -1.60757  , -1.64426  ,  1.68925  ,  1.21393  ,\n",
       "        2.0976   , -0.415502 ,  1.38358  ,  1.96632  , -0.281563 ,\n",
       "        0.463067 , -0.10405  ,  0.298547 , -1.95347  ,  1.17299  ,\n",
       "       -0.21517  ,  1.87654  , -0.464946 , -0.105183 , -0.441476 ,\n",
       "       -1.12336  ,  2.30802  , -1.42575  ,  3.18382  ,  2.68174  ,\n",
       "       -0.594929 ,  0.419408 , -3.13046  ,  1.05513  , -0.546862 ,\n",
       "        0.227277 , -0.067337 ,  1.0119   , -2.67388  ,  3.06748  ,\n",
       "        2.19581  , -0.570493 , -1.50234  ,  2.8291   , -0.233852 ,\n",
       "       -1.95223  , -0.858177 ,  0.447596 ,  0.703766 , -1.19499  ,\n",
       "        1.60821  ,  0.595142 ,  1.3373   ,  1.27369  ,  0.518926 ,\n",
       "       -0.93014  , -2.37045  , -0.730593 , -0.796339 , -1.95475  ,\n",
       "        0.921851 , -1.72625  ,  0.0947835, -1.00169  ,  0.859436 ,\n",
       "       -2.45586  ,  2.42763  , -1.00112  ,  1.07748  , -0.40437  ,\n",
       "       -0.0130815,  0.298923 ,  1.73343  , -1.98648  ,  0.558366 ,\n",
       "        0.618461 ,  1.22759  , -1.71584  , -0.0609592,  0.322765 ,\n",
       "        0.407554 ,  1.61751  , -1.77222  ,  0.203277 ,  1.09255  ])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence as seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_and_transform_data_to_phrases(labels, parents, tokens, words2ids):\n",
    "\n",
    "    \"\"\"\n",
    "    Dokumentacja\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    transform_label = {'-1':0, '0':1, '1':2}\n",
    "    \n",
    "    l = open(labels, \"r\")\n",
    "    labels = [[transform_label[y] for y in x.split()] for x in l.readlines()] \n",
    "    l.close()\n",
    "\n",
    "    p = open(parents,\"r\")\n",
    "    parents = [[int(y) for y in x.split()] for x in p.readlines()]\n",
    "    p.close()\n",
    "\n",
    "    t = open(tokens,\"r\")\n",
    "    tokens = [x.split() for x in t.readlines()]\n",
    "    t.close()\n",
    "    \n",
    "    k = 0\n",
    "    result = []\n",
    "    \n",
    "    for labels_i,parents_i,tokens_i in zip(labels,parents,tokens):\n",
    "        \n",
    "        k = k + 1\n",
    "         \n",
    "        s = []\n",
    "        for i in range(len(tokens_i)):\n",
    "            s.append([i,int(parents_i[i]),labels_i[i],tokens_i[i]])\n",
    "\n",
    "\n",
    "        if len(s) == 1: #przypadek gdy fraza sklada sie z jednego tokena\n",
    "\n",
    "            result.append((\\\n",
    "                                  tokens[0],\n",
    "                                  np.array([words2ids.get(tokens[0], 1)]),\\\n",
    "                                  np.array(labels_i[0]) \\\n",
    "                              ))    \n",
    "                           \n",
    "        else: \n",
    "            \n",
    "            for i in range(len(s)): \n",
    "                children = []\n",
    "                for j in range(len(s)):\n",
    "                    if s[j][1] == i+1:\n",
    "                        children.append(s[j][0])\n",
    "                s[i].append(children)\n",
    "\n",
    "                \n",
    "            words = [x[0] for x in s]\n",
    "            children = [x[4] for x in s]\n",
    "            tokens = [x[3] for x in s]\n",
    "            labels_in_batch = [x[2] for x in s]\n",
    "        \n",
    "            phrases = [[k] for k in range(len(children))]\n",
    "            for i in range(len(children)):\n",
    "                for e in phrases[i]:\n",
    "                    phrases[i].extend(children[e])\n",
    "           \n",
    "            phrases = [ np.sort(x) for x in phrases]\n",
    "          \n",
    "            phrases = list(zip([np.array(tokens_i)[x] for x in phrases],\n",
    "                               [np.array([words2ids.get(t,1) for t in tokens_i])[x] for x in phrases],\n",
    "                               labels_i))\n",
    "\n",
    "            result.extend(phrases)\n",
    "           \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = load_and_transform_data_to_phrases(\"Dane/data_poleval/training-treebank/rev_labels.txt\", \"Dane/data_poleval/training-treebank/rev_parents.txt\",\"Dane/data_poleval/training-treebank/rev_sentence.txt\",words2ids)\n",
    "test_data = load_and_transform_data_to_phrases(\"Dane/data_poleval/gold_labels\", \"Dane/data_poleval/poleval_test/polevaltest_parents.txt\",\"Dane/data_poleval/poleval_test/polevaltest_sentence.txt\",words2ids) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array(['Słodkawy'], \n",
       "        dtype='<U8'), array([2731]), 1),\n",
       " (array(['Słodkawy', 'i', 'pełen', 'klasy', '.'], \n",
       "        dtype='<U8'), array([2731, 1746, 1465,  515,    1]), 1),\n",
       " (array(['pełen'], \n",
       "        dtype='<U8'), array([1465]), 2),\n",
       " (array(['pełen', 'klasy'], \n",
       "        dtype='<U8'), array([1465,  515]), 2),\n",
       " (array(['.'], \n",
       "        dtype='<U8'), array([1]), 1),\n",
       " (array(['Letnia'], \n",
       "        dtype='<U13'), array([1277]), 1),\n",
       " (array(['Letnia', 'propozycja', 'dla', 'wysmakowanych', 'mężczyzn', '.'], \n",
       "        dtype='<U13'), array([1277, 1570, 3893, 1789, 3526,    1]), 2),\n",
       " (array(['dla', 'wysmakowanych', 'mężczyzn'], \n",
       "        dtype='<U13'), array([3893, 1789, 3526]), 2),\n",
       " (array(['wysmakowanych'], \n",
       "        dtype='<U13'), array([1789]), 1),\n",
       " (array(['wysmakowanych', 'mężczyzn'], \n",
       "        dtype='<U13'), array([1789, 3526]), 2),\n",
       " (array(['.'], \n",
       "        dtype='<U13'), array([1]), 1),\n",
       " (array(['Raczej', 'nie', 'dla', 'młodych', 'chłopców', '.'], \n",
       "        dtype='<U8'), array([3458, 4205, 3893, 4780, 4677,    1]), 1),\n",
       " (array(['nie', 'dla', 'młodych', 'chłopców', '.'], \n",
       "        dtype='<U8'), array([4205, 3893, 4780, 4677,    1]), 1),\n",
       " (array(['dla', 'młodych', 'chłopców'], \n",
       "        dtype='<U8'), array([3893, 4780, 4677]), 1),\n",
       " (array(['młodych'], \n",
       "        dtype='<U8'), array([4780]), 2)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 - pozytywny\n",
    "# 1 - neutralny\n",
    "# 0 - negatywny\n",
    "train_data[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "(X_train, y_train), \\\n",
    "(X_test, y_test) = \\\n",
    "( [x[1] for x in train_data], np.array(pd.get_dummies(np.array([x[2] for x in train_data]))) ) , \\\n",
    "( [x[1] for x in test_data], np.array(pd.get_dummies(np.array([x[2] for x in test_data]))) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([2731]),\n",
       " array([2731, 1746, 1465,  515,    1]),\n",
       " array([1465]),\n",
       " array([1465,  515]),\n",
       " array([1])]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "max_len = np.max([len(x[1]) for x in train_data+test_data])\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_len,value=0)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_len,value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0, 2731],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0, 2731, 1746, 1465,  515,    1],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0, 1465],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0, 1465,  515],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    1]], dtype=int32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9510, 40)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02355415  0.7809674   0.19547844]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.07232019,  0.7263721 ,  0.20130771])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rozklad klas\n",
    "print(np.mean(y_train,axis=0))\n",
    "np.mean(y_test,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, LSTM, SimpleRNN, Bidirectional, Activation\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "n_embeddings = embeddings.shape[0] # zawiera 1 na brakujace slowa i 1 na padding\n",
    "embedding_vecor_length = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cel: zbudować jak najlepszą sieć klasyfikującą wydźwięk fraz. \n",
    "\n",
    "Sugestia: przeanalizować poprzednie model z różnymi parametrami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, (9510, 40))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_embeddings, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7132 samples, validate on 2378 samples\n",
      "Epoch 1/30\n",
      "7132/7132 [==============================] - 16s - loss: 0.5636 - categorical_accuracy: 0.7657 - val_loss: 0.5383 - val_categorical_accuracy: 0.7738\n",
      "Epoch 2/30\n",
      "7132/7132 [==============================] - 16s - loss: 0.4169 - categorical_accuracy: 0.8365 - val_loss: 0.4833 - val_categorical_accuracy: 0.8108\n",
      "Epoch 3/30\n",
      "7132/7132 [==============================] - 16s - loss: 0.3679 - categorical_accuracy: 0.8596 - val_loss: 0.4651 - val_categorical_accuracy: 0.8171\n",
      "Epoch 4/30\n",
      "7132/7132 [==============================] - 16s - loss: 0.3349 - categorical_accuracy: 0.8755 - val_loss: 0.4497 - val_categorical_accuracy: 0.8318\n",
      "Epoch 5/30\n",
      "7132/7132 [==============================] - 15s - loss: 0.3093 - categorical_accuracy: 0.8878 - val_loss: 0.4414 - val_categorical_accuracy: 0.8415\n",
      "Epoch 6/30\n",
      "7132/7132 [==============================] - 15s - loss: 0.2875 - categorical_accuracy: 0.8986 - val_loss: 0.4397 - val_categorical_accuracy: 0.8394\n",
      "Epoch 7/30\n",
      "7132/7132 [==============================] - 15s - loss: 0.2685 - categorical_accuracy: 0.9073 - val_loss: 0.4380 - val_categorical_accuracy: 0.8402\n",
      "Epoch 8/30\n",
      "7132/7132 [==============================] - 15s - loss: 0.2515 - categorical_accuracy: 0.9162 - val_loss: 0.4364 - val_categorical_accuracy: 0.8444\n",
      "Epoch 9/30\n",
      "7132/7132 [==============================] - 15s - loss: 0.2353 - categorical_accuracy: 0.9199 - val_loss: 0.4335 - val_categorical_accuracy: 0.8558\n",
      "Epoch 10/30\n",
      "7132/7132 [==============================] - 15s - loss: 0.2211 - categorical_accuracy: 0.9257 - val_loss: 0.4383 - val_categorical_accuracy: 0.8490\n",
      "Epoch 11/30\n",
      "7132/7132 [==============================] - 15s - loss: 0.2073 - categorical_accuracy: 0.9316 - val_loss: 0.4346 - val_categorical_accuracy: 0.8562\n",
      "Epoch 12/30\n",
      "7132/7132 [==============================] - 15s - loss: 0.1952 - categorical_accuracy: 0.9372 - val_loss: 0.4357 - val_categorical_accuracy: 0.8574\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13e886c50>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(n_embeddings, embedding_vecor_length, input_length=max_len, weights=[embeddings]))\n",
    "model.add(SimpleRNN(100))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['categorical_accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(patience=2, monitor='val_loss')\n",
    "\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=batch_size, callbacks=[early_stopping], validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5024/5047 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.77848226675213461"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     (None, 40, 300)           1500000   \n",
      "_________________________________________________________________\n",
      "simple_rnn_11 (SimpleRNN)    (None, 100)               40100     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 1,540,403\n",
      "Trainable params: 1,540,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LSTM - porównaj na zbiorze testowym jakość działania modelu wziętego z najlepszej iteracji oraz modelu po zatrzymaniu uczenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7132 samples, validate on 2378 samples\n",
      "Epoch 1/30\n",
      "7132/7132 [==============================] - 58s - loss: 0.6987 - categorical_accuracy: 0.7650 - val_loss: 0.6023 - val_categorical_accuracy: 0.7611\n",
      "Epoch 2/30\n",
      "7132/7132 [==============================] - 55s - loss: 0.4972 - categorical_accuracy: 0.8044 - val_loss: 0.5327 - val_categorical_accuracy: 0.7733\n",
      "Epoch 3/30\n",
      "7132/7132 [==============================] - 59s - loss: 0.4455 - categorical_accuracy: 0.8236 - val_loss: 0.4949 - val_categorical_accuracy: 0.7939\n",
      "Epoch 4/30\n",
      "7132/7132 [==============================] - 63s - loss: 0.4150 - categorical_accuracy: 0.8392 - val_loss: 0.4702 - val_categorical_accuracy: 0.8053\n",
      "Epoch 5/30\n",
      "7132/7132 [==============================] - 68s - loss: 0.3920 - categorical_accuracy: 0.8493 - val_loss: 0.4513 - val_categorical_accuracy: 0.8116\n",
      "Epoch 6/30\n",
      "7132/7132 [==============================] - 61s - loss: 0.3732 - categorical_accuracy: 0.8568 - val_loss: 0.4365 - val_categorical_accuracy: 0.8213\n",
      "Epoch 7/30\n",
      "7132/7132 [==============================] - 62s - loss: 0.3573 - categorical_accuracy: 0.8639 - val_loss: 0.4251 - val_categorical_accuracy: 0.8255\n",
      "Epoch 8/30\n",
      "7132/7132 [==============================] - 62s - loss: 0.3431 - categorical_accuracy: 0.8704 - val_loss: 0.4133 - val_categorical_accuracy: 0.8364\n",
      "Epoch 9/30\n",
      "7132/7132 [==============================] - 66s - loss: 0.3302 - categorical_accuracy: 0.8759 - val_loss: 0.4130 - val_categorical_accuracy: 0.8326\n",
      "Epoch 10/30\n",
      "7132/7132 [==============================] - 63s - loss: 0.3186 - categorical_accuracy: 0.8808 - val_loss: 0.3986 - val_categorical_accuracy: 0.8436\n",
      "Epoch 11/30\n",
      "7132/7132 [==============================] - 66s - loss: 0.3080 - categorical_accuracy: 0.8836 - val_loss: 0.3956 - val_categorical_accuracy: 0.8444\n",
      "Epoch 12/30\n",
      "7132/7132 [==============================] - 71s - loss: 0.2979 - categorical_accuracy: 0.8883 - val_loss: 0.3875 - val_categorical_accuracy: 0.8507\n",
      "Epoch 13/30\n",
      "7132/7132 [==============================] - 69s - loss: 0.2886 - categorical_accuracy: 0.8943 - val_loss: 0.3876 - val_categorical_accuracy: 0.8503\n",
      "Epoch 14/30\n",
      "7132/7132 [==============================] - 68s - loss: 0.2795 - categorical_accuracy: 0.8989 - val_loss: 0.3774 - val_categorical_accuracy: 0.8549\n",
      "Epoch 15/30\n",
      "7132/7132 [==============================] - 67s - loss: 0.2711 - categorical_accuracy: 0.9010 - val_loss: 0.3802 - val_categorical_accuracy: 0.8545\n",
      "Epoch 16/30\n",
      "7132/7132 [==============================] - 63s - loss: 0.2635 - categorical_accuracy: 0.9044 - val_loss: 0.3766 - val_categorical_accuracy: 0.8553\n",
      "Epoch 17/30\n",
      "7132/7132 [==============================] - 64s - loss: 0.2559 - categorical_accuracy: 0.9068 - val_loss: 0.3704 - val_categorical_accuracy: 0.8595\n",
      "Epoch 18/30\n",
      "7132/7132 [==============================] - 63s - loss: 0.2486 - categorical_accuracy: 0.9119 - val_loss: 0.3684 - val_categorical_accuracy: 0.8600\n",
      "Epoch 19/30\n",
      "7132/7132 [==============================] - 63s - loss: 0.2418 - categorical_accuracy: 0.9162 - val_loss: 0.3732 - val_categorical_accuracy: 0.8608\n",
      "Epoch 20/30\n",
      "7132/7132 [==============================] - 64s - loss: 0.2354 - categorical_accuracy: 0.9183 - val_loss: 0.3629 - val_categorical_accuracy: 0.8642\n",
      "Epoch 21/30\n",
      "7132/7132 [==============================] - 64s - loss: 0.2289 - categorical_accuracy: 0.9225 - val_loss: 0.3704 - val_categorical_accuracy: 0.8654\n",
      "Epoch 22/30\n",
      "7132/7132 [==============================] - 63s - loss: 0.2228 - categorical_accuracy: 0.9233 - val_loss: 0.3655 - val_categorical_accuracy: 0.8663\n",
      "Epoch 23/30\n",
      "7132/7132 [==============================] - 69s - loss: 0.2173 - categorical_accuracy: 0.9264 - val_loss: 0.3583 - val_categorical_accuracy: 0.8692\n",
      "Epoch 24/30\n",
      "7132/7132 [==============================] - 71s - loss: 0.2116 - categorical_accuracy: 0.9296 - val_loss: 0.3554 - val_categorical_accuracy: 0.8734\n",
      "Epoch 25/30\n",
      "7132/7132 [==============================] - 71s - loss: 0.2063 - categorical_accuracy: 0.9333 - val_loss: 0.3573 - val_categorical_accuracy: 0.8730\n",
      "Epoch 26/30\n",
      "7132/7132 [==============================] - 67s - loss: 0.2009 - categorical_accuracy: 0.9352 - val_loss: 0.3546 - val_categorical_accuracy: 0.8759\n",
      "Epoch 27/30\n",
      "7132/7132 [==============================] - 69s - loss: 0.1960 - categorical_accuracy: 0.9369 - val_loss: 0.3540 - val_categorical_accuracy: 0.8743\n",
      "Epoch 28/30\n",
      "7132/7132 [==============================] - 68s - loss: 0.1909 - categorical_accuracy: 0.9391 - val_loss: 0.3518 - val_categorical_accuracy: 0.8751\n",
      "Epoch 29/30\n",
      "7132/7132 [==============================] - 69s - loss: 0.1863 - categorical_accuracy: 0.9425 - val_loss: 0.3487 - val_categorical_accuracy: 0.8751\n",
      "Epoch 30/30\n",
      "7132/7132 [==============================] - 66s - loss: 0.1817 - categorical_accuracy: 0.9436 - val_loss: 0.3486 - val_categorical_accuracy: 0.8764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1445159b0>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(n_embeddings, embedding_vecor_length, input_length=max_len, weights=[embeddings]))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['categorical_accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(patience=2, monitor='val_loss')\n",
    "\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=batch_size, callbacks=[early_stopping], validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5047/5047 [==============================] - 8s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.79552209220216941"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 40, 300)           1500000   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 1,660,703\n",
      "Trainable params: 1,660,703\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LSTM + warstwa dense na końcu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7132 samples, validate on 2378 samples\n",
      "Epoch 1/30\n",
      "7132/7132 [==============================] - 75s - loss: 0.5967 - categorical_accuracy: 0.7895 - val_loss: 0.6439 - val_categorical_accuracy: 0.7553\n",
      "Epoch 2/30\n",
      "7132/7132 [==============================] - 71s - loss: 0.5701 - categorical_accuracy: 0.7895 - val_loss: 0.6295 - val_categorical_accuracy: 0.7553\n",
      "Epoch 3/30\n",
      "7132/7132 [==============================] - 71s - loss: 0.5592 - categorical_accuracy: 0.7895 - val_loss: 0.6216 - val_categorical_accuracy: 0.7553\n",
      "Epoch 4/30\n",
      "7132/7132 [==============================] - 70s - loss: 0.5464 - categorical_accuracy: 0.7895 - val_loss: 0.6049 - val_categorical_accuracy: 0.7553\n",
      "Epoch 5/30\n",
      "7132/7132 [==============================] - 71s - loss: 0.5325 - categorical_accuracy: 0.7895 - val_loss: 0.5983 - val_categorical_accuracy: 0.7553\n",
      "Epoch 6/30\n",
      "7132/7132 [==============================] - 72s - loss: 0.5164 - categorical_accuracy: 0.7902 - val_loss: 0.5791 - val_categorical_accuracy: 0.7561\n",
      "Epoch 7/30\n",
      "7132/7132 [==============================] - 70s - loss: 0.4987 - categorical_accuracy: 0.7973 - val_loss: 0.5612 - val_categorical_accuracy: 0.7607\n",
      "Epoch 8/30\n",
      "7132/7132 [==============================] - 71s - loss: 0.4806 - categorical_accuracy: 0.8093 - val_loss: 0.5482 - val_categorical_accuracy: 0.7641\n",
      "Epoch 9/30\n",
      "7132/7132 [==============================] - 72s - loss: 0.4627 - categorical_accuracy: 0.8172 - val_loss: 0.5182 - val_categorical_accuracy: 0.7788\n",
      "Epoch 10/30\n",
      "7132/7132 [==============================] - 71s - loss: 0.4462 - categorical_accuracy: 0.8228 - val_loss: 0.5057 - val_categorical_accuracy: 0.7813\n",
      "Epoch 11/30\n",
      "7132/7132 [==============================] - 72s - loss: 0.4308 - categorical_accuracy: 0.8274 - val_loss: 0.4901 - val_categorical_accuracy: 0.7876\n",
      "Epoch 12/30\n",
      "7132/7132 [==============================] - 69s - loss: 0.4161 - categorical_accuracy: 0.8322 - val_loss: 0.4773 - val_categorical_accuracy: 0.7923\n",
      "Epoch 13/30\n",
      "7132/7132 [==============================] - 67s - loss: 0.4038 - categorical_accuracy: 0.8381 - val_loss: 0.4659 - val_categorical_accuracy: 0.7952\n",
      "Epoch 14/30\n",
      "7132/7132 [==============================] - 66s - loss: 0.3919 - categorical_accuracy: 0.8472 - val_loss: 0.4512 - val_categorical_accuracy: 0.8150\n",
      "Epoch 15/30\n",
      "7132/7132 [==============================] - 66s - loss: 0.3807 - categorical_accuracy: 0.8542 - val_loss: 0.4438 - val_categorical_accuracy: 0.8188\n",
      "Epoch 16/30\n",
      "7132/7132 [==============================] - 67s - loss: 0.3710 - categorical_accuracy: 0.8608 - val_loss: 0.4372 - val_categorical_accuracy: 0.8225\n",
      "Epoch 17/30\n",
      "7132/7132 [==============================] - 70s - loss: 0.3614 - categorical_accuracy: 0.8639 - val_loss: 0.4291 - val_categorical_accuracy: 0.8267\n",
      "Epoch 18/30\n",
      "7132/7132 [==============================] - 65s - loss: 0.3529 - categorical_accuracy: 0.8688 - val_loss: 0.4193 - val_categorical_accuracy: 0.8394\n",
      "Epoch 19/30\n",
      "7132/7132 [==============================] - 63s - loss: 0.3444 - categorical_accuracy: 0.8730 - val_loss: 0.4132 - val_categorical_accuracy: 0.8436\n",
      "Epoch 20/30\n",
      "7132/7132 [==============================] - 65s - loss: 0.3362 - categorical_accuracy: 0.8775 - val_loss: 0.4022 - val_categorical_accuracy: 0.8486\n",
      "Epoch 21/30\n",
      "7132/7132 [==============================] - 67s - loss: 0.3291 - categorical_accuracy: 0.8817 - val_loss: 0.4101 - val_categorical_accuracy: 0.8440\n",
      "Epoch 22/30\n",
      "7132/7132 [==============================] - 68s - loss: 0.3216 - categorical_accuracy: 0.8856 - val_loss: 0.3986 - val_categorical_accuracy: 0.8545\n",
      "Epoch 23/30\n",
      "7132/7132 [==============================] - 74s - loss: 0.3149 - categorical_accuracy: 0.8864 - val_loss: 0.3951 - val_categorical_accuracy: 0.8553\n",
      "Epoch 24/30\n",
      "7132/7132 [==============================] - 73s - loss: 0.3081 - categorical_accuracy: 0.8904 - val_loss: 0.3894 - val_categorical_accuracy: 0.8562\n",
      "Epoch 25/30\n",
      "7132/7132 [==============================] - 68s - loss: 0.3017 - categorical_accuracy: 0.8934 - val_loss: 0.3930 - val_categorical_accuracy: 0.8574\n",
      "Epoch 26/30\n",
      "7132/7132 [==============================] - 68s - loss: 0.2954 - categorical_accuracy: 0.8968 - val_loss: 0.3786 - val_categorical_accuracy: 0.8646\n",
      "Epoch 27/30\n",
      "7132/7132 [==============================] - 59s - loss: 0.2892 - categorical_accuracy: 0.9002 - val_loss: 0.3812 - val_categorical_accuracy: 0.8612\n",
      "Epoch 28/30\n",
      "7132/7132 [==============================] - 54s - loss: 0.2832 - categorical_accuracy: 0.9028 - val_loss: 0.3750 - val_categorical_accuracy: 0.8663\n",
      "Epoch 29/30\n",
      "7132/7132 [==============================] - 54s - loss: 0.2773 - categorical_accuracy: 0.9073 - val_loss: 0.3782 - val_categorical_accuracy: 0.8675\n",
      "Epoch 30/30\n",
      "7132/7132 [==============================] - 57s - loss: 0.2718 - categorical_accuracy: 0.9077 - val_loss: 0.3742 - val_categorical_accuracy: 0.8717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x147637f60>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(n_embeddings, embedding_vecor_length, input_length=max_len, weights=[embeddings]))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='sigmoid'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['categorical_accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(patience=2, monitor='val_loss')\n",
    "\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=batch_size, callbacks=[early_stopping], validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5024/5047 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.79532395469473927"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, 40, 300)           1500000   \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 1,670,803\n",
      "Trainable params: 1,670,803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7132 samples, validate on 2378 samples\n",
      "Epoch 1/30\n",
      "7132/7132 [==============================] - 82s - loss: 0.6846 - categorical_accuracy: 0.7709 - val_loss: 0.6155 - val_categorical_accuracy: 0.7557\n",
      "Epoch 2/30\n",
      "7132/7132 [==============================] - 72s - loss: 0.5190 - categorical_accuracy: 0.7963 - val_loss: 0.5573 - val_categorical_accuracy: 0.7645\n",
      "Epoch 3/30\n",
      "7132/7132 [==============================] - 72s - loss: 0.4723 - categorical_accuracy: 0.8125 - val_loss: 0.5177 - val_categorical_accuracy: 0.7763\n",
      "Epoch 4/30\n",
      "7132/7132 [==============================] - 75s - loss: 0.4412 - categorical_accuracy: 0.8245 - val_loss: 0.4871 - val_categorical_accuracy: 0.7952\n",
      "Epoch 5/30\n",
      "7132/7132 [==============================] - 80s - loss: 0.4175 - categorical_accuracy: 0.8330 - val_loss: 0.4649 - val_categorical_accuracy: 0.8019\n",
      "Epoch 6/30\n",
      "7132/7132 [==============================] - 82s - loss: 0.3981 - categorical_accuracy: 0.8444 - val_loss: 0.4488 - val_categorical_accuracy: 0.8095\n",
      "Epoch 7/30\n",
      "7132/7132 [==============================] - 78s - loss: 0.3818 - categorical_accuracy: 0.8531 - val_loss: 0.4353 - val_categorical_accuracy: 0.8167\n",
      "Epoch 8/30\n",
      "7132/7132 [==============================] - 70s - loss: 0.3673 - categorical_accuracy: 0.8613 - val_loss: 0.4245 - val_categorical_accuracy: 0.8242\n",
      "Epoch 9/30\n",
      "7132/7132 [==============================] - 73s - loss: 0.3548 - categorical_accuracy: 0.8658 - val_loss: 0.4116 - val_categorical_accuracy: 0.8360\n",
      "Epoch 10/30\n",
      "7132/7132 [==============================] - 78s - loss: 0.3431 - categorical_accuracy: 0.8716 - val_loss: 0.4055 - val_categorical_accuracy: 0.8394\n",
      "Epoch 11/30\n",
      "7132/7132 [==============================] - 78s - loss: 0.3322 - categorical_accuracy: 0.8759 - val_loss: 0.3990 - val_categorical_accuracy: 0.8431\n",
      "Epoch 12/30\n",
      "7132/7132 [==============================] - 75s - loss: 0.3218 - categorical_accuracy: 0.8822 - val_loss: 0.4009 - val_categorical_accuracy: 0.8398\n",
      "Epoch 13/30\n",
      "7132/7132 [==============================] - 72s - loss: 0.3123 - categorical_accuracy: 0.8853 - val_loss: 0.3919 - val_categorical_accuracy: 0.8482\n",
      "Epoch 14/30\n",
      "7132/7132 [==============================] - 77s - loss: 0.3031 - categorical_accuracy: 0.8897 - val_loss: 0.3833 - val_categorical_accuracy: 0.8528\n",
      "Epoch 15/30\n",
      "7132/7132 [==============================] - 81s - loss: 0.2943 - categorical_accuracy: 0.8940 - val_loss: 0.3815 - val_categorical_accuracy: 0.8528\n",
      "Epoch 16/30\n",
      "7132/7132 [==============================] - 75s - loss: 0.2859 - categorical_accuracy: 0.8957 - val_loss: 0.3758 - val_categorical_accuracy: 0.8616\n",
      "Epoch 17/30\n",
      "7132/7132 [==============================] - 75s - loss: 0.2777 - categorical_accuracy: 0.8996 - val_loss: 0.3696 - val_categorical_accuracy: 0.8633\n",
      "Epoch 18/30\n",
      "7132/7132 [==============================] - 76s - loss: 0.2701 - categorical_accuracy: 0.9044 - val_loss: 0.3677 - val_categorical_accuracy: 0.8638\n",
      "Epoch 19/30\n",
      "7132/7132 [==============================] - 78s - loss: 0.2626 - categorical_accuracy: 0.9080 - val_loss: 0.3690 - val_categorical_accuracy: 0.8646\n",
      "Epoch 20/30\n",
      "7132/7132 [==============================] - 78s - loss: 0.2553 - categorical_accuracy: 0.9096 - val_loss: 0.3633 - val_categorical_accuracy: 0.8633\n",
      "Epoch 21/30\n",
      "7132/7132 [==============================] - 68s - loss: 0.2487 - categorical_accuracy: 0.9136 - val_loss: 0.3665 - val_categorical_accuracy: 0.8650\n",
      "Epoch 22/30\n",
      "7132/7132 [==============================] - 67s - loss: 0.2420 - categorical_accuracy: 0.9166 - val_loss: 0.3607 - val_categorical_accuracy: 0.8638\n",
      "Epoch 23/30\n",
      "7132/7132 [==============================] - 65s - loss: 0.2356 - categorical_accuracy: 0.9187 - val_loss: 0.3597 - val_categorical_accuracy: 0.8633\n",
      "Epoch 24/30\n",
      "7132/7132 [==============================] - 65s - loss: 0.2290 - categorical_accuracy: 0.9212 - val_loss: 0.3579 - val_categorical_accuracy: 0.8638\n",
      "Epoch 25/30\n",
      "7132/7132 [==============================] - 67s - loss: 0.2236 - categorical_accuracy: 0.9220 - val_loss: 0.3584 - val_categorical_accuracy: 0.8650\n",
      "Epoch 26/30\n",
      "7132/7132 [==============================] - 73s - loss: 0.2178 - categorical_accuracy: 0.9251 - val_loss: 0.3653 - val_categorical_accuracy: 0.8667\n",
      "Epoch 27/30\n",
      "7132/7132 [==============================] - 72s - loss: 0.2118 - categorical_accuracy: 0.9275 - val_loss: 0.3721 - val_categorical_accuracy: 0.8671\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14cd37da0>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(n_embeddings, embedding_vecor_length, input_length=max_len, weights=[embeddings]))\n",
    "model.add(Bidirectional(LSTM(100)))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['categorical_accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(patience=2, monitor='val_loss')\n",
    "\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=batch_size, callbacks=[early_stopping], validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5047/5047 [==============================] - 12s    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.79770160478390106"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 40, 300)           1500000   \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 3)                 603       \n",
      "=================================================================\n",
      "Total params: 1,821,403\n",
      "Trainable params: 1,821,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dwuwarstwowy LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7132 samples, validate on 2378 samples\n",
      "Epoch 1/30\n",
      "7132/7132 [==============================] - 107s - loss: 0.6853 - categorical_accuracy: 0.7815 - val_loss: 0.6396 - val_categorical_accuracy: 0.7553\n",
      "Epoch 2/30\n",
      "7132/7132 [==============================] - 121s - loss: 0.5662 - categorical_accuracy: 0.7895 - val_loss: 0.6135 - val_categorical_accuracy: 0.7553\n",
      "Epoch 3/30\n",
      "7132/7132 [==============================] - 100s - loss: 0.5475 - categorical_accuracy: 0.7895 - val_loss: 0.6011 - val_categorical_accuracy: 0.7553\n",
      "Epoch 4/30\n",
      "7132/7132 [==============================] - 98s - loss: 0.5327 - categorical_accuracy: 0.7891 - val_loss: 0.5850 - val_categorical_accuracy: 0.7536\n",
      "Epoch 5/30\n",
      "7132/7132 [==============================] - 100s - loss: 0.5183 - categorical_accuracy: 0.7876 - val_loss: 0.5754 - val_categorical_accuracy: 0.7515\n",
      "Epoch 6/30\n",
      "7132/7132 [==============================] - 99s - loss: 0.5039 - categorical_accuracy: 0.7873 - val_loss: 0.5613 - val_categorical_accuracy: 0.7540\n",
      "Epoch 7/30\n",
      "7132/7132 [==============================] - 100s - loss: 0.4887 - categorical_accuracy: 0.7895 - val_loss: 0.5417 - val_categorical_accuracy: 0.7590\n",
      "Epoch 8/30\n",
      "7132/7132 [==============================] - 133s - loss: 0.4728 - categorical_accuracy: 0.7954 - val_loss: 0.5332 - val_categorical_accuracy: 0.7632\n",
      "Epoch 9/30\n",
      "7132/7132 [==============================] - 130s - loss: 0.4566 - categorical_accuracy: 0.8050 - val_loss: 0.5152 - val_categorical_accuracy: 0.7750\n",
      "Epoch 10/30\n",
      "7132/7132 [==============================] - 128s - loss: 0.4402 - categorical_accuracy: 0.8180 - val_loss: 0.5069 - val_categorical_accuracy: 0.7809\n",
      "Epoch 11/30\n",
      "7132/7132 [==============================] - 106s - loss: 0.4249 - categorical_accuracy: 0.8268 - val_loss: 0.4921 - val_categorical_accuracy: 0.7897\n",
      "Epoch 12/30\n",
      "7132/7132 [==============================] - 117s - loss: 0.4101 - categorical_accuracy: 0.8371 - val_loss: 0.4796 - val_categorical_accuracy: 0.8007\n",
      "Epoch 13/30\n",
      "7132/7132 [==============================] - 123s - loss: 0.3963 - categorical_accuracy: 0.8480 - val_loss: 0.4706 - val_categorical_accuracy: 0.8099\n",
      "Epoch 14/30\n",
      "7132/7132 [==============================] - 123s - loss: 0.3837 - categorical_accuracy: 0.8564 - val_loss: 0.4576 - val_categorical_accuracy: 0.8200\n",
      "Epoch 15/30\n",
      "7132/7132 [==============================] - 123s - loss: 0.3719 - categorical_accuracy: 0.8610 - val_loss: 0.4502 - val_categorical_accuracy: 0.8276\n",
      "Epoch 16/30\n",
      "7132/7132 [==============================] - 129s - loss: 0.3608 - categorical_accuracy: 0.8660 - val_loss: 0.4390 - val_categorical_accuracy: 0.8356\n",
      "Epoch 17/30\n",
      "7132/7132 [==============================] - 122s - loss: 0.3510 - categorical_accuracy: 0.8710 - val_loss: 0.4449 - val_categorical_accuracy: 0.8284\n",
      "Epoch 18/30\n",
      "7132/7132 [==============================] - 115s - loss: 0.3414 - categorical_accuracy: 0.8741 - val_loss: 0.4297 - val_categorical_accuracy: 0.8406\n",
      "Epoch 19/30\n",
      "7132/7132 [==============================] - 111s - loss: 0.3322 - categorical_accuracy: 0.8776 - val_loss: 0.4306 - val_categorical_accuracy: 0.8415\n",
      "Epoch 20/30\n",
      "7132/7132 [==============================] - 123s - loss: 0.3243 - categorical_accuracy: 0.8812 - val_loss: 0.4291 - val_categorical_accuracy: 0.8410\n",
      "Epoch 21/30\n",
      "7132/7132 [==============================] - 122s - loss: 0.3162 - categorical_accuracy: 0.8847 - val_loss: 0.4250 - val_categorical_accuracy: 0.8423\n",
      "Epoch 22/30\n",
      "7132/7132 [==============================] - 117s - loss: 0.3088 - categorical_accuracy: 0.8892 - val_loss: 0.4194 - val_categorical_accuracy: 0.8444\n",
      "Epoch 23/30\n",
      "7132/7132 [==============================] - 103s - loss: 0.3019 - categorical_accuracy: 0.8897 - val_loss: 0.4154 - val_categorical_accuracy: 0.8503\n",
      "Epoch 24/30\n",
      "7132/7132 [==============================] - 103s - loss: 0.2936 - categorical_accuracy: 0.8948 - val_loss: 0.4303 - val_categorical_accuracy: 0.8410\n",
      "Epoch 25/30\n",
      "7132/7132 [==============================] - 114s - loss: 0.2867 - categorical_accuracy: 0.8978 - val_loss: 0.4132 - val_categorical_accuracy: 0.8516\n",
      "Epoch 26/30\n",
      "7132/7132 [==============================] - 111s - loss: 0.2804 - categorical_accuracy: 0.9013 - val_loss: 0.4140 - val_categorical_accuracy: 0.8499\n",
      "Epoch 27/30\n",
      "7132/7132 [==============================] - 114s - loss: 0.2739 - categorical_accuracy: 0.9030 - val_loss: 0.4142 - val_categorical_accuracy: 0.8541\n",
      "Epoch 28/30\n",
      "7132/7132 [==============================] - 122s - loss: 0.2674 - categorical_accuracy: 0.9079 - val_loss: 0.4190 - val_categorical_accuracy: 0.8511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x154a61eb8>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(n_embeddings, embedding_vecor_length, input_length=max_len, weights=[embeddings]))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['categorical_accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(patience=2, monitor='val_loss')\n",
    "\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=batch_size, callbacks=[early_stopping], validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5047/5047 [==============================] - 16s    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.78482266680094093"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_20 (Embedding)     (None, 40, 300)           1500000   \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 40, 100)           160400    \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 1,690,753\n",
      "Trainable params: 1,690,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Modelowanie szeregów czasowych\n",
    "\n",
    "https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff6dd0a20b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "dataframe = pandas.read_csv('Dane/international-airline-passengers.csv', usecols=[1], engine='python', skipfooter=3)\n",
    "plt.plot(dataframe)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQgixBElwF0KIJUiCuxBCLEH/Hw63zn53/YCCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff6d6bd8e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(dataset)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 48\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back-1):\n",
    "\t\ta = dataset[i:(i+look_back), 0]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, 0])\n",
    "\treturn numpy.array(dataX), numpy.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "look_back = 3\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.01544401,  0.02702703,  0.05405405]],\n",
       "\n",
       "       [[ 0.02702703,  0.05405405,  0.04826255]],\n",
       "\n",
       "       [[ 0.05405405,  0.04826255,  0.03281853]],\n",
       "\n",
       "       [[ 0.04826255,  0.03281853,  0.05984557]],\n",
       "\n",
       "       [[ 0.03281853,  0.05984557,  0.08494207]],\n",
       "\n",
       "       [[ 0.05984557,  0.08494207,  0.08494207]],\n",
       "\n",
       "       [[ 0.08494207,  0.08494207,  0.06177607]],\n",
       "\n",
       "       [[ 0.08494207,  0.06177607,  0.02895753]],\n",
       "\n",
       "       [[ 0.06177607,  0.02895753,  0.        ]],\n",
       "\n",
       "       [[ 0.02895753,  0.        ,  0.02702703]]], dtype=float32)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainY[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 82 samples, validate on 10 samples\n",
      "Epoch 1/100\n",
      "0s - loss: 0.0543 - val_loss: 0.2018\n",
      "Epoch 2/100\n",
      "0s - loss: 0.0256 - val_loss: 0.1309\n",
      "Epoch 3/100\n",
      "0s - loss: 0.0163 - val_loss: 0.0934\n",
      "Epoch 4/100\n",
      "0s - loss: 0.0137 - val_loss: 0.0752\n",
      "Epoch 5/100\n",
      "0s - loss: 0.0126 - val_loss: 0.0681\n",
      "Epoch 6/100\n",
      "0s - loss: 0.0117 - val_loss: 0.0602\n",
      "Epoch 7/100\n",
      "0s - loss: 0.0107 - val_loss: 0.0532\n",
      "Epoch 8/100\n",
      "0s - loss: 0.0100 - val_loss: 0.0454\n",
      "Epoch 9/100\n",
      "0s - loss: 0.0090 - val_loss: 0.0427\n",
      "Epoch 10/100\n",
      "0s - loss: 0.0082 - val_loss: 0.0372\n",
      "Epoch 11/100\n",
      "0s - loss: 0.0074 - val_loss: 0.0321\n",
      "Epoch 12/100\n",
      "0s - loss: 0.0066 - val_loss: 0.0269\n",
      "Epoch 13/100\n",
      "0s - loss: 0.0060 - val_loss: 0.0210\n",
      "Epoch 14/100\n",
      "0s - loss: 0.0055 - val_loss: 0.0191\n",
      "Epoch 15/100\n",
      "0s - loss: 0.0049 - val_loss: 0.0201\n",
      "Epoch 16/100\n",
      "0s - loss: 0.0046 - val_loss: 0.0174\n",
      "Epoch 17/100\n",
      "0s - loss: 0.0042 - val_loss: 0.0140\n",
      "Epoch 18/100\n",
      "0s - loss: 0.0039 - val_loss: 0.0138\n",
      "Epoch 19/100\n",
      "0s - loss: 0.0038 - val_loss: 0.0123\n",
      "Epoch 20/100\n",
      "0s - loss: 0.0037 - val_loss: 0.0115\n",
      "Epoch 21/100\n",
      "0s - loss: 0.0036 - val_loss: 0.0112\n",
      "Epoch 22/100\n",
      "0s - loss: 0.0035 - val_loss: 0.0111\n",
      "Epoch 23/100\n",
      "0s - loss: 0.0034 - val_loss: 0.0116\n",
      "Epoch 24/100\n",
      "0s - loss: 0.0033 - val_loss: 0.0114\n",
      "Epoch 25/100\n",
      "0s - loss: 0.0033 - val_loss: 0.0113\n",
      "Epoch 26/100\n",
      "0s - loss: 0.0033 - val_loss: 0.0112\n",
      "Epoch 27/100\n",
      "0s - loss: 0.0033 - val_loss: 0.0113\n",
      "Epoch 28/100\n",
      "0s - loss: 0.0032 - val_loss: 0.0111\n",
      "Epoch 29/100\n",
      "0s - loss: 0.0032 - val_loss: 0.0109\n",
      "Epoch 30/100\n",
      "0s - loss: 0.0032 - val_loss: 0.0109\n",
      "Epoch 31/100\n",
      "0s - loss: 0.0032 - val_loss: 0.0107\n",
      "Epoch 32/100\n",
      "0s - loss: 0.0031 - val_loss: 0.0107\n",
      "Epoch 33/100\n",
      "0s - loss: 0.0031 - val_loss: 0.0107\n",
      "Epoch 34/100\n",
      "0s - loss: 0.0031 - val_loss: 0.0106\n",
      "Epoch 35/100\n",
      "0s - loss: 0.0030 - val_loss: 0.0105\n",
      "Epoch 36/100\n",
      "0s - loss: 0.0031 - val_loss: 0.0104\n",
      "Epoch 37/100\n",
      "0s - loss: 0.0030 - val_loss: 0.0102\n",
      "Epoch 38/100\n",
      "0s - loss: 0.0030 - val_loss: 0.0101\n",
      "Epoch 39/100\n",
      "0s - loss: 0.0030 - val_loss: 0.0101\n",
      "Epoch 40/100\n",
      "0s - loss: 0.0030 - val_loss: 0.0100\n",
      "Epoch 41/100\n",
      "0s - loss: 0.0029 - val_loss: 0.0099\n",
      "Epoch 42/100\n",
      "0s - loss: 0.0029 - val_loss: 0.0098\n",
      "Epoch 43/100\n",
      "0s - loss: 0.0029 - val_loss: 0.0098\n",
      "Epoch 44/100\n",
      "0s - loss: 0.0029 - val_loss: 0.0096\n",
      "Epoch 45/100\n",
      "0s - loss: 0.0029 - val_loss: 0.0095\n",
      "Epoch 46/100\n",
      "0s - loss: 0.0028 - val_loss: 0.0096\n",
      "Epoch 47/100\n",
      "0s - loss: 0.0028 - val_loss: 0.0095\n",
      "Epoch 48/100\n",
      "0s - loss: 0.0029 - val_loss: 0.0093\n",
      "Epoch 49/100\n",
      "0s - loss: 0.0028 - val_loss: 0.0093\n",
      "Epoch 50/100\n",
      "0s - loss: 0.0028 - val_loss: 0.0091\n",
      "Epoch 51/100\n",
      "0s - loss: 0.0028 - val_loss: 0.0094\n",
      "Epoch 52/100\n",
      "0s - loss: 0.0027 - val_loss: 0.0090\n",
      "Epoch 53/100\n",
      "0s - loss: 0.0028 - val_loss: 0.0090\n",
      "Epoch 54/100\n",
      "0s - loss: 0.0028 - val_loss: 0.0089\n",
      "Epoch 55/100\n",
      "0s - loss: 0.0028 - val_loss: 0.0089\n",
      "Epoch 56/100\n",
      "0s - loss: 0.0027 - val_loss: 0.0088\n",
      "Epoch 57/100\n",
      "0s - loss: 0.0027 - val_loss: 0.0087\n",
      "Epoch 58/100\n",
      "0s - loss: 0.0027 - val_loss: 0.0087\n",
      "Epoch 59/100\n",
      "0s - loss: 0.0026 - val_loss: 0.0085\n",
      "Epoch 60/100\n",
      "0s - loss: 0.0026 - val_loss: 0.0084\n",
      "Epoch 61/100\n",
      "0s - loss: 0.0027 - val_loss: 0.0085\n",
      "Epoch 62/100\n",
      "0s - loss: 0.0026 - val_loss: 0.0083\n",
      "Epoch 63/100\n",
      "0s - loss: 0.0026 - val_loss: 0.0083\n",
      "Epoch 64/100\n",
      "0s - loss: 0.0025 - val_loss: 0.0086\n",
      "Epoch 65/100\n",
      "0s - loss: 0.0026 - val_loss: 0.0081\n",
      "Epoch 66/100\n",
      "0s - loss: 0.0025 - val_loss: 0.0083\n",
      "Epoch 67/100\n",
      "0s - loss: 0.0025 - val_loss: 0.0082\n",
      "Epoch 68/100\n",
      "0s - loss: 0.0026 - val_loss: 0.0079\n",
      "Epoch 69/100\n",
      "0s - loss: 0.0024 - val_loss: 0.0079\n",
      "Epoch 70/100\n",
      "0s - loss: 0.0024 - val_loss: 0.0083\n",
      "Epoch 71/100\n",
      "0s - loss: 0.0025 - val_loss: 0.0077\n",
      "Epoch 72/100\n",
      "0s - loss: 0.0025 - val_loss: 0.0079\n",
      "Epoch 73/100\n",
      "0s - loss: 0.0025 - val_loss: 0.0079\n",
      "Epoch 74/100\n",
      "0s - loss: 0.0025 - val_loss: 0.0075\n",
      "Epoch 75/100\n",
      "0s - loss: 0.0024 - val_loss: 0.0075\n",
      "Epoch 76/100\n",
      "0s - loss: 0.0024 - val_loss: 0.0075\n",
      "Epoch 77/100\n",
      "0s - loss: 0.0024 - val_loss: 0.0074\n",
      "Epoch 78/100\n",
      "0s - loss: 0.0024 - val_loss: 0.0074\n",
      "Epoch 79/100\n",
      "0s - loss: 0.0024 - val_loss: 0.0073\n",
      "Epoch 80/100\n",
      "0s - loss: 0.0023 - val_loss: 0.0077\n",
      "Epoch 81/100\n",
      "0s - loss: 0.0025 - val_loss: 0.0072\n",
      "Epoch 82/100\n",
      "0s - loss: 0.0023 - val_loss: 0.0074\n",
      "Epoch 83/100\n",
      "0s - loss: 0.0023 - val_loss: 0.0071\n",
      "Epoch 84/100\n",
      "0s - loss: 0.0023 - val_loss: 0.0070\n",
      "Epoch 85/100\n",
      "0s - loss: 0.0024 - val_loss: 0.0070\n",
      "Epoch 86/100\n",
      "0s - loss: 0.0024 - val_loss: 0.0069\n",
      "Epoch 87/100\n",
      "0s - loss: 0.0023 - val_loss: 0.0069\n",
      "Epoch 88/100\n",
      "0s - loss: 0.0023 - val_loss: 0.0068\n",
      "Epoch 89/100\n",
      "0s - loss: 0.0022 - val_loss: 0.0071\n",
      "Epoch 90/100\n",
      "0s - loss: 0.0023 - val_loss: 0.0067\n",
      "Epoch 91/100\n",
      "0s - loss: 0.0022 - val_loss: 0.0069\n",
      "Epoch 92/100\n",
      "0s - loss: 0.0022 - val_loss: 0.0066\n",
      "Epoch 93/100\n",
      "0s - loss: 0.0022 - val_loss: 0.0066\n",
      "Epoch 94/100\n",
      "0s - loss: 0.0022 - val_loss: 0.0069\n",
      "Epoch 95/100\n",
      "0s - loss: 0.0022 - val_loss: 0.0066\n",
      "Epoch 96/100\n",
      "0s - loss: 0.0022 - val_loss: 0.0066\n",
      "Epoch 97/100\n",
      "0s - loss: 0.0022 - val_loss: 0.0065\n",
      "Epoch 98/100\n",
      "0s - loss: 0.0022 - val_loss: 0.0064\n",
      "Epoch 99/100\n",
      "0s - loss: 0.0022 - val_loss: 0.0063\n",
      "Epoch 100/100\n",
      "0s - loss: 0.0021 - val_loss: 0.0062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff6d5ed5c88>"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 26.44 RMSE\n",
      "Test Score: 54.42 RMSE\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff6d561f400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
