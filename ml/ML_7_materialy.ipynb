{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan dnia\n",
    "\n",
    "1. Problem nierównolicznych klas\n",
    "\n",
    "2. Nowe klasyfikatory\n",
    "\n",
    "3. Case study\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Krzywa ROC\n",
    "\n",
    "Mamy wektor prawdziwych klas $y = [0, 1, 1, ..., 0, 0]$ oraz wektor prawdopodobieństw zwrócony przez klasyfikator $p = [0.35, 0.55, 0.91, ..., 0.1, 0.44]$. Klasyfikujemy obserwacje według reguły $p(x) > t$ dla pewnego progu $t$.\n",
    "\n",
    "Zdefiniujmy:\n",
    " - _false positive rate_ FPR - procent obserwacji klasy $0$ zaklasyfikowanych jako $1$,\n",
    " - _true positive rate_ TPR - procent obserwacji klasy $1$ zaklasyfikowanych jako $1$.\n",
    "\n",
    "Obliczamy TPR($t$) oraz FPR($t$) dla każdego $t\\in p$. Krywa ROC to wykres liniowy powstały z punktów (FPR($t$), TPR($t$)) (dla $t$ uporządkowanych malejąco).\n",
    "\n",
    "Wskaźnik AUC to wielkość pola pod krzywą ROC. Wskaźnik AUC jest dobrą miara dla niezbalansowanych klas (podobnie jak F1).\n",
    "\n",
    "<img src=\"http://scikit-learn.org/stable/_images/sphx_glr_plot_roc_001.png\" width=\"400\">\n",
    "\n",
    "Źródło: http://scikit-learn.org/stable/_images/sphx_glr_plot_roc_001.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "y = np.array([0, 0, 1, 1, 1])\n",
    "scores = np.array([0.01, 0.4, 0.35, 0.8, 0.6])\n",
    "fpr, tpr, thresholds = roc_curve(y, scores)\n",
    "print(thresholds)\n",
    "print(fpr)\n",
    "print(tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(fpr,tpr, label = \"AUC = %.2f\" % roc_auc_score(y,scores))\n",
    "plt.xlabel(\"FDR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.legend(loc=\"lower right\", prop={'size':15})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Problem nierównomierności klas (_imbalanced data_)\n",
    "\n",
    "https://svds.com/learning-imbalanced-classes/\n",
    "\n",
    "Podejścia:\n",
    "\n",
    "- \"ważona\" funkcja celu\n",
    "- over-sampling\n",
    "- under-sampling\n",
    "- data-augmentation (przede wszystkim przy pracy z obrazami)\n",
    "- podejścia inne niż klasyfikacja statystyczna (detekcja obserwacji nietypowych, własne heurystyki)\n",
    "\n",
    "Zawsze można też manipulować progiem klasyfikacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opis danych:\n",
    "\n",
    "It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Dane/creditcard.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.unique(data.Class,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean(data.Class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przygotowanie zbioru :\n",
    "\n",
    "1. Co zrobić ze zmienną Time?\n",
    "\n",
    "2. Przeanalizuj rozkład zmiennej Amount (w tym celu narysuj histogram oraz boxplot). Zaproponuj jest transformację."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.drop(['Time'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(data.Amount)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(data.Amount,bins=[0,200,500,1000,5000,25000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.boxplot(data.Amount)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.min(data.Amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['Amount'] = np.log(1+data[\"Amount\"])\n",
    "\n",
    "plt.hist(data.Amount)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data.drop([\"Class\"],axis=1,inplace=False)\n",
    "Y = data.Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przykład: Regresja logistyczna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zawsze zaczynamy od określenia punktu odniesienia!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "c = LogisticRegression()\n",
    "preds = cross_val_predict(estimator=c, X=X, y=Y, method=\"predict_proba\",cv = StratifiedKFold(3))[:,1]\n",
    "roc_auc_score(Y,preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ważenie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = LogisticRegression(class_weight={0:1,1:100})\n",
    "preds = cross_val_predict(estimator=c, X=X, y=Y, method=\"predict_proba\",cv = StratifiedKFold(5))[:,1]\n",
    "roc_auc_score(Y,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = LogisticRegression(class_weight={0:1,1:577})\n",
    "preds = cross_val_predict(estimator=c, X=X, y=Y, method=\"predict_proba\",cv = StratifiedKFold(5))[:,1]\n",
    "roc_auc_score(Y,preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zadanie powtórz analizę dla drzewa decyzyjnego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "c = DecisionTreeClassifier()\n",
    "preds = cross_val_predict(estimator=c, X=X, y=Y, method=\"predict_proba\",cv = StratifiedKFold(3))[:,1]\n",
    "roc_auc_score(Y,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = DecisionTreeClassifier(class_weight={0:1,1:100})\n",
    "preds = cross_val_predict(estimator=c, X=X, y=Y, method=\"predict_proba\",cv = StratifiedKFold(5))[:,1]\n",
    "roc_auc_score(Y,preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie: przetestuj undersamplig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean(Y[:100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def cross_validate_undersampling(X, Y, model, size=1, cv=StratifiedKFold(3,random_state=1)):\n",
    "    \n",
    "    \"\"\"\n",
    "    X - ramka danych\n",
    "    Y - pd.Series\n",
    "    size - określa stosunek liczby obserwacji klasy 0 do klasy 1 w zbiorze wynikowym\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    preds = []\n",
    "    true_labels = []\n",
    "    \n",
    "    for train_index, test_index in cv.split(X,Y):\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    \n",
    "\n",
    "        ind0 = np.random.choice(np.where(y_train==0)[0], int(size*np.sum(y_train==1)),replace=False)  # (size * liczba oserwacji klasy 1) losowych indeksów obserwacji klasy 0 (bez powtórzeń)\n",
    "        ind_final = np.r_[ind0, np.where(y_train==1)[0]] # indeksy ind0 i indeksy klasy obserwacji (można wykorzystać np.r_[...])\n",
    "        X_train_subsample = X_train.iloc[ind_final]\n",
    "        y_train_subsample = y_train.iloc[ind_final]\n",
    "        \n",
    "        clf = deepcopy(model)\n",
    "        clf.fit(X_train_subsample, y_train_subsample) # uczymy model stworzonym zbiorze\n",
    "               \n",
    "        preds.extend(clf.predict_proba(X_test)[:,1]) # prawdopodobienstwa na zbiorze testowym\n",
    "        true_labels.extend(y_test)\n",
    "\n",
    "    return roc_auc_score(true_labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "for model in [LogisticRegression(),DecisionTreeClassifier()]:\n",
    "    for size in [1,2,5,0.5]:\n",
    "        print(cross_validate_undersampling(X,Y,model,size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def cross_validate_undersampling(X, Y, model, size=1, cv=StratifiedKFold(5,random_state=1)):\n",
    "    \n",
    "    preds = []\n",
    "    true_labels = []\n",
    "    \n",
    "    for train_index, test_index in cv.split(X,Y):\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    \n",
    "\n",
    "        ind0 = np.random.choice(np.where(y_train==0)[0],int(size*sum(y_train==1)),replace=False)\n",
    "        ind_final = np.r_[ind0, np.where(y_train==1)[0]]\n",
    "        X_train_subsample = X_train.iloc[ind_final]\n",
    "        y_train_subsample = y_train.iloc[ind_final]\n",
    "        \n",
    "        clf = deepcopy(model)\n",
    "        clf.fit(X_train_subsample,y_train_subsample)\n",
    "               \n",
    "        preds.extend(clf.predict_proba(X_test)[:,1])\n",
    "        true_labels.extend(y_test)\n",
    "\n",
    "    return roc_auc_score(true_labels,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for model in [LogisticRegression(),DecisionTreeClassifier()]:\n",
    "    for size in [1,2,5,10,0.2]:\n",
    "        print(cross_validate_undersampling(X,Y,model,size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie: przetestuj oversampling. Napisz w tym celu funkcję analogiczną do funkcji testującej undersampling. Uwaga: obliczenia potrwają dłużej!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def cross_validate_oversampling(X, Y, model, size=1, cv=StratifiedKFold(5,random_state=1)):\n",
    "    \n",
    "    preds = []\n",
    "    true_labels = []\n",
    "    \n",
    "    for train_index, test_index in cv.split(X,Y):\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    \n",
    "\n",
    "        ind1 = np.resize(np.where(y_train==1)[0], int(size*np.sum(y_train==1)))\n",
    "        ind_final = np.r_[ind1, np.where(y_train==0)[0]]\n",
    "        X_train_subsample = X_train.iloc[ind_final]\n",
    "        y_train_subsample = y_train.iloc[ind_final]\n",
    "        \n",
    "        clf = deepcopy(model)\n",
    "        clf.fit(X_train_subsample,y_train_subsample)\n",
    "               \n",
    "        preds.extend(clf.predict_proba(X_test)[:,1])\n",
    "        true_labels.extend(y_test)\n",
    "\n",
    "    return roc_auc_score(true_labels,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for model in [LogisticRegression(),DecisionTreeClassifier()]:\n",
    "    for size in [1,0.5]:\n",
    "        print(cross_validate_undersampling(X[:50000],Y[:50000],model,size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie: przetestuj modyfikowany undersampling: k-krotnie dokonujemy predykcji przy użyciu undersamplingu, a następnie ostateczna predykcja to średnia z prawdopodobieństw k predykcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validate_repeated_undersampling(X, Y, model, n_estimators=3, cv=StratifiedKFold(5,random_state=1)):\n",
    "    \n",
    "    preds = []\n",
    "    true_labels = []\n",
    "    size=1\n",
    "        \n",
    "    for train_index, test_index in cv.split(X,Y):\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    \n",
    "        scores = np.zeros((n_estimators,len(y_test)))\n",
    "        for i in range(n_estimators):\n",
    "            ind0 = np.random.choice(np.where(y_train==0)[0],int(size*sum(y_train==1)),replace=False)\n",
    "            ind_final = np.r_[ind0, np.where(y_train==1)[0]]\n",
    "            X_train_subsample = X_train.iloc[ind_final]\n",
    "            y_train_subsample = y_train.iloc[ind_final]\n",
    "\n",
    "            clf = deepcopy(model)\n",
    "            clf.fit(X_train_subsample,y_train_subsample)\n",
    "            \n",
    "            scores[i,:] = clf.predict_proba(X_test)[:,1]\n",
    "            #print(scores)\n",
    "\n",
    "        preds.extend(scores.mean(0))\n",
    "        #print(scores)\n",
    "        #print(preds)\n",
    "        true_labels.extend(y_test)\n",
    "\n",
    "    return roc_auc_score(true_labels,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for model in [LogisticRegression(),DecisionTreeClassifier()]:\n",
    "    for k in [3, 10]:\n",
    "        print(cross_validate_repeated_undersampling(X,Y,model,n_estimators=k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Klasyfikatory: \n",
    "\n",
    "## LDA (_linear discriminant analysis_)   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;          QDA (_quadratic discriminant analysis_)\n",
    "\n",
    "\n",
    "Korzystamy ze wzoru Bayesa:\n",
    "\n",
    "## $$P(Y=k|x) \\sim P(x|Y=k)P(Y=k)$$\n",
    "\n",
    "### $P(x|Y=k)$ - wyestymowany rozkład wielowymiarowy normalny. (LDA zakłada jednakową macierz kowariancji w podgrupach, QDA - różne macierze)\n",
    "### $P(Y=k)$ - prawdopodobieństwo a priori.\n",
    "\n",
    "### Klasyfikujemy do klasy, która ma największe prawdopodobieństwo $P(Y=k|x)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://nauka.metodolog.pl/wp-content/uploads/2016/05/liniowa-i-kwadratowa-funkcja-dyskryminacyjna.png\" width=\"500\">\n",
    "Źródło: http://nauka.metodolog.pl/liniowa-analiza-dyskryminacji/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"https://www.researchgate.net/profile/Alexander_Fotheringham/publication/229909322/figure/fig9/AS:295501053349908@1447464349480/Figure-1-Discriminant-analysis-illustrated-LHS-shows-the-result-of-LDA-applied-to-the.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Podejścia bezmodelowe do klasyfikacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metoda K Najbliższych sąsiadów (KNN)\n",
    "\n",
    "<img src=\"https://i.pinimg.com/originals/65/36/b9/6536b9a63fc427e0fc3e1a9687b49aff.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "?KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ciekawy argument - _weights_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metoda najbliższego centroidu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestCentroid\n",
    "?NearestCentroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W praktyce bardzo rzadko stosowana (o ile w ogóle:))\n",
    "\n",
    "Pytanie: dla jakich danych metoda ma sens?\n",
    "\n",
    "- kiedy jesteśmy w stanie sensownie określić metrykę i centroid ma sens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie - dyskusja w parach:\n",
    "\n",
    "Załóżmy, że w naszych danych obserwacje klasy 1 stanowią 1% zbioru, a naszym zadaniem jest klasyfikacja. Zaproponuj podejście do problemu oparte na analizie najbliższych sąsiadów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie\n",
    "\n",
    "Bierzemy dane:\n",
    "https://archive.ics.uci.edu/ml/datasets/Bank+Marketing#\n",
    "\n",
    "bank-full\n",
    "\n",
    "1) Przetwórz dane do postaci, którą przyjmą modele (zamień zmienne nominalne na mumeryczne - one-hot-encoding)\n",
    "\n",
    "2) Podziel dane na zbiór uczący, walidacyjny i testowy z zachowaniem proporcji klas (google -> stratified train test split sklearn (można użyć dwukrotnie podziału na uczacy/testowy))\n",
    "\n",
    "3) Na zbiorze uczącym przy użyciu krowalidacji znajdź optymalne (miara f1) parametry modeli (gridsearchCV):\n",
    "\n",
    " - regresja logistyczna: type regularyzacji, współczynnik regularyzacji\n",
    " - drzewo decyzyjne: minimalna liczba obserwacji w liściu,\n",
    " - LDA (tylko parametey domyślne - przekazujemy grid search'owi pusty słownik)\n",
    " - QDA (tylko parametey domyślne - przekazujemy grid search'owi pusty słownik)\n",
    " - lasy losowe: liczba drzew\n",
    " - K najbliższych sąsiadów: liczba sąsiadów\n",
    "\n",
    " W tym celu:\n",
    " - stwórz listę modeli (listę wywołań konstruktorów z domyślnymi parametrami (postaci: [ KMeans(), ...] ))\n",
    " - stwórz listę siatek parametrów - jeden element listy to siatka parametrów (w postaci jakiej wymaga gridsearchCV) dla odpowiadającego modelu\n",
    " - przejdź pętlą po parach model,siatka (przyda się funkcja zip), wypisz w każdej iteracji wyniki (grid_scores_) kroswalidacji (przyda się funkcja pprint z modułu pprint) i zapisz w liście najlepsze parametry (.best\\_params\\_) dla każdego modelu\n",
    "\n",
    " Możesz poszerzyć siatki o inne parametry.\n",
    "\n",
    "\n",
    "4) Przetestuj każdy model z najlepszymi parametrami na zbiorze validacyjnym, przechodząc pętlą po parach model,zestaw_najlepszych_parametrow (wykorzystaj operację \\**_dict_ do nadania parametrów na podstawie słownika _dict_) i wypisz Accuracy, miarę F1 oraz AUC otrzymane na zbiorze walidacyjnym\n",
    "\n",
    "5) Przetestuj na zbiorze validacyjnym \"model warstwowy\" względem zmiennej housing, konstruowany w następujący sposób: uczymy model (np. drzewo decyzyjne) w podgrupach wyznaczonych przez zmienną housing - tzn. uczymy jedno drzewo na podzbiorze, w którym housing=Yes, a drugie w podzbiorze housing=No. Następnie dokonujemy predykcji w taki sposób, że dla nowej obserwacji patrzymy na wartość zmiennej housing i w zależności od jej wartości dokonujemy predykcji drzewem nauczonym na odpowiednim podzbiorze.\n",
    "\n",
    "6) Wykorzystanie analizy skupień w klasyfikacji. Zbadaj dwa podejścia:\n",
    "  - Pogrupuj dane algorytmem k-średnich. Dodaj do danych zmienną NOMINALNĄ - kodowaną one-hot (nie numeryczną), której wartości są etykietami grupy obserwacji. Na danych rozszerzonych o tę zmienną powtórz punkty 3 i 4 (nie zmieniaj wcześniej przyjętych siatek parametrów). Przetestuj w kroswalidacji różne liczby skupień, a następnie przestestuja na zbiorze walidacyjnym kilka najlepszych modeli. UWAGA: pogrupowanie musi się odbyć na zbirze treningowym, natomiast wartości tej zmiennej na zbiorze walidacyjnym otrzymujemy przyporządowując obserwacje do najbliższych klastrów! [Zadanie z \\*: wykonaj analogiczną procedurę dla grupowania hierarchicznego - w tym przypadku nie dysponujemy metodą predict - samemu trzeba zaimplementować przyporządkowywanie obserwacji do klastrów).\n",
    "  - Pogrupuj dane algorytmem k-średnich. W każdym klastrze z osobna dopasuj model klasyfikacji (ten sam model uczony oddzielnie na każdym podzbiorze). Dokonuj predykcji obserwacji w następujący sposób: przypisz obserwację do najbliższego klastra, a następnie dokonaj predykcji przy użyciu modelu uczonego w tym klastrze.\n",
    "\n",
    "7) Przetestuj over/under sampling\n",
    "\n",
    "8) Przetestuj na zbiorze validacyjnym transformację danych przy użyciu PCA - powtórz stworzony do tej pory kod zmieniając go tak, aby na starcie zamiast inicjacji modeli były pipeliny PCA+Model. \n",
    "   \n",
    "\n",
    "# 9) Oceń jakość działania pięciu najlepszych modeli/pipeline'ów na zbiorze testowym\n",
    "\n",
    "10*) Spróbuj przetworzyć oryginalne zmienne do lepszej postaci. Powtórz cały proces w celu przetestowania wpływu sposobu przetworzenia danych. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('Dane/bank/bank-full.csv',sep=';')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.drop([\"day\",\"month\",\"duration\"],axis=1,inplace=True)\n",
    "data = pd.get_dummies(data,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_table(table, labels=y,split = (0.78,0.11,0.11),\n",
    "\n",
    "                seed = 42):\n",
    "\n",
    "    '''Splits the table into train, validation and test sets.\n",
    "\n",
    "       Returns a tuple (X_train, X_val, X_test, y_train, y_val, yest)'''\n",
    "\n",
    "\n",
    "\n",
    "    # split into training set and the rest\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "\n",
    "        table, labels,\n",
    "\n",
    "        test_size = 1 - split[0],\n",
    "\n",
    "        random_state = seed, stratify = labels)\n",
    "\n",
    "\n",
    "\n",
    "    # split the remaining data into validation and test sets\n",
    "\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp,\n",
    "\n",
    "        test_size = split[2]/(split[1]+split[2]), random_state = seed,\n",
    "\n",
    "        stratify = y_temp)\n",
    "\n",
    "\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = split_table(data.drop([\"y_yes\"],axis=1,inplace=False),data.y_yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mniej sprytny sposób:\n",
    "\n",
    "X = data.drop([\"y_yes\"],axis=1,inplace=False)\n",
    "colnames = X.columns\n",
    "y = data.y_yes\n",
    "\n",
    "sss = StratifiedShuffleSplit(2,5000)\n",
    "ind = list(sss.split(X,y))[0]\n",
    "print(len(ind))\n",
    "\n",
    "\n",
    "X_train_tmp, y_train_tmp = X.iloc[ind[0]], y.iloc[ind[0]]\n",
    "X_test, y_test = X.iloc[ind[1]], y.iloc[ind[1]]\n",
    "\n",
    "sss = StratifiedShuffleSplit(2,5000)\n",
    "ind = list(sss.split(X_train_tmp,y_train_tmp))[0]\n",
    "\n",
    "X_train, y_train = X_train_tmp.iloc[ind[0]], y_train_tmp.iloc[ind[0]]\n",
    "X_valid, y_valid = X_train_tmp.iloc[ind[1]], y_train_tmp.iloc[ind[1]]\n",
    "\n",
    "del X_train_tmp, y_train_tmp\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_valid = sc.transform(X_valid)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "print(X_train.shape, X_valid.shape, X_test.shape)\n",
    "print(np.mean(y_train),np.mean(y_valid),np.mean(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#\n",
    "\n",
    "models = [LogisticRegression(),\n",
    "          KNeighborsClassifier(), \n",
    "          LinearDiscriminantAnalysis(), \n",
    "          QuadraticDiscriminantAnalysis(),\n",
    "          DecisionTreeClassifier(), \n",
    "          RandomForestClassifier()]\n",
    "param_grids = [{\"penalty\":[\"l1\",\"l2\"],\"C\":[0.1,1,10,100,1000,10000]},\n",
    "              {\"n_neighbors\":[1,3,7,19]},\n",
    "              {},\n",
    "              {},\n",
    "              {\"min_samples_leaf\":[5,10,25,50,100]},\n",
    "              {\"n_estimators\":[10,100,300,500]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [LogisticRegression(),\n",
    "          KNeighborsClassifier(), \n",
    "          LinearDiscriminantAnalysis(), \n",
    "          QuadraticDiscriminantAnalysis(),\n",
    "          DecisionTreeClassifier(), \n",
    "          RandomForestClassifier()]\n",
    "param_grids = [{\"penalty\":[\"l1\",\"l2\"],\"C\":[0.1,1,10,100,1000,10000]},\n",
    "              {\"n_neighbors\":[1,3,7,19]},\n",
    "              {},\n",
    "              {},\n",
    "              {\"min_samples_leaf\":[5,10,25,50,100]},\n",
    "              {\"n_estimators\":[10,100,300,500]}]\n",
    "\n",
    "best_params = []\n",
    "for model, grid in zip(models,param_grids):\n",
    "    \n",
    "    print(model)\n",
    "    cv = GridSearchCV(model,grid,scoring=\"f1\")\n",
    "    cv.fit(X_train,y_train)\n",
    "    best_params.append(cv.best_params_)\n",
    "    pprint(cv.grid_scores_)\n",
    "    \n",
    "    \n",
    "\n",
    "for mod, params in zip(models,best_params):\n",
    "\n",
    "    model = deepcopy(mod)\n",
    "    model.set_params(**params)\n",
    "    model.fit(X_train,y_train)\n",
    "    preds = model.predict_proba(X_val)[:,1]\n",
    "    print(accuracy_score(y_val,preds>0.5), f1_score(y_val,preds>0.5), roc_auc_score(y_val,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "from copy import deepcopy\n",
    "\n",
    "for mod, params in zip(models,best_params):\n",
    "\n",
    "    model = deepcopy(mod)\n",
    "    model.set_params(**params)\n",
    "    model.fit(X_train,y_train)\n",
    "    preds = model.predict_proba(X_val)[:,1]\n",
    "    print(accuracy_score(y_val,preds>0.5), f1_score(y_val,preds>0.5), roc_auc_score(y_val,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_valid = X_val\n",
    "y_valid = y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TRZEBA PAMIETAC ZE ZMIENNE ZERO JEDYNKOWE SA ZESTANDARYZOWANE I NIE PRZYJMUJA 1 (TRZEBA POROWNYWAC Z ZEREM)\n",
    "\n",
    "ind1 = (X_train[\"housing_yes\"]>0).reshape(-1)\n",
    "x_1 = X_train.iloc[ind1].drop(\"housing_yes\",axis=1,inplace=False)\n",
    "y_1 = y_train.iloc[ind1]\n",
    "print(np.mean(y_1))\n",
    "\n",
    "ind2 = (X_train[\"housing_yes\"]<=0).reshape(-1)\n",
    "x_2 = X_train.iloc[ind2].drop(\"housing_yes\",axis=1,inplace=False)\n",
    "y_2 = y_train.iloc[ind2]\n",
    "print(np.mean(y_2))\n",
    "\n",
    "for model,params in zip(models,best_params):    \n",
    "\n",
    "    model1 = deepcopy(model)\n",
    "    model1.set_params(**params)\n",
    "    model1.fit(x_1,y_1)\n",
    "  \n",
    "    \n",
    "    model2 = deepcopy(model)\n",
    "    model2.set_params(**params)\n",
    "    model2.fit(x_2,y_2)\n",
    "    \n",
    "    #print(pd.crosstab(y_1,model1.predict(x_1)))\n",
    "    #print(pd.crosstab(y_2,model1.predict(x_2)))\n",
    "    \n",
    "    preds = np.zeros_like(y_valid)\n",
    "    \n",
    "    ind1 = (X_valid[\"housing_yes\"]>0).reshape(-1)\n",
    "    \n",
    "    #print(colnames!=\"housing_yes\")\n",
    "    #print(model1.predict_proba(X_valid[ind1][:,colnames!=\"housing_yes\"])[:,1])\n",
    "    preds[ind1] = model1.predict_proba(X_valid.iloc[ind1].drop(\"housing_yes\",axis=1,inplace=False))[:,1]\n",
    "    \n",
    "    ind2 = (X_valid[\"housing_yes\"]<=0).reshape(-1)\n",
    "    preds[ind2] = model2.predict_proba(X_valid.iloc[ind2].drop(\"housing_yes\",axis=1,inplace=False))[:,1]\n",
    "    \n",
    "    #print(accuracy_score(y_valid))\n",
    "    #print(preds[:10])\n",
    "    print(accuracy_score(y_valid,preds>0.5), f1_score(y_valid,preds>0.5), roc_auc_score(y_valid,preds))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in [2,3]:\n",
    "    \n",
    "    print(\"k = \", k)\n",
    "    \n",
    "    km = KMeans(k)\n",
    "    X_train2 = np.concatenate([X_train, pd.get_dummies(km.fit_predict(X_train),drop_first=True)],axis=1)\n",
    "    X_valid2 = np.concatenate([X_valid, pd.get_dummies(km.predict(X_valid),drop_first=True)],axis=1)\n",
    "    X_test2 = np.concatenate([X_test, pd.get_dummies(km.predict(X_test),drop_first=True)],axis=1)\n",
    "    \n",
    "    for mod, params in zip(models,best_params):\n",
    "\n",
    "        model = deepcopy(mod)\n",
    "        model.set_params(**params)\n",
    "        model.fit(X_train2,y_train)\n",
    "        preds = model.predict_proba(X_valid2)[:,1]\n",
    "        print(accuracy_score(y_valid,preds>0.5), f1_score(y_valid,preds>0.5), roc_auc_score(y_valid,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for K in [2,3]:\n",
    "    \n",
    "    print(\"K = \", K)\n",
    "    \n",
    "    km = KMeans(K)\n",
    "    train_clusters = km.fit_predict(X_train)\n",
    "    valid_clusters = km.predict(X_valid)\n",
    "\n",
    "    for model,params in zip(models,best_params):\n",
    "\n",
    "        preds = np.zeros_like(y_valid)\n",
    "\n",
    "        for k in range(K):\n",
    "\n",
    "            ind = train_clusters==k\n",
    "            x = X_train[ind]\n",
    "            y = y_train[ind]\n",
    "\n",
    "            mod = deepcopy(model)\n",
    "            mod.set_params(**params)\n",
    "            mod.fit(x,y)\n",
    "\n",
    "            preds[valid_clusters==k] = mod.predict_proba(X_valid[valid_clusters==k])[:,1]\n",
    "            #print(preds[:10])\n",
    "        print(accuracy_score(y_valid,preds>0.5), f1_score(y_valid,preds>0.5), roc_auc_score(y_valid,preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_models = [model.set_params(**params) for model,params in zip(models,best_params)]\n",
    "pipelines = [Pipeline([(\"pca\",PCA()),(\"model\",model)]) for model in best_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "from copy import deepcopy\n",
    "\n",
    "for k in [10,20,30,40]:\n",
    "    print(k)\n",
    "    for pipe in pipelines:\n",
    "\n",
    "        model = deepcopy(pipe)\n",
    "        model.set_params(pca__n_components=k)\n",
    "        model.fit(X_train,y_train)\n",
    "        preds = model.predict_proba(X_valid)[:,1]\n",
    "        print(accuracy_score(y_valid,preds>0.5), f1_score(y_valid,preds>0.5), roc_auc_score(y_valid,preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
